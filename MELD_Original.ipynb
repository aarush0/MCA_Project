{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MELD_Original.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnoTYDT5mDcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#from tensorflow.keras.models import Sequential\n",
        "#from tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "import argparse\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Lambda, LSTM, TimeDistributed, Masking, Bidirectional\n",
        "from tensorflow.keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os, pickle\n",
        "import numpy as np\n",
        "'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os, sys\n",
        "from collections import Counter, defaultdict\n",
        "from functools import cmp_to_key\n",
        "import argparse\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Lambda, LSTM, TimeDistributed, Masking, Bidirectional, concatenate, Layer\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate, Dot\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model, load_model\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import keras\n",
        "#from keras_multi_head import MultiHead\n",
        "#from keras_multi_head import MultiHeadAttention\n",
        "from datetime import datetime\n",
        "#from keras_self_attention import SeqSelfAttention\n",
        "import tensorflow as tf\n",
        "from keras import regularizers\n",
        "\n",
        "x = pickle.load(open(\"/content/drive/My Drive/mca/MCA_Project/data/pickles/data_{}.p\".format(\"emotion\"),\"rb\"))\n",
        "revs, W, word_idx_map, vocab, _, label_index = x[0], x[1], x[2], x[3], x[4], x[5]\n",
        "\n",
        "def get_word_indices(data_x):\n",
        "  length = len(data_x.split())\n",
        "  return np.array([word_idx_map[word] for word in data_x.split()] + [0]*(50-length))[:50]\n",
        "\n",
        "def comp_id(x, y):\n",
        "  xd = int(x[:x.find('_')])\n",
        "  xu = int(x[x.find('_')+1:])\n",
        "\n",
        "  yd = int(y[:y.find('_')])\n",
        "  yu = int(y[y.find('_')+1:])\n",
        "\n",
        "  if xd != yd:\n",
        "    return xd - yd\n",
        "  else:\n",
        "    return xu - yu\n",
        "\n",
        "\n",
        "def preprocess():\n",
        "\n",
        "  train_data, val_data, test_data = {},{},{}\n",
        "\n",
        "  counts_train = np.zeros((5,1))\n",
        "  counts_test = np.zeros((5,1))\n",
        "  counts_val = np.zeros((5,1))\n",
        "\n",
        "  tr_labels, v_labels, ts_labels = {}, {}, {}\n",
        "\n",
        "  for i in range(len(revs)):\n",
        "\n",
        "    utterance_id = revs[i]['dialog']+\"_\"+revs[i]['utterance']\n",
        "    \n",
        "    sentence_word_indices = get_word_indices(revs[i]['text'])\n",
        "    \n",
        "    label = label_index[revs[i]['y']]\n",
        "\n",
        "    if label != 0 and label != 3 and label != 4 and label != 6:\n",
        "      continue\n",
        "\n",
        "    if label == 0:\n",
        "      label = 0\n",
        "    elif label == 3:\n",
        "      label = 1\n",
        "    elif label == 4:\n",
        "      label = 2\n",
        "    else:\n",
        "      label = 3 \n",
        "\n",
        "    if revs[i]['split']==\"train\":\n",
        "      tr_labels[utterance_id] = label\n",
        "    if revs[i]['split']==\"val\":\n",
        "      v_labels[utterance_id] = label\n",
        "    if revs[i]['split']==\"test\":\n",
        "      ts_labels[utterance_id] = label\n",
        "\n",
        "    if revs[i]['split']==\"train\" and counts_train[label] > 683:\n",
        "      continue\n",
        "\n",
        "    if revs[i]['split']==\"train\":\n",
        "        train_data[utterance_id]=(sentence_word_indices,label)\n",
        "        counts_train[label] += 1\n",
        "    elif revs[i]['split']==\"val\":\n",
        "        val_data[utterance_id]=(sentence_word_indices,label)\n",
        "        counts_val[label] += 1\n",
        "    elif revs[i]['split']==\"test\":\n",
        "        test_data[utterance_id]=(sentence_word_indices,label)\n",
        "        counts_test[label] += 1\n",
        "\n",
        "  dialogs = []\n",
        "  utrs = -1\n",
        "  d_cur = -1\n",
        "\n",
        "  t_d = {}\n",
        "  t_map = {}\n",
        "  sorted_tr_keys = sorted(train_data.keys(), key=cmp_to_key(comp_id))\n",
        "\n",
        "  for i in sorted_tr_keys:\n",
        "    d = i[:i.find('_')]\n",
        "    u = i[i.find('_') + 1:]\n",
        "    ouid = d + '_' + u\n",
        "\n",
        "    if d not in dialogs:\n",
        "      d_cur += 1\n",
        "      utrs = 0\n",
        "      dialogs.append(d)\n",
        "    else:\n",
        "      utrs += 1\n",
        "\n",
        "    df = d_cur\n",
        "    uf = utrs\n",
        "\n",
        "    uid = str(df) +'_' + str(uf)\n",
        "    t_d[uid] = train_data[i]\n",
        "\n",
        "    t_map[uid] = ouid\n",
        "\n",
        "  dialogs = []\n",
        "  utrs = -1\n",
        "  d_cur = -1\n",
        "\n",
        "  v_d = {}\n",
        "  v_map = {}\n",
        "  sorted_val_keys = sorted(val_data.keys(), key=cmp_to_key(comp_id))\n",
        "\n",
        "  for i in sorted_val_keys:\n",
        "    d = i[:i.find('_')]\n",
        "    u = i[i.find('_') + 1:]\n",
        "    ouid = d + '_' + u\n",
        "\n",
        "    if d not in dialogs:\n",
        "      d_cur += 1\n",
        "      utrs = 0\n",
        "      dialogs.append(d)\n",
        "    else:\n",
        "      utrs += 1\n",
        "\n",
        "    df = d_cur\n",
        "    uf = utrs\n",
        "\n",
        "    uid = str(df) +'_' + str(uf)\n",
        "    v_d[uid] = val_data[i]\n",
        "    v_map[uid] = ouid\n",
        "\n",
        "  dialogs = []\n",
        "  utrs = -1\n",
        "  d_cur = -1\n",
        "\n",
        "  ts_d = {}\n",
        "  ts_map = {}\n",
        "  sorted_ts_keys = sorted(test_data.keys(), key=cmp_to_key(comp_id))\n",
        "\n",
        "  for i in sorted_ts_keys:\n",
        "    d = i[:i.find('_')]\n",
        "    u = i[i.find('_') + 1:]\n",
        "    ouid = d + '_' + u\n",
        "\n",
        "    if d not in dialogs:\n",
        "      d_cur += 1\n",
        "      utrs = 0\n",
        "      dialogs.append(d)\n",
        "    else:\n",
        "      utrs += 1\n",
        "\n",
        "    df = d_cur\n",
        "    uf = utrs\n",
        "\n",
        "    uid = str(df) +'_' + str(uf)\n",
        "    ts_d[uid] = test_data[i]\n",
        "    ts_map[uid] = ouid\n",
        "  \n",
        "  return t_d, v_d, ts_d, t_map, v_map, ts_map, tr_labels, v_labels, ts_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIuH9Uykmb_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length=50 # Maximum length of the sentence\n",
        "\n",
        "class Dataloader:\n",
        "    \n",
        "    def __init__(self, mode=None):\n",
        "\n",
        "        self.MODE = mode # Sentiment or Emotion classification mode\n",
        "        self.max_l = max_length\n",
        "\n",
        "\n",
        "        x = pickle.load(open(\"/content/drive/My Drive/mca/MCA_Project/data/pickles/data_{}.p\".format(self.MODE.lower()),\"rb\"))\n",
        "        self.revs, self.W, self.word_idx_map, self.vocab, _, label_index = x[0], x[1], x[2], x[3], x[4], x[5]\n",
        "        \n",
        "        self.num_classes = 4\n",
        "        print(\"Labels used for this classification: {'neutral': 0, 'sadness': 1, 'joy': 2, 'anger': 3}\")\n",
        "\n",
        "        self.train_data, self.val_data, self.test_data, self.tr_map, self.v_map, self.ts_map, self.tr_labels, self.v_labels, self.ts_labels = preprocess()\n",
        "\n",
        "        # Creating dialogue:[utterance_1, utterance_2, ...] ids\n",
        "        self.train_dialogue_ids = self.get_dialogue_ids(self.train_data.keys())\n",
        "        self.val_dialogue_ids = self.get_dialogue_ids(self.val_data.keys())\n",
        "        self.test_dialogue_ids = self.get_dialogue_ids(self.test_data.keys())\n",
        "\n",
        "        # Max utternance in a dialog in the dataset\n",
        "        self.max_utts = self.get_max_utts(self.train_dialogue_ids, self.val_dialogue_ids, self.test_dialogue_ids)\n",
        "\n",
        "    def get_dialogue_ids(self, keys):\n",
        "        ids=defaultdict(list)\n",
        "        for key in keys:\n",
        "            ids[key.split(\"_\")[0]].append(int(key.split(\"_\")[1]))\n",
        "        for ID, utts in ids.items():\n",
        "            ids[ID]=[str(utt) for utt in sorted(utts)]\n",
        "        return ids\n",
        "\n",
        "    def get_max_utts(self, train_ids, val_ids, test_ids):\n",
        "        max_utts_train = max([len(train_ids[vid]) for vid in train_ids.keys()])\n",
        "        max_utts_val = max([len(val_ids[vid]) for vid in val_ids.keys()])\n",
        "        max_utts_test = max([len(test_ids[vid]) for vid in test_ids.keys()])\n",
        "        return np.max([max_utts_train, max_utts_val, max_utts_test])\n",
        "\n",
        "    def get_one_hot(self, label):\n",
        "        label_arr = [0]*self.num_classes\n",
        "        label_arr[label]=1\n",
        "        return label_arr[:]\n",
        "\n",
        "    def get_dialogue_text_embs(self):\n",
        "        key = list(self.train_data.keys())[0]\n",
        "        \n",
        "        pad = [0]*len(self.train_data[key][0])\n",
        "\n",
        "        def get_emb(dialogue_id, local_data):\n",
        "            dialogue_text = []\n",
        "            for vid in dialogue_id.keys():\n",
        "                local_text = []\n",
        "                for utt in dialogue_id[vid]:\n",
        "                    local_text.append(local_data[vid+\"_\"+str(utt)][0][:])\n",
        "                for _ in range(self.max_utts-len(local_text)):\n",
        "                    local_text.append(pad[:])\n",
        "                dialogue_text.append(local_text[:self.max_utts])\n",
        "            return np.array(dialogue_text)\n",
        "\n",
        "        self.train_dialogue_features = get_emb(self.train_dialogue_ids, self.train_data)\n",
        "        self.val_dialogue_features = get_emb(self.val_dialogue_ids, self.val_data)\n",
        "        self.test_dialogue_features = get_emb(self.test_dialogue_ids, self.test_data)\n",
        "\n",
        "    def get_dialogue_labels(self):\n",
        "\n",
        "        def get_labels(ids, data):\n",
        "            dialogue_label=[]\n",
        "\n",
        "            for vid, utts in ids.items():\n",
        "                local_labels=[]\n",
        "                for utt in utts:\n",
        "                    local_labels.append(self.get_one_hot(data[vid+\"_\"+str(utt)][1]))\n",
        "                for _ in range(self.max_utts-len(local_labels)):\n",
        "                    local_labels.append(self.get_one_hot(1)) # Dummy label\n",
        "                dialogue_label.append(local_labels[:self.max_utts])\n",
        "            return np.array(dialogue_label)\n",
        "\n",
        "        self.train_dialogue_label=get_labels(self.train_dialogue_ids, self.train_data)\n",
        "        self.val_dialogue_label=get_labels(self.val_dialogue_ids, self.val_data)\n",
        "        self.test_dialogue_label=get_labels(self.test_dialogue_ids, self.test_data)\n",
        "\n",
        "    def get_dialogue_labels_audio(self):\n",
        "\n",
        "        def get_labels(ids, data, Map, map1):\n",
        "            dialogue_label=[]\n",
        "\n",
        "            for vid, utts in ids.items():\n",
        "                local_labels=[]\n",
        "                for utt in utts:\n",
        "                    local_labels.append(self.get_one_hot(Map[map1[vid+'_'+str(utt)]]))\n",
        "                for _ in range(self.max_utts-len(local_labels)):\n",
        "                    local_labels.append(self.get_one_hot(1)) # Dummy label\n",
        "                dialogue_label.append(local_labels[:self.max_utts])\n",
        "            return np.array(dialogue_label)\n",
        "\n",
        "        self.train_dialogue_label=get_labels(self.train_dialogue_ids, self.train_data, self.tr_labels, self.tr_map)\n",
        "        self.val_dialogue_label=get_labels(self.val_dialogue_ids, self.val_data, self.v_labels, self.v_map)\n",
        "        self.test_dialogue_label=get_labels(self.test_dialogue_ids, self.test_data, self.ts_labels, self.ts_map)\n",
        "\n",
        "        \n",
        "    def get_dialogue_lengths(self):\n",
        "\n",
        "        self.train_dialogue_length, self.val_dialogue_length, self.test_dialogue_length=[], [], []\n",
        "        for vid, utts in self.train_dialogue_ids.items():\n",
        "            self.train_dialogue_length.append(len(utts))\n",
        "        for vid, utts in self.val_dialogue_ids.items():\n",
        "            self.val_dialogue_length.append(len(utts))\n",
        "        for vid, utts in self.test_dialogue_ids.items():\n",
        "            self.test_dialogue_length.append(len(utts))\n",
        "\n",
        "    def get_masks(self):\n",
        "\n",
        "        self.train_mask = np.zeros((len(self.train_dialogue_length), self.max_utts), dtype='int')\n",
        "        for i in range(len(self.train_dialogue_length)):\n",
        "            self.train_mask[i,:self.train_dialogue_length[i]]=1\n",
        "        self.val_mask = np.zeros((len(self.val_dialogue_length), self.max_utts), dtype='int')\n",
        "        for i in range(len(self.val_dialogue_length)):\n",
        "            self.val_mask[i,:self.val_dialogue_length[i]]=1\n",
        "        self.test_mask = np.zeros((len(self.test_dialogue_length), self.max_utts), dtype='int')\n",
        "        for i in range(len(self.test_dialogue_length)):\n",
        "            self.test_mask[i,:self.test_dialogue_length[i]]=1\n",
        "        \n",
        "    def load_text_data(self, ):\n",
        "\n",
        "        self.get_dialogue_text_embs()\n",
        "        self.get_dialogue_lengths()\n",
        "        self.get_dialogue_labels()\n",
        "        self.get_masks()\n",
        "\n",
        "    def load_audio_data(self, ):\n",
        "\n",
        "        AUDIO_PATH = \"/content/drive/My Drive/mca/MCA_Project/data/pickles/audio_embeddings_feature_selection_{}.pkl\".format(self.MODE.lower())\n",
        "        self.train_audio_emb, self.val_audio_emb, self.test_audio_emb = pickle.load(open(AUDIO_PATH,\"rb\"))\n",
        "\n",
        "        self.get_dialogue_audio_embs()\n",
        "        self.get_dialogue_lengths()\n",
        "        self.get_dialogue_labels_audio()\n",
        "        self.get_masks()\n",
        "\n",
        "    def get_dialogue_audio_embs(self):\n",
        "        key = list(self.train_audio_emb.keys())[0]\n",
        "        pad = [0]*len(self.train_audio_emb[key])\n",
        "\n",
        "        def get_emb(dialogue_id, audio_emb, Map):\n",
        "            dialogue_audio=[]\n",
        "            for vid in dialogue_id.keys():\n",
        "                local_audio=[]\n",
        "                for utt in dialogue_id[vid]:\n",
        "                    try:\n",
        "                        local_audio.append(audio_emb[Map[vid+\"_\"+str(utt)]][:])\n",
        "                    except:\n",
        "                        print(vid+\"_\"+str(utt))\n",
        "                        local_audio.append(pad[:])\n",
        "                for _ in range(self.max_utts-len(local_audio)):\n",
        "                    local_audio.append(pad[:])\n",
        "                dialogue_audio.append(local_audio[:self.max_utts])\n",
        "            return np.array(dialogue_audio)\n",
        "\n",
        "        self.train_dialogue_features = get_emb(self.train_dialogue_ids, self.train_audio_emb, self.tr_map)\n",
        "        self.val_dialogue_features = get_emb(self.val_dialogue_ids, self.val_audio_emb, self.v_map)\n",
        "        self.test_dialogue_features = get_emb(self.test_dialogue_ids, self.test_audio_emb, self.ts_map)\n",
        "\n",
        "\n",
        "    def load_video_data(self, ):\n",
        "\n",
        "        self.val_video_emb = np.load('/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/val_video_features.pkl', allow_pickle='True').item()\n",
        "        self.train_video_emb = np.load('/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/train_video_features_temp.pkl', allow_pickle='True').item()\n",
        "        self.test_video_emb = np.load('/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/test_video_features.pkl', allow_pickle='True').item()\n",
        "        \n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "        \n",
        "        def normalize(emb):\n",
        "          X = []\n",
        "          scaler = StandardScaler()\n",
        "\n",
        "          for e in sorted(emb.keys()):\n",
        "            X.append(emb[e])\n",
        "\n",
        "          X = scaler.fit_transform(X)\n",
        "\n",
        "          for n, e in enumerate(sorted(emb.keys())):\n",
        "            emb[e] = X[n]\n",
        "\n",
        "          return emb\n",
        "\n",
        "\n",
        "        self.val_video_emb = normalize(self.val_video_emb)\n",
        "        self.test_video_emb = normalize(self.test_video_emb)\n",
        "        self.train_video_emb = normalize(self.train_video_emb)\n",
        "        \n",
        "        self.get_dialogue_video_embs()\n",
        "        self.get_dialogue_lengths()\n",
        "        self.get_dialogue_labels_video()\n",
        "        self.get_masks()\n",
        "\n",
        "    def get_dialogue_video_embs(self):\n",
        "        key = list(self.train_video_emb.keys())[0]\n",
        "        pad = [0]*len(self.train_video_emb[key])\n",
        "\n",
        "        def get_emb(dialogue_id, video_emb, Map):\n",
        "            dialogue_video=[]\n",
        "            for vid in dialogue_id.keys():\n",
        "                local_video=[]\n",
        "                for utt in dialogue_id[vid]:\n",
        "                    try:\n",
        "                        local_video.append(video_emb[Map[vid+\"_\"+str(utt)]][:])\n",
        "                    except:\n",
        "                        print(vid+\"_\"+str(utt), end = \" \")\n",
        "                        local_video.append(pad[:])\n",
        "                for _ in range(self.max_utts-len(local_video)):\n",
        "                    local_video.append(pad[:])\n",
        "                dialogue_video.append(local_video[:self.max_utts])\n",
        "            return np.array(dialogue_video)\n",
        "\n",
        "        print(\"train\")\n",
        "        self.train_dialogue_features = get_emb(self.train_dialogue_ids, self.train_video_emb, self.tr_map)\n",
        "        print(\"val\")\n",
        "        self.val_dialogue_features = get_emb(self.val_dialogue_ids, self.val_video_emb, self.v_map)\n",
        "        print(\"test\")\n",
        "        self.test_dialogue_features = get_emb(self.test_dialogue_ids, self.test_video_emb, self.ts_map)\n",
        "        print(\"\")\n",
        "\n",
        "    def get_dialogue_labels_video(self):\n",
        "\n",
        "        def get_labels(ids, data, Map, map1):\n",
        "            dialogue_label=[]\n",
        "\n",
        "            for vid, utts in ids.items():\n",
        "                local_labels=[]\n",
        "                for utt in utts:\n",
        "                    local_labels.append(self.get_one_hot(Map[map1[vid+'_'+str(utt)]]))\n",
        "                for _ in range(self.max_utts-len(local_labels)):\n",
        "                    local_labels.append(self.get_one_hot(1)) # Dummy label\n",
        "                dialogue_label.append(local_labels[:self.max_utts])\n",
        "            return np.array(dialogue_label)\n",
        "\n",
        "        self.train_dialogue_label=get_labels(self.train_dialogue_ids, self.train_data, self.tr_labels, self.tr_map)\n",
        "        self.val_dialogue_label=get_labels(self.val_dialogue_ids, self.val_data, self.v_labels, self.v_map)\n",
        "        self.test_dialogue_label=get_labels(self.test_dialogue_ids, self.test_data, self.ts_labels, self.ts_map)\n",
        "\n",
        "    def load_bimodal_data(self,train_text_x, train_audio_x,val_text_x, val_audio_x,test_text_x, test_audio_x):\n",
        "        \n",
        "        TEXT_UNIMODAL = \"/content/drive/My Drive/mca/MCA_Project/data/pickles/text_{}.pkl\".format(self.MODE.lower())\n",
        "        AUDIO_UNIMODAL = \"/content/drive/My Drive/mca/MCA_Project/data/pickles/audio_{}.pkl\".format(self.MODE.lower())\n",
        "\n",
        "        #Load features\n",
        "        train_text_x, val_text_x, test_text_x = pickle.load(open(TEXT_UNIMODAL, \"rb\"), encoding='latin1')\n",
        "        train_audio_x, val_audio_x, test_audio_x = pickle.load(open(AUDIO_UNIMODAL, \"rb\"), encoding='latin1')\n",
        "\n",
        "        def concatenate_fusion(ID, text, audio):\n",
        "            bimodal=[]\n",
        "            for vid, utts in ID.items():\n",
        "                bimodal.append(np.concatenate( (text[vid],2*audio[vid]) , axis=0))\n",
        "            return np.array(bimodal)\n",
        "\n",
        "        self.train_dialogue_features = concatenate_fusion(self.train_dialogue_ids, train_text_x, train_audio_x)\n",
        "        self.val_dialogue_features = concatenate_fusion(self.val_dialogue_ids, val_text_x, val_audio_x)\n",
        "        self.test_dialogue_features = concatenate_fusion(self.test_dialogue_ids, test_text_x, test_audio_x)\n",
        "\n",
        "        self.get_dialogue_lengths()\n",
        "        self.get_dialogue_labels()\n",
        "        self.get_masks()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F16Rf7Bn4aU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Lambda, LSTM, TimeDistributed, Masking, Bidirectional\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model, load_model\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os, pickle\n",
        "import numpy as np\n",
        "\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "class bc_LSTM:\n",
        "  def __init__(self,ar):\n",
        "    self.classification_mode = \"emotion\"\n",
        "    self.PATH = \"/content/drive/My Drive/mca/MCA_Project/data/models/{}_weights_{}.hdf5\".format(ar,self.classification_mode.lower())\n",
        "    self.OUTPUT_PATH = \"/content/drive/My Drive/mca/MCA_Project/data/pickles/{}_{}.pkl\".format(ar,self.classification_mode.lower())\n",
        "\n",
        "  def load_data(self,m):\n",
        "    print('Loading data')\n",
        "    self.data = Dataloader(mode = self.classification_mode)\n",
        "    if m == \"text\":\n",
        "      self.data.load_text_data()\n",
        "    elif m == \"audio\":\n",
        "      self.data.load_audio_data()\n",
        "    elif m ==\"bimodal\":\n",
        "      self.data.load_text_data()\n",
        "      self.train_x = self.data.train_dialogue_features\n",
        "      self.val_x = self.data.val_dialogue_features\n",
        "      self.test_x = self.data.test_dialogue_features\n",
        "      \n",
        "      self.train_y = self.data.train_dialogue_label\n",
        "      self.val_y = self.data.val_dialogue_label\n",
        "      self.test_y = self.data.test_dialogue_label\n",
        "      \n",
        "      self.train_mask = self.data.train_mask\n",
        "      self.val_mask = self.data.val_mask\n",
        "      self.test_mask = self.data.test_mask\n",
        "      \n",
        "      self.train_id = self.data.train_dialogue_ids.keys()\n",
        "      self.val_id = self.data.val_dialogue_ids.keys()\n",
        "      self.test_id = self.data.test_dialogue_ids.keys()\n",
        "      \n",
        "      self.sequence_length = self.train_x.shape[1]\n",
        "      self.classes = self.train_y.shape[2]\n",
        "      \n",
        "      self.train_x_text = self.train_x\n",
        "      self.val_x_text = self.val_x\n",
        "      self.test_x_text = self.test_x\n",
        "      \n",
        "      self.train_y_text = self.train_y\n",
        "      self.val_y_text = self.val_y \n",
        "      self.test_y_text = self.test_y \n",
        "      \n",
        "      self.train_mask_text = self.train_mask \n",
        "      self.val_mask_text = self.val_mask \n",
        "      self.test_mask_text = self.test_mask\n",
        "      \n",
        "      self.train_id_text = self.train_id \n",
        "      self.val_id_text = self.val_id \n",
        "      self.test_id_text = self.test_id \n",
        "      \n",
        "      self.sequence_length_text = self.sequence_length\n",
        "      \n",
        "      self.data.load_audio_data()\n",
        "      self.train_x = self.data.train_dialogue_features\n",
        "      self.val_x = self.data.val_dialogue_features\n",
        "      self.test_x = self.data.test_dialogue_features\n",
        "      \n",
        "      self.train_y = self.data.train_dialogue_label\n",
        "      self.val_y = self.data.val_dialogue_label\n",
        "      self.test_y = self.data.test_dialogue_label\n",
        "      \n",
        "      self.train_mask = self.data.train_mask\n",
        "      self.val_mask = self.data.val_mask\n",
        "      self.test_mask = self.data.test_mask\n",
        "      \n",
        "      self.train_id = self.data.train_dialogue_ids.keys()\n",
        "      self.val_id = self.data.val_dialogue_ids.keys()\n",
        "      self.test_id = self.data.test_dialogue_ids.keys()\n",
        "      \n",
        "      self.sequence_length = self.train_x.shape[1]\n",
        "      self.classes = self.train_y.shape[2]\n",
        "      self.train_x_audio = self.train_x\n",
        "      self.val_x_audio = self.val_x\n",
        "      self.test_x_audio = self.test_x\n",
        "      \n",
        "      self.train_y_audio = self.train_y\n",
        "      self.val_y_audio = self.val_y \n",
        "      self.test_y_audio = self.test_y \n",
        "      \n",
        "      self.train_mask_audio = self.train_mask \n",
        "      self.val_mask_audio = self.val_mask \n",
        "      self.test_mask_audio = self.test_mask\n",
        "      \n",
        "      self.train_id_audio = self.train_id \n",
        "      self.val_id_audio = self.val_id \n",
        "      self.test_id_audio = self.test_id\n",
        "      self.sequence_length_audio = self.sequence_length\n",
        "\n",
        "      #self.data.load_bimodal_data()\n",
        "\n",
        "    self.train_x = self.data.train_dialogue_features\n",
        "    self.val_x = self.data.val_dialogue_features\n",
        "    self.test_x = self.data.test_dialogue_features\n",
        "    \n",
        "    self.train_y = self.data.train_dialogue_label\n",
        "    self.val_y = self.data.val_dialogue_label\n",
        "    self.test_y = self.data.test_dialogue_label\n",
        "    \n",
        "    self.train_mask = self.data.train_mask\n",
        "    self.val_mask = self.data.val_mask\n",
        "    self.test_mask = self.data.test_mask\n",
        "    \n",
        "    self.train_id = self.data.train_dialogue_ids.keys()\n",
        "    self.val_id = self.data.val_dialogue_ids.keys()\n",
        "    self.test_id = self.data.test_dialogue_ids.keys()\n",
        "    \n",
        "    self.sequence_length = self.train_x.shape[1]\n",
        "    \n",
        "    self.classes = self.train_y.shape[2]\n",
        "    \n",
        "    if m == \"text\":\n",
        "      print(\"WELCOME TO TEXT DATA ADDITION\")\n",
        "      self.train_x_text = self.train_x\n",
        "      self.val_x_text = self.val_x\n",
        "      self.test_x_text = self.test_x\n",
        "      \n",
        "      self.train_y_text = self.train_y\n",
        "      self.val_y_text = self.val_y \n",
        "      self.test_y_text = self.test_y \n",
        "      \n",
        "      self.train_mask_text = self.train_mask \n",
        "      self.val_mask_text = self.val_mask \n",
        "      self.test_mask_text = self.test_mask\n",
        "      \n",
        "      self.train_id_text = self.train_id \n",
        "      self.val_id_text = self.val_id \n",
        "      self.test_id_text = self.test_id \n",
        "      \n",
        "      self.sequence_length_text = self.sequence_length\n",
        "    \n",
        "    if m == \"audio\":\n",
        "      self.train_x_audio = self.train_x\n",
        "      self.val_x_audio = self.val_x\n",
        "      self.test_x_audio = self.test_x\n",
        "      \n",
        "      self.train_y_audio = self.train_y\n",
        "      self.val_y_audio = self.val_y \n",
        "      self.test_y_audio = self.test_y \n",
        "      \n",
        "      self.train_mask_audio = self.train_mask \n",
        "      self.val_mask_audio = self.val_mask \n",
        "      self.test_mask_audio = self.test_mask\n",
        "      \n",
        "      self.train_id_audio = self.train_id \n",
        "      self.val_id_audio = self.val_id \n",
        "      self.test_id_audio = self.test_id\n",
        "      self.sequence_length_audio = self.sequence_length\n",
        "      \n",
        "    if m == \"bimodal\":\n",
        "      #self.train_x_video = self.train_x\n",
        "      #self.val_x_video = self.val_x\n",
        "      #self.test_x_video = self.test_x\n",
        "      \n",
        "      #self.train_y_video = self.train_y\n",
        "      #self.val_y_video = self.val_y \n",
        "      #self.test_y_video = self.test_y \n",
        "      \n",
        "      #self.train_mask_video = self.train_mask \n",
        "      #self.val_mask_video = self.val_mask \n",
        "      #self.test_mask_video = self.test_mask\n",
        "      \n",
        "      #self.train_id_video = self.train_id \n",
        "      #self.val_id_video = self.val_id \n",
        "      #self.test_id_video = self.test_id \n",
        "      #self.sequence_length_video = self.sequence_length\n",
        "\n",
        "      self.data.load_bimodal_data(self.train_x_text, self.train_x_audio,self.val_x_text, self.val_x_audio,self.test_x_text, self.test_x_audio )\n",
        "      \n",
        "      self.epochs = 20\n",
        "      self.batch_size = 50\n",
        "  \n",
        "  def calc_test_result(self, pred_label, test_label, test_mask):\n",
        "    true_label=[]\n",
        "    predicted_label=[]\n",
        "    for i in range(pred_label.shape[0]):\n",
        "      for j in range(pred_label.shape[1]):\n",
        "        if test_mask[i,j]==1:\n",
        "          true_label.append(np.argmax(test_label[i,j] ))\n",
        "          predicted_label.append(np.argmax(pred_label[i,j] ))\n",
        "    print(\"Confusion Matrix :\")\n",
        "    print(confusion_matrix(true_label, predicted_label))\n",
        "    print(\"Classification Report :\")\n",
        "    print(classification_report(true_label, predicted_label, digits=4))\n",
        "    print('Weighted FScore: \\n ', precision_recall_fscore_support(true_label, predicted_label, average='weighted'))\n",
        "    \n",
        "  def get_audio_model(self):\n",
        "    # Modality specific hyperparameters\n",
        "    self.epochs = 20\n",
        "    self.batch_size = 50\n",
        "    \n",
        "    self.embedding_dim = self.train_x.shape[2]\n",
        "    print(\"Creating Model...\")\n",
        "    \n",
        "    inputs = Input(shape=(self.sequence_length, self.embedding_dim), dtype='float32')\n",
        "    masked = Masking(mask_value =0)(inputs)\n",
        "    lstm = Bidirectional(LSTM(300, activation='tanh', return_sequences = True, dropout=0.4))(masked)\n",
        "    lstm = Bidirectional(LSTM(300, activation='tanh', return_sequences = True, dropout=0.4), name=\"utter\")(lstm)\n",
        "    output = TimeDistributed(Dense(self.classes,activation='softmax'))(lstm)\n",
        "    \n",
        "    model = Model(inputs, output)\n",
        "    return model\n",
        "    \n",
        "  def get_text_model(self):\n",
        "    \n",
        "    self.epochs = 20\n",
        "    self.batch_size = 50\n",
        "    \n",
        "    self.embedding_dim = self.data.W.shape[1]\n",
        "    \n",
        "    self.vocabulary_size = self.data.W.shape[0]\n",
        "    self.filter_sizes = [3,4,5]\n",
        "    self.num_filters = 512\n",
        "    \n",
        "    print(\"Creating Model...\")\n",
        "    \n",
        "    sentence_length = self.train_x.shape[2]\n",
        "\n",
        "\t\t# Initializing sentence representation layers\n",
        "    embedding = Embedding(input_dim=self.vocabulary_size, output_dim=self.embedding_dim, weights=[self.data.W], input_length=sentence_length, trainable=False)\n",
        "    conv_0 = Conv2D(self.num_filters, kernel_size=(self.filter_sizes[0], self.embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')\n",
        "    conv_1 = Conv2D(self.num_filters, kernel_size=(self.filter_sizes[1], self.embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')\n",
        "    conv_2 = Conv2D(self.num_filters, kernel_size=(self.filter_sizes[2], self.embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')\n",
        "    maxpool_0 = MaxPool2D(pool_size=(sentence_length - self.filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')\n",
        "    maxpool_1 = MaxPool2D(pool_size=(sentence_length - self.filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')\n",
        "    maxpool_2 = MaxPool2D(pool_size=(sentence_length - self.filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')\n",
        "    dense_func = Dense(100, activation='tanh', name=\"dense\")\n",
        "    dense_final = Dense(units=self.classes, activation='softmax')\n",
        "    reshape_func = Reshape((sentence_length, self.embedding_dim, 1))\n",
        "    \n",
        "    self.sentence_length = self.train_x.shape[2]\n",
        "    \n",
        "    self.embedding_dim = self.data.W.shape[1]\n",
        "    \n",
        "    self.vocabulary_size = self.data.W.shape[0]\n",
        "    \n",
        "    embedding = Embedding(input_dim=self.vocabulary_size, output_dim=self.embedding_dim, weights=[self.data.W], input_length=self.sentence_length, trainable=False)\n",
        "    \n",
        "    def slicer(x, index):\n",
        "      return x[:,K.constant(index, dtype='int32'),:]\n",
        "      \n",
        "    def slicer_output_shape(input_shape):\n",
        "      shape = list(input_shape)\n",
        "      assert len(shape) == 3  # batch, seq_len, sent_len\n",
        "      new_shape = (shape[0], shape[2])\n",
        "      return new_shape\n",
        "      \n",
        "    def reshaper(x):\n",
        "      return K.expand_dims(x, axis=3)\n",
        "      \n",
        "    def flattener(x):\n",
        "      x = K.reshape(x, [-1,x.shape[1]*x.shape[3]])\n",
        "      return x\n",
        "      \n",
        "    def flattener_output_shape(input_shape):\n",
        "      shape = list(input_shape)\n",
        "      new_shape = (shape[0],3*shape[3])\n",
        "      return new_shape\n",
        "      \n",
        "    inputs = Input(shape=(self.sequence_length, sentence_length), dtype='int32')\n",
        "    cnn_output = []\n",
        "    for ind in range(self.sequence_length):\n",
        "      \n",
        "      local_input = Lambda(slicer, output_shape=slicer_output_shape, arguments={\"index\":ind})(inputs) # Batch, word_indices\n",
        "\t\t\t\n",
        "\t\t\t#cnn-sent\n",
        "      emb_output = embedding(local_input)\n",
        "      reshape = Lambda(reshaper)(emb_output)\n",
        "      concatenated_tensor = Concatenate(axis=1)([maxpool_0(conv_0(reshape)), maxpool_1(conv_1(reshape)), maxpool_2(conv_2(reshape))])\n",
        "      flatten = Lambda(flattener, output_shape=flattener_output_shape,)(concatenated_tensor)\n",
        "      dense_output = dense_func(flatten)\n",
        "      dropout = Dropout(0.5)(dense_output)\n",
        "      cnn_output.append(dropout)\n",
        "    \n",
        "    def stack(x):\n",
        "      return K.stack(x, axis=1)\n",
        "    cnn_outputs = Lambda(stack)(cnn_output)\n",
        "    \n",
        "    masked = Masking(mask_value =0)(cnn_outputs)\n",
        "    lstm = Bidirectional(LSTM(300, activation='relu', return_sequences = True, dropout=0.3))(masked)\n",
        "    lstm = Bidirectional(LSTM(300, activation='relu', return_sequences = True, dropout=0.3), name=\"utter\")(lstm)\n",
        "    output = TimeDistributed(Dense(self.classes,activation='softmax'))(lstm)\n",
        "    \n",
        "    model = Model(inputs, output)\n",
        "    return model\n",
        "\n",
        "  def train_model(self,ar):\n",
        "    \n",
        "    print(\"WELCOME TO TRAIN MODEL\")\n",
        "    checkpoint = ModelCheckpoint(self.PATH, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "    \n",
        "    if ar== \"audio\":\n",
        "      model = self.get_audio_model()\n",
        "      model.compile(optimizer='adadelta', loss='categorical_crossentropy', sample_weight_mode='temporal')\n",
        "    elif ar == \"text\":\n",
        "      model = self.get_text_model()\n",
        "      model.compile(optimizer='adam', loss='categorical_crossentropy', sample_weight_mode='temporal')\n",
        "    elif ar == \"bimodal\":\n",
        "      model = self.get_bimodal_model()\n",
        "      model.compile(optimizer='adam', loss='categorical_crossentropy', sample_weight_mode='temporal')\n",
        "      \n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "    model.fit(self.train_x, self.train_y,\n",
        "\t\t                epochs=self.epochs,\n",
        "\t\t                batch_size=self.batch_size,\n",
        "\t\t                sample_weight=self.train_mask,\n",
        "\t\t                shuffle=True, \n",
        "\t\t                callbacks=[early_stopping, checkpoint],\n",
        "\t\t                validation_data=(self.val_x, self.val_y, self.val_mask))\n",
        "    \n",
        "    self.model = model\n",
        "    self.test_model()\n",
        "    \n",
        "  \n",
        "  def test_model(self):\n",
        "    \n",
        "    print(\"WELCOME TO TEST MODEL\")\n",
        "\n",
        "    model = self.model\n",
        "    self.calc_test_result(model.predict(self.test_x), self.test_y, self.test_mask)\n",
        "\n",
        "  def get_bimodal_model(self):\n",
        "    \n",
        "    self.epochs = 20\n",
        "    self.batch_size = 10\n",
        "    \n",
        "    self.embedding_dim = self.train_x.shape[2]\n",
        "    print(\"Creating Model...\")\n",
        "    \n",
        "    inputs = Input(shape=(self.sequence_length, self.embedding_dim), dtype='float32')\n",
        "    masked = Masking(mask_value =0)(inputs)\n",
        "    lstm = Bidirectional(LSTM(300, activation='tanh', return_sequences = True, dropout=0.4), name=\"utter\")(masked)\n",
        "    output = TimeDistributed(Dense(self.classes,activation='softmax'))(lstm)\n",
        "    \n",
        "    model = Model(inputs, output)\n",
        "    return model\n",
        "\t\t\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmbI6w7ywh4E",
        "colab_type": "code",
        "outputId": "d1989d13-7260-491f-cbbd-176a169e345b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "\t# Setup argument parser\n",
        "\t\n",
        "\t# Check directory existence\n",
        "\tfor directory in [\"/content/drive/My Drive/mca/MCA_Project/data/pickles\", \"/content/drive/My Drive/mca/MCA_Project/data/models\"]:\n",
        "\t\tif not os.path.exists(directory):\n",
        "\t\t    os.makedirs(directory)\n",
        "\n",
        "\n",
        "\tN = bc_LSTM('text')\n",
        "\tN.load_data('text')\n",
        "\tN.train_model('text')\n",
        "\t\n",
        "\t#N = bc_LSTM('audio')\n",
        "\t#N.load_data('audio')\n",
        "\t#N.train_model('audio')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data\n",
            "Labels used for this classification: {'neutral': 0, 'sadness': 1, 'joy': 2, 'anger': 3}\n",
            "WELCOME TO TEXT DATA ADDITION\n",
            "WELCOME TO TRAIN MODEL\n",
            "Creating Model...\n",
            "Train on 666 samples, validate on 114 samples\n",
            "Epoch 1/2\n",
            "666/666 [==============================] - 108s 163ms/step - loss: 0.2131 - val_loss: 0.3836\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.38355, saving model to /content/drive/My Drive/mca/MCA_Project/data/models/text_weights_emotion.hdf5\n",
            "Epoch 2/2\n",
            "666/666 [==============================] - 105s 157ms/step - loss: 0.2047 - val_loss: 0.3944\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.38355\n",
            "WELCOME TO TEST MODEL\n",
            "Confusion Matrix :\n",
            "[[661 559  33   3]\n",
            " [119  81   6   2]\n",
            " [168 168  65   1]\n",
            " [186 133  21   5]]\n",
            "Classification Report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5829    0.5263    0.5531      1256\n",
            "           1     0.0861    0.3894    0.1410       208\n",
            "           2     0.5200    0.1617    0.2467       402\n",
            "           3     0.4545    0.0145    0.0281       345\n",
            "\n",
            "    accuracy                         0.3673      2211\n",
            "   macro avg     0.4109    0.2730    0.2422      2211\n",
            "weighted avg     0.5047    0.3673    0.3767      2211\n",
            "\n",
            "Weighted FScore: \n",
            "  (0.5046926339505533, 0.3672546359113523, 0.37671817728460955, None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPl3EP1CESYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_bimodal_model(self):\n",
        "\n",
        "\t\t# Modality specific hyperparameters\n",
        "\t\tself.epochs = 20\n",
        "\t\tself.batch_size = 10\n",
        "\n",
        "\t\t# Modality specific parameters\n",
        "\t\tself.embedding_dim = self.train_x.shape[2]\n",
        "\n",
        "\t\tprint(\"Creating Model...\")\n",
        "\t\t\n",
        "\t\tinputs = Input(shape=(self.sequence_length, self.embedding_dim), dtype='float32')\n",
        "\t\tmasked = Masking(mask_value =0)(inputs)\n",
        "\t\tlstm = Bidirectional(LSTM(300, activation='tanh', return_sequences = True, dropout=0.4), name=\"utter\")(masked)\n",
        "\t\toutput = TimeDistributed(Dense(self.classes,activation='softmax'))(lstm)\n",
        "\n",
        "\t\tmodel = Model(inputs, output)\n",
        "\t\treturn model\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}