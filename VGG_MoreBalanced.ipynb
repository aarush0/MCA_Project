{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_MoreBalanced.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Arc6Dss6kRHI",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#from tensorflow.keras.models import Sequential\n",
        "#from tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "import argparse\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Lambda, LSTM, TimeDistributed, Masking, Bidirectional\n",
        "from tensorflow.keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os, pickle\n",
        "import numpy as np\n",
        "'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os, sys\n",
        "from collections import Counter, defaultdict\n",
        "from functools import cmp_to_key\n",
        "import argparse\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Lambda, LSTM, TimeDistributed, Masking, Bidirectional, concatenate, Layer\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate, Dot\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model, load_model\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import keras\n",
        "#from keras_multi_head import MultiHead\n",
        "#from keras_multi_head import MultiHeadAttention\n",
        "from datetime import datetime\n",
        "#from keras_self_attention import SeqSelfAttention\n",
        "import tensorflow as tf\n",
        "from keras import regularizers\n",
        "\n",
        "x = pickle.load(open(\"/content/drive/My Drive/mca/MCA_Project/data/pickles/data_{}.p\".format(\"emotion\"),\"rb\"))\n",
        "revs, W, word_idx_map, vocab, _, label_index = x[0], x[1], x[2], x[3], x[4], x[5]\n",
        "\n",
        "def get_word_indices(data_x):\n",
        "  length = len(data_x.split())\n",
        "  return np.array([word_idx_map[word] for word in data_x.split()] + [0]*(50-length))[:50]\n",
        "\n",
        "def comp_id(x, y):\n",
        "  xd = int(x[:x.find('_')])\n",
        "  xu = int(x[x.find('_')+1:])\n",
        "\n",
        "  yd = int(y[:y.find('_')])\n",
        "  yu = int(y[y.find('_')+1:])\n",
        "\n",
        "  if xd != yd:\n",
        "    return xd - yd\n",
        "  else:\n",
        "    return xu - yu\n",
        "\n",
        "\n",
        "def preprocess():\n",
        "\n",
        "  train_data, val_data, test_data = {},{},{}\n",
        "\n",
        "  counts_train = np.zeros((5,1))\n",
        "  counts_test = np.zeros((5,1))\n",
        "  counts_val = np.zeros((5,1))\n",
        "\n",
        "  tr_labels, v_labels, ts_labels = {}, {}, {}\n",
        "\n",
        "  for i in range(len(revs)):\n",
        "\n",
        "    utterance_id = revs[i]['dialog']+\"_\"+revs[i]['utterance']\n",
        "    \n",
        "    sentence_word_indices = get_word_indices(revs[i]['text'])\n",
        "    \n",
        "    label = label_index[revs[i]['y']]\n",
        "\n",
        "    if label != 0 and label != 3 and label != 4 and label != 6:\n",
        "      continue\n",
        "\n",
        "    if label == 0:\n",
        "      label = 0\n",
        "    elif label == 3:\n",
        "      label = 1\n",
        "    elif label == 4:\n",
        "      label = 2\n",
        "    else:\n",
        "      label = 3 \n",
        "\n",
        "    if revs[i]['split']==\"train\":\n",
        "      tr_labels[utterance_id] = label\n",
        "    if revs[i]['split']==\"val\":\n",
        "      v_labels[utterance_id] = label\n",
        "    if revs[i]['split']==\"test\":\n",
        "      ts_labels[utterance_id] = label\n",
        "\n",
        "    if revs[i]['split']==\"train\" and counts_train[label] > 683:\n",
        "      continue\n",
        "\n",
        "    if revs[i]['split']==\"train\":\n",
        "        train_data[utterance_id]=(sentence_word_indices,label)\n",
        "        counts_train[label] += 1\n",
        "    elif revs[i]['split']==\"val\":\n",
        "        val_data[utterance_id]=(sentence_word_indices,label)\n",
        "        counts_val[label] += 1\n",
        "    elif revs[i]['split']==\"test\":\n",
        "        test_data[utterance_id]=(sentence_word_indices,label)\n",
        "        counts_test[label] += 1\n",
        "\n",
        "  dialogs = []\n",
        "  utrs = -1\n",
        "  d_cur = -1\n",
        "\n",
        "  t_d = {}\n",
        "  t_map = {}\n",
        "  sorted_tr_keys = sorted(train_data.keys(), key=cmp_to_key(comp_id))\n",
        "\n",
        "  for i in sorted_tr_keys:\n",
        "    d = i[:i.find('_')]\n",
        "    u = i[i.find('_') + 1:]\n",
        "    ouid = d + '_' + u\n",
        "\n",
        "    if d not in dialogs:\n",
        "      d_cur += 1\n",
        "      utrs = 0\n",
        "      dialogs.append(d)\n",
        "    else:\n",
        "      utrs += 1\n",
        "\n",
        "    df = d_cur\n",
        "    uf = utrs\n",
        "\n",
        "    uid = str(df) +'_' + str(uf)\n",
        "    t_d[uid] = train_data[i]\n",
        "\n",
        "    t_map[uid] = ouid\n",
        "\n",
        "  dialogs = []\n",
        "  utrs = -1\n",
        "  d_cur = -1\n",
        "\n",
        "  v_d = {}\n",
        "  v_map = {}\n",
        "  sorted_val_keys = sorted(val_data.keys(), key=cmp_to_key(comp_id))\n",
        "\n",
        "  for i in sorted_val_keys:\n",
        "    d = i[:i.find('_')]\n",
        "    u = i[i.find('_') + 1:]\n",
        "    ouid = d + '_' + u\n",
        "\n",
        "    if d not in dialogs:\n",
        "      d_cur += 1\n",
        "      utrs = 0\n",
        "      dialogs.append(d)\n",
        "    else:\n",
        "      utrs += 1\n",
        "\n",
        "    df = d_cur\n",
        "    uf = utrs\n",
        "\n",
        "    uid = str(df) +'_' + str(uf)\n",
        "    v_d[uid] = val_data[i]\n",
        "    v_map[uid] = ouid\n",
        "\n",
        "  dialogs = []\n",
        "  utrs = -1\n",
        "  d_cur = -1\n",
        "\n",
        "  ts_d = {}\n",
        "  ts_map = {}\n",
        "  sorted_ts_keys = sorted(test_data.keys(), key=cmp_to_key(comp_id))\n",
        "\n",
        "  for i in sorted_ts_keys:\n",
        "    d = i[:i.find('_')]\n",
        "    u = i[i.find('_') + 1:]\n",
        "    ouid = d + '_' + u\n",
        "\n",
        "    if d not in dialogs:\n",
        "      d_cur += 1\n",
        "      utrs = 0\n",
        "      dialogs.append(d)\n",
        "    else:\n",
        "      utrs += 1\n",
        "\n",
        "    df = d_cur\n",
        "    uf = utrs\n",
        "\n",
        "    uid = str(df) +'_' + str(uf)\n",
        "    ts_d[uid] = test_data[i]\n",
        "    ts_map[uid] = ouid\n",
        "  \n",
        "  return t_d, v_d, ts_d, t_map, v_map, ts_map, tr_labels, v_labels, ts_labels\n",
        "\n",
        "\n",
        "#preprocess()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHUDVw50T3Zy",
        "colab_type": "code",
        "outputId": "ee8e78bf-0b36-4e01-942d-844752d93317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz0qBRjPfmaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "max_length=50 # Maximum length of the sentence\n",
        "\n",
        "class Dataloader:\n",
        "    \n",
        "    def __init__(self, mode=None):\n",
        "\n",
        "        self.MODE = mode # Sentiment or Emotion classification mode\n",
        "        self.max_l = max_length\n",
        "\n",
        "\n",
        "        x = pickle.load(open(\"/content/drive/My Drive/mca/MCA_Project/data/pickles/data_{}.p\".format(self.MODE.lower()),\"rb\"))\n",
        "        self.revs, self.W, self.word_idx_map, self.vocab, _, label_index = x[0], x[1], x[2], x[3], x[4], x[5]\n",
        "        \n",
        "        self.num_classes = 4\n",
        "        print(\"Labels used for this classification: {'neutral': 0, 'sadness': 1, 'joy': 2, 'anger': 3}\")\n",
        "\n",
        "        self.train_data, self.val_data, self.test_data, self.tr_map, self.v_map, self.ts_map, self.tr_labels, self.v_labels, self.ts_labels = preprocess()\n",
        "\n",
        "        # Creating dialogue:[utterance_1, utterance_2, ...] ids\n",
        "        self.train_dialogue_ids = self.get_dialogue_ids(self.train_data.keys())\n",
        "        self.val_dialogue_ids = self.get_dialogue_ids(self.val_data.keys())\n",
        "        self.test_dialogue_ids = self.get_dialogue_ids(self.test_data.keys())\n",
        "\n",
        "        # Max utternance in a dialog in the dataset\n",
        "        self.max_utts = self.get_max_utts(self.train_dialogue_ids, self.val_dialogue_ids, self.test_dialogue_ids)\n",
        "\n",
        "    def get_dialogue_ids(self, keys):\n",
        "        ids=defaultdict(list)\n",
        "        for key in keys:\n",
        "            ids[key.split(\"_\")[0]].append(int(key.split(\"_\")[1]))\n",
        "        for ID, utts in ids.items():\n",
        "            ids[ID]=[str(utt) for utt in sorted(utts)]\n",
        "        return ids\n",
        "\n",
        "    def get_max_utts(self, train_ids, val_ids, test_ids):\n",
        "        max_utts_train = max([len(train_ids[vid]) for vid in train_ids.keys()])\n",
        "        max_utts_val = max([len(val_ids[vid]) for vid in val_ids.keys()])\n",
        "        max_utts_test = max([len(test_ids[vid]) for vid in test_ids.keys()])\n",
        "        return np.max([max_utts_train, max_utts_val, max_utts_test])\n",
        "\n",
        "    def get_one_hot(self, label):\n",
        "        label_arr = [0]*self.num_classes\n",
        "        label_arr[label]=1\n",
        "        return label_arr[:]\n",
        "\n",
        "    def get_dialogue_text_embs(self):\n",
        "        key = list(self.train_data.keys())[0]\n",
        "        \n",
        "        pad = [0]*len(self.train_data[key][0])\n",
        "\n",
        "        def get_emb(dialogue_id, local_data):\n",
        "            dialogue_text = []\n",
        "            for vid in dialogue_id.keys():\n",
        "                local_text = []\n",
        "                for utt in dialogue_id[vid]:\n",
        "                    local_text.append(local_data[vid+\"_\"+str(utt)][0][:])\n",
        "                for _ in range(self.max_utts-len(local_text)):\n",
        "                    local_text.append(pad[:])\n",
        "                dialogue_text.append(local_text[:self.max_utts])\n",
        "            return np.array(dialogue_text)\n",
        "\n",
        "        self.train_dialogue_features = get_emb(self.train_dialogue_ids, self.train_data)\n",
        "        self.val_dialogue_features = get_emb(self.val_dialogue_ids, self.val_data)\n",
        "        self.test_dialogue_features = get_emb(self.test_dialogue_ids, self.test_data)\n",
        "\n",
        "    def get_dialogue_labels(self):\n",
        "\n",
        "        def get_labels(ids, data):\n",
        "            dialogue_label=[]\n",
        "\n",
        "            for vid, utts in ids.items():\n",
        "                local_labels=[]\n",
        "                for utt in utts:\n",
        "                    local_labels.append(self.get_one_hot(data[vid+\"_\"+str(utt)][1]))\n",
        "                for _ in range(self.max_utts-len(local_labels)):\n",
        "                    local_labels.append(self.get_one_hot(1)) # Dummy label\n",
        "                dialogue_label.append(local_labels[:self.max_utts])\n",
        "            return np.array(dialogue_label)\n",
        "\n",
        "        self.train_dialogue_label=get_labels(self.train_dialogue_ids, self.train_data)\n",
        "        self.val_dialogue_label=get_labels(self.val_dialogue_ids, self.val_data)\n",
        "        self.test_dialogue_label=get_labels(self.test_dialogue_ids, self.test_data)\n",
        "\n",
        "    def get_dialogue_labels_audio(self):\n",
        "\n",
        "        def get_labels(ids, data, Map, map1):\n",
        "            dialogue_label=[]\n",
        "\n",
        "            for vid, utts in ids.items():\n",
        "                local_labels=[]\n",
        "                for utt in utts:\n",
        "                    local_labels.append(self.get_one_hot(Map[map1[vid+'_'+str(utt)]]))\n",
        "                for _ in range(self.max_utts-len(local_labels)):\n",
        "                    local_labels.append(self.get_one_hot(1)) # Dummy label\n",
        "                dialogue_label.append(local_labels[:self.max_utts])\n",
        "            return np.array(dialogue_label)\n",
        "\n",
        "        self.train_dialogue_label=get_labels(self.train_dialogue_ids, self.train_data, self.tr_labels, self.tr_map)\n",
        "        self.val_dialogue_label=get_labels(self.val_dialogue_ids, self.val_data, self.v_labels, self.v_map)\n",
        "        self.test_dialogue_label=get_labels(self.test_dialogue_ids, self.test_data, self.ts_labels, self.ts_map)\n",
        "\n",
        "        \n",
        "    def get_dialogue_lengths(self):\n",
        "\n",
        "        self.train_dialogue_length, self.val_dialogue_length, self.test_dialogue_length=[], [], []\n",
        "        for vid, utts in self.train_dialogue_ids.items():\n",
        "            self.train_dialogue_length.append(len(utts))\n",
        "        for vid, utts in self.val_dialogue_ids.items():\n",
        "            self.val_dialogue_length.append(len(utts))\n",
        "        for vid, utts in self.test_dialogue_ids.items():\n",
        "            self.test_dialogue_length.append(len(utts))\n",
        "\n",
        "    def get_masks(self):\n",
        "\n",
        "        self.train_mask = np.zeros((len(self.train_dialogue_length), self.max_utts), dtype='int')\n",
        "        for i in range(len(self.train_dialogue_length)):\n",
        "            self.train_mask[i,:self.train_dialogue_length[i]]=1\n",
        "        self.val_mask = np.zeros((len(self.val_dialogue_length), self.max_utts), dtype='int')\n",
        "        for i in range(len(self.val_dialogue_length)):\n",
        "            self.val_mask[i,:self.val_dialogue_length[i]]=1\n",
        "        self.test_mask = np.zeros((len(self.test_dialogue_length), self.max_utts), dtype='int')\n",
        "        for i in range(len(self.test_dialogue_length)):\n",
        "            self.test_mask[i,:self.test_dialogue_length[i]]=1\n",
        "        \n",
        "    def load_text_data(self, ):\n",
        "\n",
        "        self.get_dialogue_text_embs()\n",
        "        self.get_dialogue_lengths()\n",
        "        self.get_dialogue_labels()\n",
        "        self.get_masks()\n",
        "\n",
        "    def load_audio_data(self, ):\n",
        "\n",
        "        AUDIO_PATH = \"/content/drive/My Drive/mca/MCA_Project/data/pickles/audio_embeddings_feature_selection_{}.pkl\".format(self.MODE.lower())\n",
        "        self.train_audio_emb, self.val_audio_emb, self.test_audio_emb = pickle.load(open(AUDIO_PATH,\"rb\"))\n",
        "\n",
        "        self.get_dialogue_audio_embs()\n",
        "        self.get_dialogue_lengths()\n",
        "        self.get_dialogue_labels_audio()\n",
        "        self.get_masks()\n",
        "\n",
        "    def get_dialogue_audio_embs(self):\n",
        "        key = list(self.train_audio_emb.keys())[0]\n",
        "        pad = [0]*len(self.train_audio_emb[key])\n",
        "\n",
        "        def get_emb(dialogue_id, audio_emb, Map):\n",
        "            dialogue_audio=[]\n",
        "            for vid in dialogue_id.keys():\n",
        "                local_audio=[]\n",
        "                for utt in dialogue_id[vid]:\n",
        "                    try:\n",
        "                        local_audio.append(audio_emb[Map[vid+\"_\"+str(utt)]][:])\n",
        "                    except:\n",
        "                        print(vid+\"_\"+str(utt))\n",
        "                        local_audio.append(pad[:])\n",
        "                for _ in range(self.max_utts-len(local_audio)):\n",
        "                    local_audio.append(pad[:])\n",
        "                dialogue_audio.append(local_audio[:self.max_utts])\n",
        "            return np.array(dialogue_audio)\n",
        "\n",
        "        self.train_dialogue_features = get_emb(self.train_dialogue_ids, self.train_audio_emb, self.tr_map)\n",
        "        self.val_dialogue_features = get_emb(self.val_dialogue_ids, self.val_audio_emb, self.v_map)\n",
        "        self.test_dialogue_features = get_emb(self.test_dialogue_ids, self.test_audio_emb, self.ts_map)\n",
        "\n",
        "\n",
        "    def load_video_data(self, ):\n",
        "\n",
        "        self.val_video_emb = np.load('/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/vgg_x_dev.pkl', allow_pickle='True')\n",
        "        self.train_video_emb = np.load('/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/vgg_x_train.pkl', allow_pickle='True')\n",
        "        self.test_video_emb = np.load('/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/vgg_x_test.pkl', allow_pickle='True')\n",
        "        self.vid_to_row_train = np.load('/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/dia_frames_train_dia_to_row.pkl', allow_pickle=True).item()\n",
        "        self.vid_to_row_val = np.load('/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/dia_frames_dev_dia_to_row.pkl', allow_pickle=True).item()\n",
        "        self.vid_to_row_test = np.load('/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/dia_frames_test_dia_to_row.pkl', allow_pickle=True).item()\n",
        "        \n",
        "        self.val_video_emb = self.val_video_emb.reshape(self.val_video_emb.shape[0], 2048)\n",
        "        self.train_video_emb = self.train_video_emb.reshape(self.train_video_emb.shape[0], 2048)\n",
        "        self.test_video_emb = self.test_video_emb.reshape(self.test_video_emb.shape[0], 2048)\n",
        "        #print(self.val_video_emb.shape, val_video_emb)\n",
        "        '''\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "        \n",
        "        def normalize(emb):\n",
        "          X = []\n",
        "          scaler = StandardScaler()\n",
        "\n",
        "          for e in sorted(emb.keys()):\n",
        "            X.append(emb[e])\n",
        "\n",
        "          X = scaler.fit_transform(X)\n",
        "\n",
        "          for n, e in enumerate(sorted(emb.keys())):\n",
        "            emb[e] = X[n]\n",
        "\n",
        "          return emb\n",
        "          '''\n",
        "\n",
        "        #self.val_video_emb = normalize(self.val_video_emb)\n",
        "        #self.test_video_emb = normalize(self.test_video_emb)\n",
        "        #self.train_video_emb = normalize(self.train_video_emb)\n",
        "        \n",
        "        self.get_dialogue_video_embs()\n",
        "        self.get_dialogue_lengths()\n",
        "        self.get_dialogue_labels_video()\n",
        "        self.get_masks()\n",
        "\n",
        "    def get_dialogue_video_embs(self):\n",
        "        #key = list(self.train_video_emb.keys())[0]\n",
        "        pad = [0]*2048\n",
        "        \n",
        "        #print(len(pad))\n",
        "        \n",
        "        def get_emb(dialogue_id, video_emb, Map, to_row):\n",
        "            dialogue_video=[]\n",
        "            ct = 0\n",
        "            for vid in dialogue_id.keys():\n",
        "                local_video=[]\n",
        "                for utt in dialogue_id[vid]:\n",
        "                    try:\n",
        "                        #local_video.append(video_emb[Map[vid+\"_\"+str(utt)]][:])\n",
        "                        key = Map[vid + \"_\" + str(utt)]\n",
        "                        sm = np.zeros((2048,))\n",
        "                        for k in to_row[key]:\n",
        "                          tmp = video_emb[k, :]\n",
        "                          sm += tmp\n",
        "\n",
        "                        sm = sm/len(to_row[key])\n",
        "\n",
        "                        sm = list(sm)\n",
        "\n",
        "                        local_video.append(sm)\n",
        "                    except:\n",
        "                        #print(vid+\"_\"+str(utt), end = \" \")\n",
        "                        local_video.append(pad[:])\n",
        "                        ct += 1\n",
        "                    #print(len(local_video))\n",
        "                for _ in range(self.max_utts-len(local_video)):\n",
        "                    local_video.append(pad[:])\n",
        "                dialogue_video.append(local_video[:self.max_utts])\n",
        "                #print(len(dialogue_video), len(dialogue_video[0]), len(dialogue_video[0][0]))\n",
        "\n",
        "            #print(np.array(dialogue_video).shape)\n",
        "            dialogue_video = np.array(dialogue_video)\n",
        "            dialogue_video = np.nan_to_num(dialogue_video)\n",
        "\n",
        "            print(np.argwhere(np.isnan(dialogue_video)))\n",
        "            return dialogue_video\n",
        "\n",
        "        print(\"train\")\n",
        "        self.train_dialogue_features = get_emb(self.train_dialogue_ids, self.train_video_emb, self.tr_map, self.vid_to_row_train)\n",
        "        print(\"val\")\n",
        "        self.val_dialogue_features = get_emb(self.val_dialogue_ids, self.val_video_emb, self.v_map, self.vid_to_row_val)\n",
        "        print(\"test\")\n",
        "        self.test_dialogue_features = get_emb(self.test_dialogue_ids, self.test_video_emb, self.ts_map, self.vid_to_row_test)\n",
        "        print(\"\")\n",
        "\n",
        "    def get_dialogue_labels_video(self):\n",
        "\n",
        "        def get_labels(ids, data, Map, map1):\n",
        "            dialogue_label=[]\n",
        "\n",
        "            for vid, utts in ids.items():\n",
        "                local_labels=[]\n",
        "                for utt in utts:\n",
        "                    local_labels.append(self.get_one_hot(Map[map1[vid+'_'+str(utt)]]))\n",
        "                for _ in range(self.max_utts-len(local_labels)):\n",
        "                    local_labels.append(self.get_one_hot(1)) # Dummy label\n",
        "                dialogue_label.append(local_labels[:self.max_utts])\n",
        "            return np.array(dialogue_label)\n",
        "\n",
        "        self.train_dialogue_label=get_labels(self.train_dialogue_ids, self.train_data, self.tr_labels, self.tr_map)\n",
        "        self.val_dialogue_label=get_labels(self.val_dialogue_ids, self.val_data, self.v_labels, self.v_map)\n",
        "        self.test_dialogue_label=get_labels(self.test_dialogue_ids, self.test_data, self.ts_labels, self.ts_map)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FRuRPuMHt_uP",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "class Network1:\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tself.classification_mode = \"emotion\"\n",
        "\t\t\n",
        "\tdef load_data(self,m):\n",
        "    \n",
        "\t\tprint('Loading data')\n",
        "    \n",
        "\t\tself.data = Dataloader(mode = self.classification_mode)\n",
        "    \n",
        "\t\tif m == \"text\":\n",
        "\t\t\tself.data.load_text_data()\n",
        "\t\telif m == \"audio\":\n",
        "\t\t\tself.data.load_audio_data()\n",
        "\t\telif m ==\"video\":\n",
        "\t\t\tself.data.load_video_data()\n",
        "\t\telse:\n",
        "\t\t\texit()\n",
        "    \n",
        "\t\tself.train_x = self.data.train_dialogue_features\n",
        "\t\tself.val_x = self.data.val_dialogue_features\n",
        "\t\tself.test_x = self.data.test_dialogue_features\n",
        "    \n",
        "\t\tself.train_y = self.data.train_dialogue_label\n",
        "\t\tself.val_y = self.data.val_dialogue_label\n",
        "\t\tself.test_y = self.data.test_dialogue_label\n",
        "    \n",
        "\t\tself.train_mask = self.data.train_mask\n",
        "\t\tself.val_mask = self.data.val_mask\n",
        "\t\tself.test_mask = self.data.test_mask\n",
        "    \n",
        "\t\tself.train_id = self.data.train_dialogue_ids.keys()\n",
        "\t\tself.val_id = self.data.val_dialogue_ids.keys()\n",
        "\t\tself.test_id = self.data.test_dialogue_ids.keys()\n",
        "    \n",
        "\t\tself.sequence_length = self.train_x.shape[1]\n",
        "\n",
        "\t\tself.classes = self.train_y.shape[2]\n",
        "    \n",
        "\t\tself.epochs = 15\n",
        "\t\tself.batch_size = 50\n",
        "\n",
        "\t\tif m == \"text\":\n",
        "\t\t\tself.train_x_text = self.train_x\n",
        "\t\t\tself.val_x_text = self.val_x\n",
        "\t\t\tself.test_x_text = self.test_x\n",
        "\n",
        "\t\t\tself.train_y_text = self.train_y\n",
        "\t\t\tself.val_y_text = self.val_y \n",
        "\t\t\tself.test_y_text = self.test_y \n",
        "\t\t\t\n",
        "\t\t\tself.train_mask_text = self.train_mask \n",
        "\t\t\tself.val_mask_text = self.val_mask \n",
        "\t\t\tself.test_mask_text = self.test_mask\n",
        "\t\t\t\n",
        "\t\t\tself.train_id_text = self.train_id \n",
        "\t\t\tself.val_id_text = self.val_id \n",
        "\t\t\tself.test_id_text = self.test_id \n",
        "\n",
        "\t\t\tself.sequence_length_text = self.sequence_length\n",
        "\n",
        "\t\tif m == \"audio\":\n",
        "\t\t\tself.train_x_audio = self.train_x\n",
        "\t\t\tself.val_x_audio = self.val_x\n",
        "\t\t\tself.test_x_audio = self.test_x\n",
        "\n",
        "\t\t\tself.train_y_audio = self.train_y\n",
        "\t\t\tself.val_y_audio = self.val_y \n",
        "\t\t\tself.test_y_audio = self.test_y \n",
        "\t\t\t\n",
        "\t\t\tself.train_mask_audio = self.train_mask \n",
        "\t\t\tself.val_mask_audio = self.val_mask \n",
        "\t\t\tself.test_mask_audio = self.test_mask\n",
        "\t\t\t\n",
        "\t\t\tself.train_id_audio = self.train_id \n",
        "\t\t\tself.val_id_audio = self.val_id \n",
        "\t\t\tself.test_id_audio = self.test_id \n",
        "\t\t\tself.sequence_length_audio = self.sequence_length\n",
        "\n",
        "\t\tif m == \"video\":\n",
        "\t\t\tself.train_x_video = self.train_x\n",
        "\t\t\tself.val_x_video = self.val_x\n",
        "\t\t\tself.test_x_video = self.test_x\n",
        "\n",
        "\t\t\tself.train_y_video = self.train_y\n",
        "\t\t\tself.val_y_video = self.val_y \n",
        "\t\t\tself.test_y_video = self.test_y \n",
        "\t\t\t\n",
        "\t\t\tself.train_mask_video = self.train_mask \n",
        "\t\t\tself.val_mask_video = self.val_mask \n",
        "\t\t\tself.test_mask_video = self.test_mask\n",
        "\t\t\t\n",
        "\t\t\tself.train_id_video = self.train_id \n",
        "\t\t\tself.val_id_video = self.val_id \n",
        "\t\t\tself.test_id_video = self.test_id \n",
        "\t\t\tself.sequence_length_video = self.sequence_length\n",
        "\n",
        "\tdef get_text_lstm(self):\n",
        "\t\tself.sentence_length = self.train_x.shape[2]\n",
        "    \n",
        "\t\tself.embedding_dim = self.data.W.shape[1]\n",
        "    \n",
        "\t\tself.vocabulary_size = self.data.W.shape[0]\n",
        "\t\t\n",
        "\t\tembedding = Embedding(input_dim=self.vocabulary_size, output_dim=self.embedding_dim, weights=[self.data.W], input_length=self.sentence_length, trainable=False)\n",
        "    \n",
        "\t\tdef slicer(x, index):\n",
        "\t\t\treturn x[:,K.constant(index, dtype='int32'),:]\n",
        "    \n",
        "\t\tdef slicer_output_shape(input_shape):\n",
        "\t\t\tshape = list(input_shape)\n",
        "\t\t\tassert len(shape) == 3  # batch, seq_len, sent_len\n",
        "\t\t\tnew_shape = (shape[0], shape[2])\n",
        "\t\t\treturn new_shape\n",
        "\n",
        "\t\tdef reshaper(x):\n",
        "\t\t\treturn K.expand_dims(x, axis=3)\n",
        "    \n",
        "\t\tdef flattener(x):\n",
        "\t\t\tx = K.reshape(x, [-1,x.shape[1]*x.shape[2]])\n",
        "\t\t\treturn x\n",
        "\n",
        "\t\tdef flattener_output_shape(input_shape):\n",
        "\t\t\tshape = list(input_shape)\n",
        "\t\t\tnew_shape = (shape[0], shape[2]*shape[1])\n",
        "\t\t\treturn new_shape\n",
        "\n",
        "\t\tinputs = Input(shape=(self.sequence_length, self.sentence_length), dtype='int32')\n",
        "\t\t\n",
        "\t\t\n",
        "\t\toutput = []\n",
        "\t\tfor ind in range(self.sequence_length):\n",
        "\t\t\tlocal_input = Lambda(slicer, output_shape=slicer_output_shape, arguments={\"index\":ind})(inputs) # Batch, word_indices\n",
        "\t\t\t#print(K.int_shape(local_input))\n",
        "\t\t\temb_output = embedding(local_input)\n",
        "\t\t\t#print(K.int_shape(emb_output))\n",
        "\t\t\treshape = Lambda(reshaper)(emb_output)\n",
        "\t\t\t#print(K.int_shape(reshape))\n",
        "\t\t\tflatten = Lambda(flattener, output_shape=flattener_output_shape,)(reshape)\n",
        "\n",
        "\t\t\toutput.append(flatten)\n",
        "\n",
        "\t\tdef stack(x):\n",
        "\t\t\treturn K.stack(x, axis=1)\n",
        "      \n",
        "\t\toutputs = Lambda(stack)(output)\n",
        "\t\tmasked = Masking(mask_value =0)(outputs)\n",
        "\t\n",
        "\t\tlstm = Bidirectional(LSTM(150, activation='relu', return_sequences = True, dropout=0.2, recurrent_dropout=0.2), name = 'lstm_t')(masked)\n",
        "\t\t\n",
        "\t\tself.text_lstm_layer = lstm\n",
        "\t\t\n",
        "\t\tlstm1 = Bidirectional(LSTM(150, activation='relu', return_sequences = True, dropout=0.2, recurrent_dropout=0.2), name=\"utter_t\")(lstm)\n",
        "\t\t\n",
        "\t\t#att_text = SeqSelfAttention(attention_activation='sigmoid')(lstm)\n",
        "\t\t#att_text = AttentionDecoder(100, 300)(lstm)\n",
        "\n",
        "\t\toutput = TimeDistributed(Dense(self.classes,activation='softmax'))(lstm1)\n",
        "\n",
        "\t\tmodel = Model(inputs, output)\n",
        "\n",
        "\t\tmodel.summary()\n",
        "\n",
        "\t\tself.text_lstm =  model\n",
        "\n",
        "\t\treturn lstm, inputs\n",
        "\n",
        "\tdef get_audio_lstm(self):\n",
        "\n",
        "\t\tself.embedding_dim = self.train_x.shape[2]\n",
        "\n",
        "\t\tprint(\"Creating Model...\")\n",
        "\t\t\n",
        "\t\tinputs = Input(shape=(self.sequence_length, self.embedding_dim), dtype='float32')\n",
        "\t\tmasked = Masking(mask_value =0)(inputs)\n",
        "\t\tlstm = Bidirectional(LSTM(150, activation='tanh', return_sequences = True, dropout=0.2, recurrent_dropout=0.2), name='lstm_a')(masked)\n",
        "\t\tself.audio_lstm_layer = lstm\n",
        "\t\tlstm1 = Bidirectional(LSTM(150, activation='tanh', return_sequences = True, dropout=0.2, recurrent_dropout=0.2), name=\"utter_a\")(lstm)\n",
        "\t\toutput = TimeDistributed(Dense(self.classes,activation='softmax'))(lstm1)\n",
        "\n",
        "\t\tmodel = Model(inputs, output)\n",
        "\n",
        "\t\tself.audio_lstm = model\n",
        "\n",
        "\t\tmodel.summary()\n",
        "\n",
        "\t\treturn lstm, inputs\n",
        "\n",
        "\tdef get_video_lstm(self):\n",
        "\n",
        "\t\tself.embedding_dim = self.train_x.shape[2]\n",
        "\n",
        "\t\tprint(\"Creating Model...\")\n",
        "\t\t\n",
        "\t\tinputs = Input(shape=(self.sequence_length, self.embedding_dim), dtype='float32')\n",
        "\t\tmasked = Masking(mask_value =0)(inputs)\n",
        "\t\tlstm = Bidirectional(LSTM(150, activation='relu', return_sequences = True, dropout=0.2, recurrent_dropout=0.2),name='lstm_a')(masked)\n",
        "\t\tself.video_lstm_layer = lstm\n",
        "\t\tlstm1 = Bidirectional(LSTM(150, activation='relu', return_sequences = True, dropout=0.2, recurrent_dropout=0.2), name=\"utter_a\")(lstm)\n",
        "\t\t#output = TimeDistributed(Dense(self.classes,activation='softmax'))(lstm1)\n",
        "\t\t#lstm2 = Bidirectional(LSTM(100, activation='relu', return_sequences = True, dropout=0.2, recurrent_dropout=0.2), name=\"utter_b\")(lstm1)\n",
        "\t\toutput2 = TimeDistributed(Dense(self.classes,activation='softmax'))(lstm1)\n",
        "\n",
        "\t\tmodel = Model(inputs, output2)\n",
        "\t\n",
        "\t\t\n",
        "\t\tfilename = '/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/vgg_balanced_himanshu.sav'\n",
        "\t\tpickle.dump(model, open(filename, 'wb'))\n",
        "\t\n",
        "\t\tmodel.summary()\n",
        "\n",
        "\t\tself.video_lstm = model\n",
        "\n",
        "\t\t#model.summary()\n",
        "\n",
        "\t\treturn lstm, inputs\n",
        "\n",
        "\tdef get_final_model(self, tl, al, ti, ai):\n",
        "\t\tts = Concatenate(axis=-1)([tl, al])\n",
        "\t\tatt_layer = MultiHeadAttention(head_num=3,name='Multi-Head',)([tl, al, ts])\n",
        "\t\t\n",
        "\t\t#att_layer = MyAttention()([tl, al])\n",
        "\t\t\n",
        "\t\tconcat_output = Concatenate(axis=-1, name='concat_layer')([tl, att_layer])\n",
        "\t\t\n",
        "\t\tlstm = Bidirectional(LSTM(150, activation='relu', return_sequences = True, dropout=0.2, recurrent_dropout=0.2), name='lstm_f')(concat_output)\n",
        "\t\t\n",
        "\t\toutput = TimeDistributed(Dense(self.classes,activation='softmax'))(lstm)\n",
        "\t\n",
        "\t\tself.merged_model = Model([ti, ai], output)\n",
        "\t\n",
        "\tdef train_lstm(self, m):\n",
        "\t\tif m == 'text':\n",
        "\t\t\tmodel = self.text_lstm\n",
        "\t\telif m == 'audio':\n",
        "\t\t\tmodel = self.audio_lstm\n",
        "\t\telif m == 'video':\n",
        "\t\t\tmodel = self.video_lstm\n",
        "\n",
        "\t\t\n",
        "\t\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'], sample_weight_mode='temporal')\n",
        "\t\tearly_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\t\n",
        "\t\tlogdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "\t\ttensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "\t\tsummary = model.fit(self.train_x, self.train_y,\n",
        "\t\t                epochs=self.epochs,\n",
        "\t\t                batch_size=self.batch_size,\n",
        "\t\t                sample_weight=self.train_mask,\n",
        "\t\t                shuffle=True, \n",
        "\t\t                callbacks=[early_stopping, tensorboard_callback],\n",
        "\t\t                validation_data=(self.val_x, self.val_y, self.val_mask))\n",
        "\t\n",
        "\t\treturn summary\n",
        "\n",
        "\t\n",
        "\t\t#self.test_model(m)\n",
        "\t\n",
        "\tdef train_network(self):\n",
        "\t\tmodel = self.merged_model\n",
        "\t\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', sample_weight_mode='temporal')\n",
        "\t\tearly_stopping = EarlyStopping(monitor='loss', patience=10)\n",
        "\t\n",
        "\t\t#logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "\t\t#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "\t\tmodel.fit([self.train_x_text, self.train_x_audio], self.train_y,\n",
        "\t\t                epochs=self.epochs,\n",
        "\t\t                batch_size=self.batch_size,\n",
        "\t\t                shuffle=True, \n",
        "\t\t                callbacks=[early_stopping, tensorboard_callback],\n",
        "\t\t\t\t\t\t\t\t\t\tsample_weight = self.train_mask,\n",
        "\t\t\t\t\t\t\t\t\t\tvalidation_data=([self.val_x_text, self.val_x_audio], self.val_y, self.val_mask))\n",
        "\t\n",
        "\t\t\n",
        "\tdef test_model(self, m):\n",
        "\t\tif m == 'text':\n",
        "\t\t\tmodel = self.text_lstm\n",
        "\t\t\t#intermediate_layer_model = Model(input=model.input, output=model.get_layer(\"lstm_t\").output)\n",
        "\t\telif m == 'audio':\n",
        "\t\t\tmodel = self.audio_lstm\n",
        "\t\t\t#intermediate_layer_model = Model(input=model.input, output=model.get_layer(\"lstm_a\").output)\n",
        "\t\telif m == 'merged':\n",
        "\t\t\tmodel = self.merged_model\n",
        "\t\t\t#intermediate_layer_model = Model(input=model.input, output=model.get_layer(\"lstm_f\").output)\n",
        "\n",
        "\t\t'''\n",
        "\t\tintermediate_output_train = intermediate_layer_model.predict(self.train_x)\n",
        "\t\tintermediate_output_val = intermediate_layer_model.predict(self.val_x)\n",
        "\t\tintermediate_output_test = intermediate_layer_model.predict(self.test_x)\n",
        "\n",
        "\t\ttrain_emb, val_emb, test_emb = {}, {}, {}\n",
        "\t\tfor idx, ID in enumerate(self.train_id):\n",
        "\t\t    train_emb[ID] = intermediate_output_train[idx]\n",
        "\t\tfor idx, ID in enumerate(self.val_id):\n",
        "\t\t    val_emb[ID] = intermediate_output_val[idx]\n",
        "\t\tfor idx, ID in enumerate(self.test_id):\n",
        "\t\t    test_emb[ID] = intermediate_output_test[idx]\n",
        "\t\t'''\n",
        "\t\tcalc_test_result(model.predict(self.test_x), self.test_y, self.test_mask)\n",
        "\t\t#calc_test_result(model.predict(self.train_x), self.train_y, self.train_mask)\n",
        "\t\t\n",
        "def calc_test_result(pred_label, test_label, test_mask):\n",
        "\n",
        "\t\ttrue_label=[]\n",
        "\t\tpredicted_label=[]\n",
        "\n",
        "\t\tfor i in range(pred_label.shape[0]):\n",
        "\t\t\tfor j in range(pred_label.shape[1]):\n",
        "\t\t\t\tif test_mask[i,j]==1:\n",
        "\t\t\t\t\ttrue_label.append(np.argmax(test_label[i,j] ))\n",
        "\t\t\t\t\tpredicted_label.append(np.argmax(pred_label[i,j] ))\n",
        "\t\t\n",
        "\t\n",
        "\t\tprint(\"Confusion Matrix :\")\n",
        "\t\tprint(confusion_matrix(true_label, predicted_label))\n",
        "\t\tprint(\"Classification Report :\")\n",
        "\t\tprint(classification_report(true_label, predicted_label, digits=4))\n",
        "\t\tprint('Weighted FScore: \\n ', precision_recall_fscore_support(true_label, predicted_label, average='weighted'))\n",
        "\t\n",
        "\t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BP8gFqo3uCZF",
        "outputId": "ee52e6ce-78eb-4a5a-be9a-e5bc2298275e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "source": [
        "def plot(summary):\n",
        "  l = summary.history['loss']\n",
        "  vl = summary.history['val_loss']\n",
        "\n",
        "  #a = summary.history['acc']\n",
        "  #va = summary.history['val_acc']\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  e = len(l)\n",
        "  iterations = list(range(e))\n",
        "  plt.plot(iterations, l, color='blue', label='train')\n",
        "  plt.plot(iterations, vl, color='red', label ='val')\n",
        "  plt.legend(loc='best')\n",
        "    \n",
        "  #axs[1].plot(iterations, a, color='red')\n",
        "  #axs[1].plot(iterations, va, color='red')\n",
        "\n",
        "\n",
        "\n",
        "N = Network1()\n",
        "\n",
        "N.load_data(\"video\")\n",
        "tl, ti = N.get_video_lstm()\n",
        "s = N.train_lstm(\"video\")\n",
        "\n",
        "model = N.video_lstm\n",
        "calc_test_result(model.predict(N.test_x_video), N.test_y, N.test_mask)\n",
        "calc_test_result(model.predict(N.train_x_video), N.train_y, N.train_mask)\n",
        "\n",
        "#N.load_data(\"text\")\n",
        "#tl, ti = N.get_text_lstm()\n",
        "#N.train_lstm(\"text\")\n",
        "\n",
        "#print(N.train_x, N.train_y)\n",
        "\n",
        "#N.load_data(\"audio\")\n",
        "#al, ai = N.get_audio_lstm()\n",
        "#N.train_lstm(\"audio\")\n",
        "\n",
        "#N.get_final_model(tl, al, ti, ai)\n",
        "\n",
        "#N.train_network()\n",
        "\n",
        "#model = N.merged_model\n",
        "#calc_test_result(model.predict([N.test_x_text, N.test_x_audio]), N.test_y, N.test_mask)\n",
        "#calc_test_result(model.predict([N.train_x_text, N.train_x_audio]), N.train_y, N.train_mask)\n",
        "\n",
        "\n",
        "\n",
        "#print(N.test_y, model.predict(N.test_x_video))\n",
        "\n",
        "plot(s)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data\n",
            "Labels used for this classification: {'neutral': 0, 'sadness': 1, 'joy': 2, 'anger': 3}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9c7c80de0a42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"video\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_video_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"video\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-e3dc5812e13e>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, m)\u001b[0m\n\u001b[1;32m     17\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_audio_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\"video\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_video_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-925909900f19>\u001b[0m in \u001b[0;36mload_video_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_video_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/vgg_x_dev.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_video_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/vgg_x_train.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_video_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/vgg_x_test.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvid_to_row_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/dia_frames_train_dia_to_row.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvid_to_row_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/dia_frames_dev_dia_to_row.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/vgg_x_test.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW8M1cJNhKa5",
        "colab_type": "code",
        "outputId": "274836fe-c240-41d2-f627-01af6346f5ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"!pip uninstall tensorflow\n",
        "#!pip uninstall tensorboard\n",
        "#!pip install tensorboard==1.14.0 --ignore-installed\n",
        "!pip install tensorflow==1.14.0 --ignore-installed\n",
        "\n",
        "#!pip install keras-multi-head\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.2.0rc3:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.2.0rc3.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.2.0rc3\n",
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 98kB/s \n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 43.4MB/s \n",
            "\u001b[?25hCollecting gast>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Collecting protobuf>=3.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/02/5432412c162989260fab61fa65e0a490c1872739eb91a659896e4d554b26/protobuf-3.11.3-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 53.6MB/s \n",
            "\u001b[?25hCollecting numpy<2.0,>=1.14.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/27/e35e7c6e6a52fab9fcc64fc2b20c6b516eba930bb02b10ace3b38200d3ab/numpy-1.18.4-cp36-cp36m-manylinux1_x86_64.whl (20.2MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2MB 84.2MB/s \n",
            "\u001b[?25hCollecting wheel>=0.26\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl\n",
            "Collecting six>=1.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\n",
            "Collecting google-pasta>=0.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.8.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/97/f74da84d4db8cfe95f9b6aa2469be79af1873fec1adb80405105ed99a0a8/grpcio-1.28.1-cp36-cp36m-manylinux2010_x86_64.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 48.2MB/s \n",
            "\u001b[?25hCollecting astor>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 58.7MB/s \n",
            "\u001b[?25hCollecting absl-py>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 67.0MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[?25hCollecting keras-preprocessing>=1.0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
            "Collecting setuptools>=41.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/df/635cdb901ee4a8a42ec68e480c49f85f4c59e8816effbf57d9e6ee8b3588/setuptools-46.1.3-py3-none-any.whl (582kB)\n",
            "\u001b[K     |████████████████████████████████| 583kB 56.3MB/s \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 58.8MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/c4/ba46d44855e6eb1770a12edace5a165a0c6de13349f592b9036257f3c3d3/Markdown-3.2.1-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 14.4MB/s \n",
            "\u001b[?25hCollecting h5py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 45.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: termcolor, absl-py, wrapt\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-cp36-none-any.whl size=4832 sha256=ac75965b64773eaf04d68f0f1ac94a1de12e68d16ab46e3e8a30ae5cc37a4857\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.9.0-cp36-none-any.whl size=121931 sha256=c4f2df001cec976e1bbf3fc5996432c0ec09aba46ae25c9851fab797ed15ba44\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=67501 sha256=c1ffef8014abfaacb67c470b131bd957f86d5367ad9f7e12a2621ec9930c5008\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c2/ed/d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a\n",
            "Successfully built termcolor absl-py wrapt\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: termcolor, six, setuptools, protobuf, grpcio, numpy, wheel, werkzeug, absl-py, markdown, tensorboard, gast, google-pasta, astor, tensorflow-estimator, h5py, keras-applications, keras-preprocessing, wrapt, tensorflow\n",
            "Successfully installed absl-py-0.9.0 astor-0.8.1 gast-0.3.3 google-pasta-0.2.0 grpcio-1.28.1 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.2.1 numpy-1.18.4 protobuf-3.11.3 setuptools-46.1.3 six-1.14.0 tensorboard-2.2.1 tensorflow-1.14.0 tensorflow-estimator-2.2.0 termcolor-1.1.0 werkzeug-1.0.1 wheel-0.34.2 wrapt-1.12.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "grpc",
                  "numpy",
                  "pkg_resources",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}