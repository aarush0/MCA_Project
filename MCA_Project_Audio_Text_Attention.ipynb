{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MCA_Project_Audio_Text_Attention",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOYvjzQ-YVMC",
        "colab_type": "code",
        "outputId": "af160a22-d97c-415d-9aaa-f447bfebf4ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "%tensorflow_version 1.1x\n",
        "from datetime import datetime\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.1x`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Arc6Dss6kRHI",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os, sys\n",
        "from collections import Counter, defaultdict\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from functools import cmp_to_key\n",
        "import pickle\n",
        "\n",
        "x = pickle.load(open(\"/content/drive/My Drive/MCA_Project/data/pickles/data_{}.p\".format(\"emotion\"),\"rb\"))\n",
        "revs, W, word_idx_map, vocab, _, label_index = x[0], x[1], x[2], x[3], x[4], x[5]\n",
        "\n",
        "def get_word_indices(data_x):\n",
        "  length = len(data_x.split())\n",
        "  return np.array([word_idx_map[word] for word in data_x.split()] + [0]*(50-length))[:50]\n",
        "\n",
        "def comp_id(x, y):\n",
        "  xd = int(x[:x.find('_')])\n",
        "  xu = int(x[x.find('_')+1:])\n",
        "\n",
        "  yd = int(y[:y.find('_')])\n",
        "  yu = int(y[y.find('_')+1:])\n",
        "\n",
        "  if xd != yd:\n",
        "    return xd - yd\n",
        "  else:\n",
        "    return xu - yu\n",
        "\n",
        "\n",
        "def preprocess():\n",
        "\n",
        "  train_data, val_data, test_data = {},{},{}\n",
        "\n",
        "  counts_train = np.zeros((5,1))\n",
        "  counts_test = np.zeros((5,1))\n",
        "  counts_val = np.zeros((5,1))\n",
        "\n",
        "  for i in range(len(revs)):\n",
        "\n",
        "    utterance_id = revs[i]['dialog']+\"_\"+revs[i]['utterance']\n",
        "    \n",
        "    sentence_word_indices = get_word_indices(revs[i]['text'])\n",
        "    \n",
        "    label = label_index[revs[i]['y']]\n",
        "\n",
        "    if label != 0 and label != 3 and label != 4 and label != 6:\n",
        "      continue\n",
        "\n",
        "    if label == 0:\n",
        "      label = 0\n",
        "    elif label == 3:\n",
        "      label = 1\n",
        "    elif label == 4:\n",
        "      label = 2\n",
        "    else:\n",
        "      label = 3 \n",
        "\n",
        "    if revs[i]['split']==\"train\" and counts_train[label] > 1000:\n",
        "      continue\n",
        "\n",
        "    if revs[i]['split']==\"train\":\n",
        "        train_data[utterance_id]=(sentence_word_indices,label)\n",
        "        counts_train[label] += 1\n",
        "    elif revs[i]['split']==\"val\":\n",
        "        val_data[utterance_id]=(sentence_word_indices,label)\n",
        "        counts_val[label] += 1\n",
        "    elif revs[i]['split']==\"test\":\n",
        "        test_data[utterance_id]=(sentence_word_indices,label)\n",
        "        counts_test[label] += 1\n",
        "\n",
        "  dialogs = []\n",
        "  utrs = -1\n",
        "  d_cur = -1\n",
        "\n",
        "  t_d = {}\n",
        "  t_map = {}\n",
        "  sorted_tr_keys = sorted(train_data.keys(), key=cmp_to_key(comp_id))\n",
        "\n",
        "  for i in sorted_tr_keys:\n",
        "    d = i[:i.find('_')]\n",
        "    u = i[i.find('_') + 1:]\n",
        "    ouid = d + '_' + u\n",
        "\n",
        "    if d not in dialogs:\n",
        "      d_cur += 1\n",
        "      utrs = 0\n",
        "      dialogs.append(d)\n",
        "    else:\n",
        "      utrs += 1\n",
        "\n",
        "    df = d_cur\n",
        "    uf = utrs\n",
        "\n",
        "    uid = str(df) +'_' + str(uf)\n",
        "    t_d[uid] = train_data[i]\n",
        "\n",
        "    t_map[uid] = ouid\n",
        "\n",
        "  print(t_map)\n",
        "  dialogs = []\n",
        "  utrs = -1\n",
        "  d_cur = -1\n",
        "\n",
        "  v_d = {}\n",
        "  v_map = {}\n",
        "  sorted_val_keys = sorted(val_data.keys(), key=cmp_to_key(comp_id))\n",
        "\n",
        "  for i in sorted_val_keys:\n",
        "    d = i[:i.find('_')]\n",
        "    u = i[i.find('_') + 1:]\n",
        "    ouid = d + '_' + u\n",
        "\n",
        "    if d not in dialogs:\n",
        "      d_cur += 1\n",
        "      utrs = 0\n",
        "      dialogs.append(d)\n",
        "    else:\n",
        "      utrs += 1\n",
        "\n",
        "    df = d_cur\n",
        "    uf = utrs\n",
        "\n",
        "    uid = str(df) +'_' + str(uf)\n",
        "    v_d[uid] = val_data[i]\n",
        "    v_map[uid] = ouid\n",
        "\n",
        "  dialogs = []\n",
        "  utrs = -1\n",
        "  d_cur = -1\n",
        "\n",
        "  ts_d = {}\n",
        "  ts_map = {}\n",
        "  sorted_ts_keys = sorted(test_data.keys(), key=cmp_to_key(comp_id))\n",
        "\n",
        "  for i in sorted_ts_keys:\n",
        "    d = i[:i.find('_')]\n",
        "    u = i[i.find('_') + 1:]\n",
        "    ouid = d + '_' + u\n",
        "\n",
        "    if d not in dialogs:\n",
        "      d_cur += 1\n",
        "      utrs = 0\n",
        "      dialogs.append(d)\n",
        "    else:\n",
        "      utrs += 1\n",
        "\n",
        "    df = d_cur\n",
        "    uf = utrs\n",
        "\n",
        "    uid = str(df) +'_' + str(uf)\n",
        "    ts_d[uid] = test_data[i]\n",
        "    ts_map[uid] = ouid\n",
        "  \n",
        "  return t_d, v_d, ts_d, t_map, v_map, ts_map\n",
        "\n",
        "\n",
        "#preprocess()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz0qBRjPfmaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "max_length=50 # Maximum length of the sentence\n",
        "\n",
        "class Dataloader:\n",
        "    \n",
        "    def __init__(self, mode=None):\n",
        "\n",
        "        try:\n",
        "            assert(mode is not None)\n",
        "        except AssertionError as e:\n",
        "            print(\"Set mode as 'Sentiment' or 'Emotion'\")\n",
        "            exit()\n",
        "\n",
        "        self.MODE = mode # Sentiment or Emotion classification mode\n",
        "        self.max_l = max_length\n",
        "\n",
        "        \"\"\"\n",
        "            Loading the dataset: \n",
        "                - revs is a dictionary with keys/value: \n",
        "                    - text: original sentence\n",
        "                    - split: train/val/test :: denotes the which split the tuple belongs to\n",
        "                    - y: label of the sentence\n",
        "                    - dialog: ID of the dialog the utterance belongs to\n",
        "                    - utterance: utterance number of the dialog ID\n",
        "                    - num_words: number of words in the utterance\n",
        "                - W: glove embedding matrix\n",
        "                - vocab: the vocabulary of the dataset\n",
        "                - word_idx_map: mapping of each word from vocab to its index in W\n",
        "                - label_index: mapping of each label (emotion or sentiment) to its assigned index, eg. label_index['neutral']=0\n",
        "        \"\"\"\n",
        "        x = pickle.load(open(\"/content/drive/My Drive/MCA_Project/data/pickles/data_{}.p\".format(self.MODE.lower()),\"rb\"))\n",
        "        self.revs, self.W, self.word_idx_map, self.vocab, _, label_index = x[0], x[1], x[2], x[3], x[4], x[5]\n",
        "        \n",
        "        self.num_classes = 4\n",
        "        print(\"Labels used for this classification: \", label_index)\n",
        "\n",
        "        self.train_data, self.val_data, self.test_data, self.tr_map, self.v_map, self.ts_map = preprocess()\n",
        "\n",
        "        # Creating dialogue:[utterance_1, utterance_2, ...] ids\n",
        "        self.train_dialogue_ids = self.get_dialogue_ids(self.train_data.keys())\n",
        "        self.val_dialogue_ids = self.get_dialogue_ids(self.val_data.keys())\n",
        "        self.test_dialogue_ids = self.get_dialogue_ids(self.test_data.keys())\n",
        "\n",
        "        # Max utternance in a dialog in the dataset\n",
        "        self.max_utts = self.get_max_utts(self.train_dialogue_ids, self.val_dialogue_ids, self.test_dialogue_ids)\n",
        "\n",
        "    def get_dialogue_ids(self, keys):\n",
        "        ids=defaultdict(list)\n",
        "        for key in keys:\n",
        "            ids[key.split(\"_\")[0]].append(int(key.split(\"_\")[1]))\n",
        "        for ID, utts in ids.items():\n",
        "            ids[ID]=[str(utt) for utt in sorted(utts)]\n",
        "        return ids\n",
        "\n",
        "    def get_max_utts(self, train_ids, val_ids, test_ids):\n",
        "        max_utts_train = max([len(train_ids[vid]) for vid in train_ids.keys()])\n",
        "        max_utts_val = max([len(val_ids[vid]) for vid in val_ids.keys()])\n",
        "        max_utts_test = max([len(test_ids[vid]) for vid in test_ids.keys()])\n",
        "        return np.max([max_utts_train, max_utts_val, max_utts_test])\n",
        "\n",
        "    def get_one_hot(self, label):\n",
        "        label_arr = [0]*self.num_classes\n",
        "        label_arr[label]=1\n",
        "        return label_arr[:]\n",
        "\n",
        "    def get_dialogue_text_embs(self):\n",
        "        key = list(self.train_data.keys())[0]\n",
        "        \n",
        "        pad = [0]*len(self.train_data[key][0])\n",
        "\n",
        "        def get_emb(dialogue_id, local_data):\n",
        "            dialogue_text = []\n",
        "            for vid in dialogue_id.keys():\n",
        "                local_text = []\n",
        "                for utt in dialogue_id[vid]:\n",
        "                    local_text.append(local_data[vid+\"_\"+str(utt)][0][:])\n",
        "                for _ in range(self.max_utts-len(local_text)):\n",
        "                    local_text.append(pad[:])\n",
        "                dialogue_text.append(local_text[:self.max_utts])\n",
        "            return np.array(dialogue_text)\n",
        "\n",
        "        self.train_dialogue_features = get_emb(self.train_dialogue_ids, self.train_data)\n",
        "        self.val_dialogue_features = get_emb(self.val_dialogue_ids, self.val_data)\n",
        "        self.test_dialogue_features = get_emb(self.test_dialogue_ids, self.test_data)\n",
        "\n",
        "    def get_dialogue_labels(self):\n",
        "\n",
        "        def get_labels(ids, data):\n",
        "            dialogue_label=[]\n",
        "\n",
        "            for vid, utts in ids.items():\n",
        "                local_labels=[]\n",
        "                for utt in utts:\n",
        "                    local_labels.append(self.get_one_hot(data[vid+\"_\"+str(utt)][1]))\n",
        "                for _ in range(self.max_utts-len(local_labels)):\n",
        "                    local_labels.append(self.get_one_hot(1)) # Dummy label\n",
        "                dialogue_label.append(local_labels[:self.max_utts])\n",
        "            return np.array(dialogue_label)\n",
        "\n",
        "        self.train_dialogue_label=get_labels(self.train_dialogue_ids, self.train_data)\n",
        "        self.val_dialogue_label=get_labels(self.val_dialogue_ids, self.val_data)\n",
        "        self.test_dialogue_label=get_labels(self.test_dialogue_ids, self.test_data)\n",
        "\n",
        "    def get_dialogue_labels_audio(self):\n",
        "\n",
        "        def get_labels(ids, data, map):\n",
        "            dialogue_label=[]\n",
        "\n",
        "            for vid, utts in ids.items():\n",
        "                local_labels=[]\n",
        "                for utt in utts:\n",
        "                    print(vid+\"_\"+str(utt), map[vid+\"_\"+str(utt)])\n",
        "                    local_labels.append(self.get_one_hot(data[map[vid+\"_\"+str(utt)]][1]))\n",
        "                for _ in range(self.max_utts-len(local_labels)):\n",
        "                    local_labels.append(self.get_one_hot(1)) # Dummy label\n",
        "                dialogue_label.append(local_labels[:self.max_utts])\n",
        "            return np.array(dialogue_label)\n",
        "\n",
        "        self.train_dialogue_label=get_labels(self.train_dialogue_ids, self.train_data, self.tr_map)\n",
        "        self.val_dialogue_label=get_labels(self.val_dialogue_ids, self.val_data, self.v_map)\n",
        "        self.test_dialogue_label=get_labels(self.test_dialogue_ids, self.test_data, self.ts_map)\n",
        "\n",
        "        \n",
        "    def get_dialogue_lengths(self):\n",
        "\n",
        "        self.train_dialogue_length, self.val_dialogue_length, self.test_dialogue_length=[], [], []\n",
        "        for vid, utts in self.train_dialogue_ids.items():\n",
        "            self.train_dialogue_length.append(len(utts))\n",
        "        for vid, utts in self.val_dialogue_ids.items():\n",
        "            self.val_dialogue_length.append(len(utts))\n",
        "        for vid, utts in self.test_dialogue_ids.items():\n",
        "            self.test_dialogue_length.append(len(utts))\n",
        "\n",
        "    def get_masks(self):\n",
        "\n",
        "        self.train_mask = np.zeros((len(self.train_dialogue_length), self.max_utts), dtype='float')\n",
        "        for i in range(len(self.train_dialogue_length)):\n",
        "            self.train_mask[i,:self.train_dialogue_length[i]]=1.0\n",
        "        self.val_mask = np.zeros((len(self.val_dialogue_length), self.max_utts), dtype='float')\n",
        "        for i in range(len(self.val_dialogue_length)):\n",
        "            self.val_mask[i,:self.val_dialogue_length[i]]=1.0\n",
        "        self.test_mask = np.zeros((len(self.test_dialogue_length), self.max_utts), dtype='float')\n",
        "        for i in range(len(self.test_dialogue_length)):\n",
        "            self.test_mask[i,:self.test_dialogue_length[i]]=1.0\n",
        "        \n",
        "    def load_text_data(self, ):\n",
        "\n",
        "        self.get_dialogue_text_embs()\n",
        "        self.get_dialogue_lengths()\n",
        "        self.get_dialogue_labels()\n",
        "        self.get_masks()\n",
        "\n",
        "    def load_audio_data(self, ):\n",
        "\n",
        "        AUDIO_PATH = \"/content/drive/My Drive/MCA_Project/data/pickles/audio_embeddings_feature_selection_{}.pkl\".format(self.MODE.lower())\n",
        "        self.train_audio_emb, self.val_audio_emb, self.test_audio_emb = pickle.load(open(AUDIO_PATH,\"rb\"))\n",
        "\n",
        "        self.get_dialogue_audio_embs()\n",
        "        self.get_dialogue_lengths()\n",
        "        #self.get_dialogue_labels_audio()\n",
        "        self.get_dialogue_labels()\n",
        "        self.get_masks()\n",
        "\n",
        "    def get_dialogue_audio_embs(self):\n",
        "        key = list(self.train_audio_emb.keys())[0]\n",
        "        pad = [0]*len(self.train_audio_emb[key])\n",
        "\n",
        "        def get_emb(dialogue_id, audio_emb, map):\n",
        "            dialogue_audio=[]\n",
        "            for vid in dialogue_id.keys():\n",
        "                local_audio=[]\n",
        "                for utt in dialogue_id[vid]:\n",
        "                    try:\n",
        "                        local_audio.append(audio_emb[map[vid+\"_\"+str(utt)]][:])\n",
        "                    except:\n",
        "                        print(\"oops\")\n",
        "                        print(vid+\"_\"+str(utt))\n",
        "                        local_audio.append(pad[:])\n",
        "                for _ in range(self.max_utts-len(local_audio)):\n",
        "                    local_audio.append(pad[:])\n",
        "                dialogue_audio.append(local_audio[:self.max_utts])\n",
        "            return np.array(dialogue_audio)\n",
        "\n",
        "        self.train_dialogue_features = get_emb(self.train_dialogue_ids, self.train_audio_emb, self.tr_map)\n",
        "        self.val_dialogue_features = get_emb(self.val_dialogue_ids, self.val_audio_emb, self.v_map)\n",
        "        self.test_dialogue_features = get_emb(self.test_dialogue_ids, self.test_audio_emb, self.ts_map)\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FRuRPuMHt_uP",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Lambda, LSTM, TimeDistributed, Masking, Bidirectional\n",
        "from tensorflow.keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os, pickle\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "#from keras_multi_head import MultiHeadAttention\n",
        "#from keras_multi_head import MultiHead\n",
        "#import tensorflow.estimator.MultiHead\n",
        "'''\n",
        "import argparse\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Lambda, LSTM, TimeDistributed, Masking, Bidirectional, concatenate\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model, load_model\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os, pickle\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf'''\n",
        "\n",
        "\n",
        "class Network1:\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tself.classification_mode = \"emotion\"\n",
        "\t\tself.modality = \"text\"\n",
        "\t\t\n",
        "    #self.PATH = \"/content/drive/My Drive/Colab Notebooks/data/models/{}_weights_{}.hdf5\".format(\"text\",self.classification_mode.lower())\n",
        "\t\t#self.OUTPUT_PATH = \"/content/drive/My Drive/Colab Notebooks/data/pickles/{}_{}.pkl\".format(\"text\",self.classification_mode.lower())\n",
        "\t\tprint(\"Model initiated for {} classification\".format(self.classification_mode))\n",
        "\n",
        "\tdef load_data(self,m):\n",
        "    \n",
        "\t\tprint('Loading data')\n",
        "    \n",
        "\t\tself.data = Dataloader(mode = self.classification_mode)\n",
        "    \n",
        "\t\tif m == \"text\":\n",
        "\t\t\tself.data.load_text_data()\n",
        "\t\telif m == \"audio\":\n",
        "\t\t\tself.data.load_audio_data()\n",
        "\t\telse:\n",
        "\t\t\texit()\n",
        "    \n",
        "\t\tself.train_x = self.data.train_dialogue_features\n",
        "\t\tself.val_x = self.data.val_dialogue_features\n",
        "\t\tself.test_x = self.data.test_dialogue_features\n",
        "    \n",
        "\t\tself.train_y = self.data.train_dialogue_label\n",
        "\t\tself.val_y = self.data.val_dialogue_label\n",
        "\t\tself.test_y = self.data.test_dialogue_label\n",
        "    \n",
        "\t\tself.train_mask = self.data.train_mask\n",
        "\t\tself.val_mask = self.data.val_mask\n",
        "\t\tself.test_mask = self.data.test_mask\n",
        "    \n",
        "\t\tself.train_id = self.data.train_dialogue_ids.keys()\n",
        "\t\tself.val_id = self.data.val_dialogue_ids.keys()\n",
        "\t\tself.test_id = self.data.test_dialogue_ids.keys()\n",
        "    \n",
        "\t\tself.sequence_length = self.train_x.shape[1]\n",
        "\n",
        "\t\tself.classes = self.train_y.shape[2]\n",
        "    \n",
        "\t\tself.epochs = 20\n",
        "\t\tself.batch_size = 50\n",
        "\n",
        "\t\tif m == \"text\":\n",
        "\t\t\tself.train_x_text = self.train_x\n",
        "\t\t\tself.val_x_text = self.val_x\n",
        "\t\t\tself.test_x_text = self.test_x\n",
        "\n",
        "\t\t\tself.train_y_text = self.train_y\n",
        "\t\t\tself.val_y_text = self.val_y \n",
        "\t\t\tself.test_y_text = self.test_y \n",
        "\t\t\t\n",
        "\t\t\tself.train_mask_text = self.train_mask \n",
        "\t\t\tself.val_mask_text = self.val_mask \n",
        "\t\t\tself.test_mask_text = self.test_mask\n",
        "\t\t\t\n",
        "\t\t\tself.train_id_text = self.train_id \n",
        "\t\t\tself.val_id_text = self.val_id \n",
        "\t\t\tself.test_id_text = self.test_id \n",
        "\n",
        "\t\t\tself.sequence_length_text = self.sequence_length\n",
        "\n",
        "\t\tif m == \"audio\":\n",
        "\t\t\tself.train_x_audio = self.train_x\n",
        "\t\t\tself.val_x_audio = self.val_x\n",
        "\t\t\tself.test_x_audio = self.test_x\n",
        "\n",
        "\t\t\tself.train_y_audio = self.train_y\n",
        "\t\t\tself.val_y_audio = self.val_y \n",
        "\t\t\tself.test_y_audio = self.test_y \n",
        "\t\t\t\n",
        "\t\t\tself.train_mask_audio = self.train_mask \n",
        "\t\t\tself.val_mask_audio = self.val_mask \n",
        "\t\t\tself.test_mask_audio = self.test_mask\n",
        "\t\t\t\n",
        "\t\t\tself.train_id_audio = self.train_id \n",
        "\t\t\tself.val_id_audio = self.val_id \n",
        "\t\t\tself.test_id_audio = self.test_id \n",
        "\t\t\tself.sequence_length_audio = self.sequence_length\n",
        "\n",
        "\tdef get_text_lstm(self):\n",
        "\t\tself.sentence_length = self.train_x.shape[2]\n",
        "    \n",
        "\t\tself.embedding_dim = self.data.W.shape[1]\n",
        "    \n",
        "\t\tself.vocabulary_size = self.data.W.shape[0]\n",
        "\t\t\n",
        "\t\tembedding = Embedding(input_dim=self.vocabulary_size, output_dim=self.embedding_dim, weights=[self.data.W], input_length=self.sentence_length, trainable=False)\n",
        "    \n",
        "\t\tdef slicer(x, index):\n",
        "\t\t\treturn x[:,K.constant(index, dtype='int32'),:]\n",
        "    \n",
        "\t\tdef slicer_output_shape(input_shape):\n",
        "\t\t\tshape = list(input_shape)\n",
        "\t\t\tassert len(shape) == 3  # batch, seq_len, sent_len\n",
        "\t\t\tnew_shape = (shape[0], shape[2])\n",
        "\t\t\treturn new_shape\n",
        "\n",
        "\t\tdef reshaper(x):\n",
        "\t\t\treturn K.expand_dims(x, axis=3)\n",
        "    \n",
        "\t\tdef flattener(x):\n",
        "\t\t\tx = K.reshape(x, [-1,x.shape[1]*x.shape[2]])\n",
        "\t\t\treturn x\n",
        "\n",
        "\t\tdef flattener_output_shape(input_shape):\n",
        "\t\t\tshape = list(input_shape)\n",
        "\t\t\tnew_shape = (shape[0], shape[2]*shape[1])\n",
        "\t\t\treturn new_shape\n",
        "\n",
        "\t\tinputs = Input(shape=(self.sequence_length, self.sentence_length), dtype='int32')\n",
        "\t\t\n",
        "\t\t\n",
        "\t\toutput = []\n",
        "\t\tfor ind in range(self.sequence_length):\n",
        "\t\t\tlocal_input = Lambda(slicer, output_shape=slicer_output_shape, arguments={\"index\":ind})(inputs) # Batch, word_indices\n",
        "\n",
        "\t\t\temb_output = embedding(local_input)\n",
        "\t\t\treshape = Lambda(reshaper)(emb_output)\n",
        "\n",
        "\t\t\tflatten = Lambda(flattener, output_shape=flattener_output_shape,)(reshape)\n",
        "\n",
        "\t\t\toutput.append(flatten)\n",
        "\n",
        "\t\tdef stack(x):\n",
        "\t\t\treturn K.stack(x, axis=1)\n",
        "      \n",
        "\t\toutputs = Lambda(stack)(output)\n",
        "\t\tmasked = Masking(mask_value =0)(outputs)\n",
        "\t\t\n",
        "\t\tlstm = Bidirectional(LSTM(200, activation='relu', return_sequences = True, dropout=0.3), name = 'lstm_t')(masked)\n",
        "\t\tself.text_lstm_layer = lstm\n",
        "\t\tprint(\"TEXT LSTM \", lstm)\n",
        "\t\tat_layer = HanAttention()\n",
        "\t\tat_layer.build(lstm.shape)\n",
        "\t\tattn_scores = at_layer.call([lstm])\n",
        "\t\tconcat_output2 = Concatenate(axis=-1, name='concat_layer')([attn_scores,lstm])\n",
        "\t\tlstm = Bidirectional(LSTM(200, activation='relu', return_sequences = True, dropout=0.3), name=\"utter_t\")(concat_output2)\n",
        "\t\toutput = TimeDistributed(Dense(self.classes,activation='softmax',kernel_initializer='uniform'))(lstm)\n",
        "\n",
        "\t\tprint(\"TEXT OUTPUT \", output)\n",
        "\t\tmodel = Model(inputs, output)\n",
        "\n",
        "\t\t#model.summary()\n",
        "\n",
        "\t\tself.text_lstm =  model\n",
        "\n",
        "\t\treturn lstm, inputs\n",
        "\n",
        "\tdef get_audio_lstm(self):\n",
        "\n",
        "\t\tself.embedding_dim = self.train_x.shape[2]\n",
        "\n",
        "\t\tprint(\"Creating Model...\")\n",
        "\t\t\n",
        "\t\tinputs = Input(shape=(self.sequence_length, self.embedding_dim), dtype='float32')\n",
        "\t\tmasked = Masking(mask_value =0)(inputs)\n",
        "\t\tlstm = Bidirectional(LSTM(200, activation='tanh', return_sequences = True, dropout=0.4), name='lstm_a')(masked)\n",
        "\t\tself.audio_lstm_layer = lstm\n",
        "\n",
        "\t\tat_layer = HanAttention()\n",
        "\t\tat_layer.build(lstm.shape)\n",
        "\t\tattn_scores = at_layer.call([lstm])\n",
        "\t\tconcat_output2 = Concatenate(axis=-1, name='concat_layer')([attn_scores,lstm])\n",
        "\t\n",
        "\t\tlstm = Bidirectional(LSTM(200, activation='tanh', return_sequences = True, dropout=0.4), name=\"utter_a\")(concat_output2)\n",
        "\t\toutput = TimeDistributed(Dense(self.classes,activation='softmax',kernel_initializer='uniform'))(lstm)\n",
        "\n",
        "\t\tmodel = Model(inputs, output)\n",
        "\n",
        "\t\tself.audio_lstm = model\n",
        "\n",
        "\t\treturn lstm, inputs\n",
        "\n",
        "\tdef get_final_model(self, tl, al, ti, ai):\n",
        "\t\t#attn_out = tensorflow.keras.layers.Attention()([tl, al])\n",
        "\t\n",
        "\t\t'''#concat_output = Concatenate(axis=-1, name='concat_layer')([tl, attn_out])\n",
        "\t\tconcat_output = Concatenate(axis=-1, name='concat_layer')([tl, al])\n",
        "\t\tbatch_size = tf.shape(concat_output)[0]\n",
        "\t\tW_3d = tf.tile(tf.expand_dims(self.W, axis=0), tf.stack([batch_size, 1, 1]))\n",
        "\t  #[batch_size, steps, features]\n",
        "\t\tinput_projection = tf.matmul(concat_output,inputs, W_3d)\n",
        "\t\t\n",
        "\t\tinput_projection = tf.tanh(input_projection)\n",
        "\t\n",
        "\t\tlen_right = 27\n",
        "\t\t#print(\"PPPPPPPPPP \", len_right)\n",
        "\t\tlen_left = 27\n",
        "\t\t#print(\"WWWWWWWWWW \", len_left)\n",
        "\t\t#tensor_left = tf.expand_dims(al, axis=2)\n",
        "\t\t#tensor_right = tf.expand_dims(tl, axis=1)\n",
        "\t\t#tensor_left = tf.tile(tensor_left, [1, 1, len_right, 1])\n",
        "\t\t#tensor_right = tf.tile(tensor_right, [1, len_left, 1, 2])\n",
        "\t\t#tensor_merged = tf.concat([tl, al], axis=-1)\n",
        "\t\t#tensor_merged = Concatenate(axis=-1, name='concat_layer')([tensor_right, tensor_left])\n",
        "\t\t#middle_output = TimeDistributed(Dense(self.classes, activation='tanh'))(tensor_merged)\n",
        "\t\t#middle_output = Dense(self.classes,activation = 'tanh')(tensor_merged)\n",
        "\t\t#attn_scores = TimeDistributed(Dense(1))(middle_output)\n",
        "\t\tattn_scores = Dense(1)(input_projection)\n",
        "\t\tattn_scores = tf.squeeze(attn_scores, axis=3)\n",
        "\t\texp_attn_scores = tf.exp(attn_scores -\n",
        "                             tf.reduce_max(attn_scores, axis=-1, keepdims=True))\n",
        "\t\texp_sum = tf.reduce_sum(exp_attn_scores, axis=-1, keepdims=True)\n",
        "\t\tattention_weights = exp_attn_scores / exp_sum\n",
        "\t\t\n",
        "\t\tprint(\"WEIGHTS \", attention_weights)\n",
        "\n",
        "\t\tconcat_output = tf.matmul(attention_weights, tl)'''\n",
        "\n",
        "\t\t'''#concat_output = Concatenate(axis=-1, name='concat_layer')([tl, al])\n",
        "\t\tattention_layer = MyLayer()([tl,al])\n",
        "\t\t##attn_sum = 0\n",
        "\t\t#for g in range(attn_scores.shape[0]):\n",
        "\t\t#\tattn_sum = attn_sum + np.exp(attn_scores[g])\n",
        "\t\t#attention_layer = tf.squeeze(attn_scores, axis=3)\n",
        "\t\tattention_layer.build()([tl,al])\n",
        "\t\tattn_scores = attention_layer.call()([tl,al])\n",
        "\t\tprint(\"ATTENTION LAYER\" , attn_scores)\n",
        "\t\texp_attn_scores = tf.exp(attn_scores -\n",
        "                             tf.reduce_max(attn_scores, axis=-1, keepdims=True))\n",
        "\t\texp_sum = tf.reduce_sum(exp_attn_scores, axis=-1, keepdims=True)\n",
        "\t\tattention_weights = exp_attn_scores / exp_sum\n",
        "\t\t\n",
        "\t\tprint(\"WEIGHTS \", attention_weights)\n",
        "\n",
        "\t\tconcat_output = tf.matmul(attention_weights, tl)\n",
        "\t \n",
        "\t\tattention_weights = exp_attn_scores / attn_sum\n",
        "\t\t\n",
        "\t\tprint(\"WEIGHTS \", attention_weights)\n",
        "\n",
        "\t\tconcat_output = tf.matmul(attention_weights, tl)\n",
        "\n",
        "\n",
        "\n",
        "\t\t#att_out=attention()(concat_output)\n",
        "\t\t#concat_output = Concatenate(axis=-1, name='concat_layer')([tl, attn_out])'''\n",
        "\n",
        "\t\t'''print(\"text_length \", tl.shape)\n",
        "\t\tprint(\"audio_length \", al.shape)\n",
        "\t\n",
        "\t\t#u_weight = np.ones((tl.shape))\n",
        "\t\t#v_weight = np.ones((al.shape))\n",
        "\t\n",
        "\t\t#print(\"u_weight \", u_weight)\n",
        "\n",
        "\t\tal_tl = tl+al\n",
        "\n",
        "\t\t#print(\"al_tl_collective_length \", al_tl_collective.shape)\n",
        "\t\t\n",
        "\t\talpha = TimeDistributed(Dense(self.classes,activation='tanh'))(al_tl)\n",
        "\t\tattn_scores = TimeDistributed(Dense(1))(alpha)\n",
        "\t\t\n",
        "\t\texp_attn_scores = tf.exp(attn_scores -\n",
        "                             tf.reduce_max(attn_scores, axis=-1, keepdims=True))\n",
        "\t\texp_sum = tf.reduce_sum(exp_attn_scores, axis=-1, keepdims=True)\n",
        "\t\tattention_weights = exp_attn_scores / exp_sum\n",
        "\t\tprint(\"WEIGHTS \", attention_weights)\n",
        "\n",
        "\t\tconcat_output = tf.matmul(attention_weights, tl)\n",
        "\t\t\n",
        "\t\tprint(tl.shape)\n",
        "\t\tprint(concat_output.shape)\n",
        "\t\n",
        "\t\tconcat_output2 = Concatenate(axis=-1, name='concat_layer')([tl, concat_output])\n",
        "\n",
        "\t\tprint(\"concat2.shape \",concat_output2.shape)'''\n",
        "\n",
        "\t\tat_layer = HanAttention()\n",
        "\t\tat_layer.build(tl.shape)\n",
        "\t\tattn_scores = at_layer.call([tl,al])\n",
        "\t\n",
        "\t\tprint(\"AT_OUTPUT \", attn_scores)\n",
        "\t\t\n",
        "\t\t#multi_head = tf.estimator.MultiHead(ti)(attn_scores)\n",
        "\t\t\n",
        "\t\t#exp_attn_scores = tf.exp(attn_scores -\n",
        "    #                         tf.reduce_max(attn_scores, axis=-1, keepdims=True))\n",
        "\t\t#exp_sum = tf.reduce_sum(exp_attn_scores, axis=-1, keepdims=True)\n",
        "\t\t#attention_weights = exp_attn_scores / exp_sum\n",
        "\t\t#print(\"WEIGHTS \", attention_weights)\n",
        "\n",
        "\t\t#concat_output = tf.matmul(attention_weights, tl)\n",
        "\t\t\n",
        "\t\t#print(multi_head.shape)\n",
        "\t\t#print(tl.shape)\n",
        "\t\n",
        "\t\t#multi_head = tf.reshape(multi_head,tl.shape)\n",
        "\t\tconcat_output2 = Concatenate(axis=-1, name='concat_layer')([attn_scores,tl])\n",
        "\n",
        "\t\t#multi_head = MultiHead( tf.keras.layers.LSTM(units=32), layer_num=5, name='Multi-Head')(concat_output2)\n",
        "\t\tlstm = Bidirectional(LSTM(200, activation='tanh', return_sequences = True, dropout=0.4), name='lstm_f')(concat_output2)\n",
        "\t\toutput = TimeDistributed(Dense(self.classes,activation='softmax',kernel_initializer='uniform'))(lstm)\n",
        "\n",
        "\t\tprint(\"ALL OUTPUT \", output)\n",
        "\t\tself.merged_model = Model([ti, ai], output)\n",
        "\t\t\n",
        "\tdef train_lstm(self, m):\n",
        "\t\tif m == 'text':\n",
        "\t\t\tmodel = self.text_lstm\n",
        "\t\telif m == 'audio':\n",
        "\t\t\tmodel = self.audio_lstm\n",
        "\t\t\n",
        "\t\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', sample_weight_mode='temporal')\n",
        "\t\tearly_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "\t\tmodel.fit(self.train_x, self.train_y,\n",
        "\t\t                epochs=self.epochs,\n",
        "\t\t                batch_size=self.batch_size,\n",
        "\t\t                sample_weight=self.train_mask,\n",
        "\t\t                shuffle=True, \n",
        "\t\t                callbacks=[early_stopping],\n",
        "\t\t                validation_data=(self.val_x, self.val_y, self.val_mask))\n",
        "\n",
        "\t\t#logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\t\t#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "\t\t#summary = model.fit(self.train_x, self.train_y,\n",
        "\t\t#                epochs=self.epochs,\n",
        "\t\t#                batch_size=self.batch_size,\n",
        "\t\t#                sample_weight=self.train_mask,\n",
        "\t\t#                shuffle=True, \n",
        "\t\t#                callbacks=[early_stopping, tensorboard_callback],\n",
        "\t\t#                validation_data=(self.val_x, self.val_y, self.val_mask))\n",
        "\t\t#filename = '/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/audio_att_model2.sav'\n",
        "\t\t#pickle.dump(open(filename, 'wb'),model)\n",
        "\t\t\n",
        "\t\t#output5 = open('/content/drive/My Drive/Audio_Model_Attention'+ '.pickle', 'wb')\n",
        "\t\t#pickle.dump(model, output5)\n",
        "\t\n",
        "\t\tself.test_model(m)\n",
        "\t\treturn model\n",
        "\t\n",
        "\tdef train_network(self):\n",
        "\t\tmodel = self.merged_model\n",
        "\t\t#print(\"HELLOOO\")\n",
        "\t\t#multi_head = MultiHeadAttention( head_num=5, name='Multi-Head' )(lstm)\n",
        "\t\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', sample_weight_mode='temporal')\n",
        "\t\tearly_stopping = EarlyStopping(monitor='loss', patience=10)\n",
        "\t\n",
        "\t\t\n",
        "\t\tlogdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\t\ttensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\t\t\n",
        "\t\t#model.summary()\n",
        "\t\tprint(self.train_x_text.shape)\n",
        "\t\tprint(self.train_x_audio.shape)\n",
        "\t\tprint(self.train_y.shape)\n",
        "\t\tprint(self.epochs)\n",
        "\t\tprint(self.batch_size)\n",
        "\t\tmodel.fit([self.train_x_text, self.train_x_audio], self.train_y, epochs=self.epochs,batch_size=self.batch_size)\n",
        "\t\n",
        "\t\treturn model\n",
        "\n",
        "\n",
        "\tdef test_model(self, m):\n",
        "\t\tif m == 'text':\n",
        "\t\t\tmodel = self.text_lstm\n",
        "\t\t\t#intermediate_layer_model = Model(input=model.input, output=model.get_layer(\"lstm_t\").output)\n",
        "\t\telif m == 'audio':\n",
        "\t\t\tmodel = self.audio_lstm\n",
        "\t\t\t#intermediate_layer_model = Model(input=model.input, output=model.get_layer(\"lstm_a\").output)\n",
        "\t\telif m == 'merged':\n",
        "\t\t\tmodel = self.merged_model\n",
        "\t\t\t#intermediate_layer_model = Model(input=model.input, output=model.get_layer(\"lstm_f\").output)\n",
        "\n",
        "\t\t'''\n",
        "\t\tintermediate_output_train = intermediate_layer_model.predict(self.train_x)\n",
        "\t\tintermediate_output_val = intermediate_layer_model.predict(self.val_x)\n",
        "\t\tintermediate_output_test = intermediate_layer_model.predict(self.test_x)\n",
        "\n",
        "\t\ttrain_emb, val_emb, test_emb = {}, {}, {}\n",
        "\t\tfor idx, ID in enumerate(self.train_id):\n",
        "\t\t    train_emb[ID] = intermediate_output_train[idx]\n",
        "\t\tfor idx, ID in enumerate(self.val_id):\n",
        "\t\t    val_emb[ID] = intermediate_output_val[idx]\n",
        "\t\tfor idx, ID in enumerate(self.test_id):\n",
        "\t\t    test_emb[ID] = intermediate_output_test[idx]\n",
        "\t\t'''\n",
        "\n",
        "\t\tfilename2 = '/content/drive/My Drive/mca/MCA_Project/Final_files/test_x_text_text.pkl'\n",
        "\t\toutfile2 = open(filename2, 'wb')\n",
        "\t\tnp.save(outfile2, self.test_x)\n",
        "\t\n",
        "\t\tfilename5 = '/content/drive/My Drive/mca/MCA_Project/Final_files/test_y_text.pkl'\n",
        "\t\toutfile5 = open(filename5, 'wb')\n",
        "\t\tnp.save(outfile5, self.test_y)\n",
        "\n",
        "\t\tfilename6 = '/content/drive/My Drive/mca/MCA_Project/Final_files/test_mask_text.pkl'\n",
        "\t\toutfile6 = open(filename6, 'wb')\n",
        "\t\tnp.save(outfile6, self.test_mask)\n",
        "\n",
        "\n",
        "\t\tcalc_test_result(model.predict(self.test_x), self.test_y, self.test_mask)\n",
        "\t\t\n",
        "def calc_test_result(pred_label, test_label, test_mask):\n",
        "\n",
        "\t\ttrue_label=[]\n",
        "\t\tpredicted_label=[]\n",
        "\n",
        "\t\tfor i in range(pred_label.shape[0]):\n",
        "\t\t\tfor j in range(pred_label.shape[1]):\n",
        "\t\t\t\tif test_mask[i,j]==1:\n",
        "\t\t\t\t\ttrue_label.append(np.argmax(test_label[i,j] ))\n",
        "\t\t\t\t\tpredicted_label.append(np.argmax(pred_label[i,j] ))\n",
        "\t\tprint(\"Confusion Matrix :\")\n",
        "\t\tprint(confusion_matrix(true_label, predicted_label))\n",
        "\t\tprint(\"Classification Report :\")\n",
        "\t\tprint(classification_report(true_label, predicted_label, digits=4))\n",
        "\t\tprint('Weighted FScore: \\n ', precision_recall_fscore_support(true_label, predicted_label, average='weighted'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3sx6Jupomob",
        "colab_type": "code",
        "outputId": "628f9070-bfe6-499f-820a-2bd402cd7f9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip install tensorflow.estimator.MultiHead"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow.estimator.MultiHead (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for tensorflow.estimator.MultiHead\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wf5bO2IRx9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class HanAttention(Layer):\n",
        "  \"\"\"\n",
        "  Refer to [Hierarchical Attention Networks for Document Classification]\n",
        "    (https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf)\n",
        "    wrap `with tf.variable_scope(name, reuse=tf.AUTO_REUSE):`\n",
        "  Input shape: (Batch size, steps, features)\n",
        "  Output shape: (Batch size, features)\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               W_regularizer=None,\n",
        "               u_regularizer=None,\n",
        "               b_regularizer=None,\n",
        "               W_constraint=None,\n",
        "               u_constraint=None,\n",
        "               b_constraint=None,\n",
        "               use_bias=True,\n",
        "               **kwargs):\n",
        "\n",
        "    super().__init__(**kwargs)\n",
        "    self.supports_masking = True\n",
        "    self.init = tf.keras.initializers.get('glorot_uniform')\n",
        "\n",
        "    self.W_regularizer = tf.keras.regularizers.get(W_regularizer)\n",
        "    self.u_regularizer = tf.keras.regularizers.get(u_regularizer)\n",
        "    self.b_regularizer = tf.keras.regularizers.get(b_regularizer)\n",
        "\n",
        "    self.W_constraint = tf.keras.constraints.get(W_constraint)\n",
        "    self.u_constraint = tf.keras.constraints.get(u_constraint)\n",
        "    self.b_constraint = tf.keras.constraints.get(b_constraint)\n",
        "\n",
        "    self.use_bias = use_bias\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # pylint: disable=attribute-defined-outside-init\n",
        "    #assert len(input_shape) == 3\n",
        "\n",
        "    self.W = self.add_weight(\n",
        "        name='{}_W'.format(self.name),\n",
        "        shape=(\n",
        "            int(input_shape[-1]),\n",
        "            int(input_shape[-1]),\n",
        "        ),\n",
        "        initializer=self.init,\n",
        "        regularizer=self.W_regularizer,\n",
        "        constraint=self.W_constraint)\n",
        "\n",
        "    if self.use_bias:\n",
        "      self.b = self.add_weight(\n",
        "          name='{}_b'.format(self.name),\n",
        "          shape=(int(input_shape[-1]),),\n",
        "          initializer='zero',\n",
        "          regularizer=self.b_regularizer,\n",
        "          constraint=self.b_constraint)\n",
        "\n",
        "    self.attention_context_vector = self.add_weight(\n",
        "        name='{}_att_context_v'.format(self.name),\n",
        "        shape=(int(input_shape[-1]),),\n",
        "        initializer=self.init,\n",
        "        regularizer=self.u_regularizer,\n",
        "        constraint=self.u_constraint)\n",
        "    self.built = True\n",
        "\n",
        "  # pylint: disable=missing-docstring, no-self-use\n",
        "  def compute_mask(self, inputs, mask=None):  # pylint: disable=unused-argument\n",
        "    # do not pass the mask to the next layers\n",
        "    return None\n",
        "\n",
        "  \n",
        "  def call(self, inputs, training=None, mask=None):\n",
        "    batch_size = tf.shape(inputs)[1]\n",
        "    W_3d = tf.tile(tf.expand_dims(self.W, axis=0), tf.stack([batch_size, 1, 1]))\n",
        "    # [batch_size, steps, features]\n",
        "    input_projection = tf.matmul(inputs, W_3d)\n",
        "\n",
        "    if self.use_bias:\n",
        "      input_projection += self.b\n",
        "\n",
        "    input_projection = tf.tanh(input_projection)\n",
        "\n",
        "    # [batch_size, steps, 1]\n",
        "    similaritys = tf.reduce_sum(\n",
        "        tf.multiply(input_projection, self.attention_context_vector),\n",
        "        axis=2,\n",
        "        keep_dims=True)\n",
        "\n",
        "    # [batch_size, steps, 1]\n",
        "    if mask is not None:\n",
        "      attention_weights = masked_softmax(similaritys, mask, axis=1)\n",
        "    else:\n",
        "      attention_weights = tf.nn.softmax(similaritys, axis=1)\n",
        "\n",
        "    # [batch_size, features]\n",
        "    attention_output = tf.reduce_sum(\n",
        "        tf.multiply(inputs, attention_weights), axis=0)\n",
        "    return attention_output\n",
        "\n",
        "  # pylint: disable=no-self-use\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    \"\"\"compute output shape\"\"\"\n",
        "    return input_shape[0], input_shape[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e88bea90-9765-4a6b-d954-9a5c0cd6e2ba",
        "id": "BP8gFqo3uCZF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "N = Network1()\n",
        "\n",
        "\"\"\"FOR TEXT\"\"\"\n",
        "N.load_data(\"text\")\n",
        "tl, ti = N.get_text_lstm()\n",
        "M = N.train_lstm(\"text\")\n",
        "\n",
        "\"\"\"FOR AUDIO\"\"\"\n",
        "#N.load_data(\"audio\")\n",
        "#al, ai = N.get_audio_lstm()\n",
        "#M = N.train_lstm(\"audio\")\n",
        "\n",
        " \n",
        "\"\"\"BIMODAL\"\"\"\n",
        "N.load_data(\"text\")\n",
        "tl, ti = N.get_text_lstm()\n",
        "\n",
        "#N.load_data(\"audio\")\n",
        "#al, ai = N.get_audio_lstm()\n",
        "\n",
        "#N.get_final_model(tl, al, ti, ai)\n",
        "\n",
        "#M = N.train_network()\n",
        "\n",
        "#model3 = N.merged_model\n",
        "\n",
        "#calc_test_result(model3.predict([N.test_x_text, N.test_x_audio]), N.test_y, N.test_mask)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"filename2 = '/content/drive/My Drive/mca/MCA_Project/Final_files/Bimodal_test_x_text.pkl'\n",
        "outfile2 = open(filename2, 'wb')\n",
        "np.save(outfile2, N.test_x_text)\n",
        "\n",
        "filename3 = '/content/drive/My Drive/mca/MCA_Project/Final_files/Bimodal_test_x_audio.pkl'\n",
        "outfile3 = open(filename3, 'wb')\n",
        "np.save(outfile3, N.test_x_audio)\n",
        "\n",
        "filename5 = '/content/drive/My Drive/mca/MCA_Project/Final_files/Bimodal_test_y.pkl'\n",
        "outfile5 = open(filename5, 'wb')\n",
        "np.save(outfile5, N.test_y)\n",
        "\n",
        "filename6 = '/content/drive/My Drive/mca/MCA_Project/Final_files/Bimodal_test_mask.pkl'\n",
        "outfile6 = open(filename6, 'wb')\n",
        "np.save(outfile6, N.test_mask)\"\"\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initiated for emotion classification\n",
            "Loading data\n",
            "Labels used for this classification:  {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger': 6}\n",
            "{'0_0': '0_0', '0_1': '0_1', '0_2': '0_2', '0_3': '0_3', '0_4': '0_5', '0_5': '0_6', '0_6': '0_7', '0_7': '0_8', '0_8': '0_9', '0_9': '0_11', '0_10': '0_13', '1_0': '1_1', '1_1': '1_4', '1_2': '1_5', '1_3': '1_8', '2_0': '2_0', '2_1': '2_1', '2_2': '2_2', '2_3': '2_3', '2_4': '2_5', '2_5': '2_7', '2_6': '2_8', '2_7': '2_9', '2_8': '2_10', '2_9': '2_11', '2_10': '2_12', '3_0': '3_3', '3_1': '3_4', '3_2': '3_5', '3_3': '3_8', '3_4': '3_9', '4_0': '4_1', '4_1': '4_3', '4_2': '4_4', '4_3': '4_5', '4_4': '4_6', '4_5': '4_7', '4_6': '4_8', '4_7': '4_9', '4_8': '4_10', '4_9': '4_11', '4_10': '4_12', '4_11': '4_13', '4_12': '4_14', '5_0': '5_0', '5_1': '5_1', '5_2': '5_2', '6_0': '6_0', '6_1': '6_2', '6_2': '6_3', '6_3': '6_4', '6_4': '6_5', '6_5': '6_6', '6_6': '6_7', '6_7': '6_9', '6_8': '6_10', '6_9': '6_11', '6_10': '6_12', '6_11': '6_13', '6_12': '6_14', '6_13': '6_15', '6_14': '6_16', '6_15': '6_18', '6_16': '6_19', '6_17': '6_20', '6_18': '6_21', '7_0': '7_0', '7_1': '7_1', '8_0': '8_0', '8_1': '8_1', '8_2': '8_2', '8_3': '8_3', '8_4': '8_4', '8_5': '8_5', '8_6': '8_6', '9_0': '9_0', '9_1': '9_1', '9_2': '9_2', '9_3': '9_3', '9_4': '9_4', '9_5': '9_5', '9_6': '9_6', '9_7': '9_7', '9_8': '9_8', '9_9': '9_9', '9_10': '9_10', '9_11': '9_11', '9_12': '9_13', '10_0': '10_0', '10_1': '10_1', '10_2': '10_2', '10_3': '10_3', '10_4': '10_4', '10_5': '10_5', '11_0': '11_0', '11_1': '11_1', '11_2': '11_2', '11_3': '11_3', '11_4': '11_4', '11_5': '11_5', '11_6': '11_6', '11_7': '11_7', '11_8': '11_8', '11_9': '11_9', '12_0': '12_0', '12_1': '12_1', '12_2': '12_5', '12_3': '12_7', '12_4': '12_11', '13_0': '13_0', '13_1': '13_1', '13_2': '13_3', '14_0': '14_0', '14_1': '14_1', '14_2': '14_2', '14_3': '14_4', '14_4': '14_5', '15_0': '15_0', '15_1': '15_1', '15_2': '15_2', '15_3': '15_3', '15_4': '15_5', '15_5': '15_6', '15_6': '15_7', '15_7': '15_8', '15_8': '15_9', '15_9': '15_10', '15_10': '15_11', '15_11': '15_12', '15_12': '15_13', '15_13': '15_14', '15_14': '15_15', '15_15': '15_16', '15_16': '15_17', '15_17': '15_18', '15_18': '15_19', '15_19': '15_20', '15_20': '15_21', '15_21': '15_22', '16_0': '16_0', '16_1': '16_1', '16_2': '16_4', '16_3': '16_5', '17_0': '17_0', '17_1': '17_1', '17_2': '17_2', '17_3': '17_3', '17_4': '17_5', '17_5': '17_7', '17_6': '17_8', '17_7': '17_9', '17_8': '17_10', '18_0': '18_0', '18_1': '18_1', '18_2': '18_2', '18_3': '18_3', '18_4': '18_4', '18_5': '18_5', '18_6': '18_7', '18_7': '18_8', '19_0': '19_0', '19_1': '19_1', '20_0': '20_0', '20_1': '20_1', '20_2': '20_2', '20_3': '20_3', '20_4': '20_4', '20_5': '20_5', '20_6': '20_6', '20_7': '20_7', '20_8': '20_8', '20_9': '20_9', '20_10': '20_10', '20_11': '20_11', '20_12': '20_12', '20_13': '20_13', '20_14': '20_14', '20_15': '20_16', '21_0': '21_1', '21_1': '21_2', '21_2': '21_4', '21_3': '21_6', '21_4': '21_7', '21_5': '21_9', '21_6': '21_10', '21_7': '21_12', '21_8': '21_13', '22_0': '22_1', '22_1': '22_3', '23_0': '23_1', '23_1': '23_3', '23_2': '23_4', '23_3': '23_5', '23_4': '23_6', '23_5': '23_7', '24_0': '24_0', '24_1': '24_1', '24_2': '24_2', '24_3': '24_3', '24_4': '24_5', '24_5': '24_6', '24_6': '24_7', '25_0': '25_0', '25_1': '25_1', '25_2': '25_2', '25_3': '25_3', '25_4': '25_4', '25_5': '25_5', '25_6': '25_6', '25_7': '25_7', '25_8': '25_8', '25_9': '25_9', '25_10': '25_10', '25_11': '25_12', '25_12': '25_13', '26_0': '26_1', '26_1': '26_3', '26_2': '26_4', '26_3': '26_5', '26_4': '26_6', '26_5': '26_7', '26_6': '26_8', '26_7': '26_9', '26_8': '26_10', '26_9': '26_11', '26_10': '26_12', '26_11': '26_13', '27_0': '27_0', '27_1': '27_1', '28_0': '28_0', '28_1': '28_1', '28_2': '28_2', '28_3': '28_3', '28_4': '28_5', '28_5': '28_7', '29_0': '29_1', '29_1': '29_2', '29_2': '29_3', '29_3': '29_6', '29_4': '29_7', '29_5': '29_8', '29_6': '29_9', '29_7': '29_10', '29_8': '29_11', '29_9': '29_13', '30_0': '30_0', '30_1': '30_1', '30_2': '30_2', '30_3': '30_3', '30_4': '30_4', '30_5': '30_6', '30_6': '30_8', '30_7': '30_9', '30_8': '30_10', '30_9': '30_12', '30_10': '30_13', '30_11': '30_14', '31_0': '31_0', '31_1': '31_1', '31_2': '31_2', '31_3': '31_3', '32_0': '32_1', '32_1': '32_3', '32_2': '32_5', '32_3': '32_8', '32_4': '32_10', '32_5': '32_11', '32_6': '32_12', '32_7': '32_13', '32_8': '32_14', '32_9': '32_15', '32_10': '32_16', '32_11': '32_18', '33_0': '33_1', '33_1': '33_2', '33_2': '33_3', '33_3': '33_5', '33_4': '33_6', '33_5': '33_7', '33_6': '33_8', '33_7': '33_9', '33_8': '33_10', '34_0': '34_0', '34_1': '34_1', '34_2': '34_2', '34_3': '34_3', '34_4': '34_4', '34_5': '34_5', '34_6': '34_6', '34_7': '34_7', '34_8': '34_8', '34_9': '34_10', '34_10': '34_11', '34_11': '34_12', '35_0': '35_1', '35_1': '35_2', '35_2': '35_3', '35_3': '35_4', '35_4': '35_5', '36_0': '36_0', '36_1': '36_1', '36_2': '36_2', '36_3': '36_3', '36_4': '36_4', '36_5': '36_5', '37_0': '37_0', '37_1': '37_1', '37_2': '37_2', '38_0': '38_0', '38_1': '38_1', '38_2': '38_2', '38_3': '38_3', '38_4': '38_5', '38_5': '38_6', '39_0': '39_0', '39_1': '39_1', '39_2': '39_3', '39_3': '39_4', '39_4': '39_5', '39_5': '39_6', '39_6': '39_7', '39_7': '39_8', '39_8': '39_9', '39_9': '39_11', '39_10': '39_12', '39_11': '39_13', '39_12': '39_14', '39_13': '39_15', '39_14': '39_16', '39_15': '39_17', '39_16': '39_18', '40_0': '40_0', '40_1': '40_1', '40_2': '40_2', '40_3': '40_3', '40_4': '40_4', '40_5': '40_5', '40_6': '40_6', '40_7': '40_7', '40_8': '40_10', '40_9': '40_12', '40_10': '40_13', '40_11': '40_14', '41_0': '41_0', '41_1': '41_2', '41_2': '41_3', '41_3': '41_4', '41_4': '41_6', '41_5': '41_7', '41_6': '41_8', '41_7': '41_9', '41_8': '41_10', '41_9': '41_11', '41_10': '41_13', '42_0': '42_0', '43_0': '43_0', '43_1': '43_1', '43_2': '43_2', '43_3': '43_3', '43_4': '43_4', '43_5': '43_5', '43_6': '43_6', '44_0': '44_0', '44_1': '44_2', '45_0': '45_0', '45_1': '45_2', '45_2': '45_3', '45_3': '45_4', '46_0': '46_0', '46_1': '46_2', '46_2': '46_4', '46_3': '46_5', '46_4': '46_6', '46_5': '46_8', '46_6': '46_9', '46_7': '46_10', '46_8': '46_12', '46_9': '46_14', '47_0': '47_0', '47_1': '47_1', '47_2': '47_2', '47_3': '47_3', '47_4': '47_4', '47_5': '47_5', '47_6': '47_6', '47_7': '47_7', '47_8': '47_8', '47_9': '47_9', '47_10': '47_12', '47_11': '47_14', '47_12': '47_15', '47_13': '47_16', '47_14': '47_17', '48_0': '48_0', '49_0': '49_0', '49_1': '49_1', '49_2': '49_2', '49_3': '49_3', '49_4': '49_5', '49_5': '49_6', '49_6': '49_7', '49_7': '49_8', '49_8': '49_9', '49_9': '49_10', '49_10': '49_11', '49_11': '49_12', '49_12': '49_14', '49_13': '49_16', '49_14': '49_17', '49_15': '49_19', '49_16': '49_21', '50_0': '50_0', '50_1': '50_1', '50_2': '50_3', '50_3': '50_4', '50_4': '50_7', '50_5': '50_8', '50_6': '50_11', '50_7': '50_12', '50_8': '50_13', '50_9': '50_15', '50_10': '50_16', '51_0': '51_0', '51_1': '51_1', '51_2': '51_2', '51_3': '51_4', '51_4': '51_5', '51_5': '51_6', '51_6': '51_7', '51_7': '51_8', '51_8': '51_9', '51_9': '51_10', '52_0': '52_0', '52_1': '52_1', '52_2': '52_2', '52_3': '52_3', '52_4': '52_4', '52_5': '52_5', '52_6': '52_6', '52_7': '52_7', '52_8': '52_8', '52_9': '52_9', '52_10': '52_10', '52_11': '52_11', '53_0': '53_0', '53_1': '53_1', '53_2': '53_2', '53_3': '53_3', '53_4': '53_4', '53_5': '53_5', '53_6': '53_6', '53_7': '53_7', '53_8': '53_8', '54_0': '54_1', '54_1': '54_2', '54_2': '54_5', '54_3': '54_6', '54_4': '54_7', '54_5': '54_8', '54_6': '54_9', '54_7': '54_10', '54_8': '54_11', '54_9': '54_14', '54_10': '54_15', '54_11': '54_16', '54_12': '54_17', '54_13': '54_18', '54_14': '54_19', '54_15': '54_20', '54_16': '54_21', '54_17': '54_22', '54_18': '54_23', '55_0': '55_0', '55_1': '55_1', '55_2': '55_2', '55_3': '55_4', '55_4': '55_5', '55_5': '55_7', '55_6': '55_8', '55_7': '55_9', '55_8': '55_10', '55_9': '55_11', '55_10': '55_12', '55_11': '55_13', '55_12': '55_14', '55_13': '55_15', '55_14': '55_16', '56_0': '56_1', '56_1': '56_3', '56_2': '56_4', '56_3': '56_5', '56_4': '56_6', '57_0': '57_1', '58_0': '58_0', '58_1': '58_1', '58_2': '58_3', '58_3': '58_4', '58_4': '58_6', '58_5': '58_7', '58_6': '58_8', '58_7': '58_9', '59_0': '59_0', '59_1': '59_1', '59_2': '59_2', '59_3': '59_3', '59_4': '59_4', '59_5': '59_5', '59_6': '59_6', '59_7': '59_7', '59_8': '59_8', '59_9': '59_9', '60_0': '61_0', '60_1': '61_1', '60_2': '61_2', '60_3': '61_3', '60_4': '61_4', '60_5': '61_5', '60_6': '61_6', '60_7': '61_7', '60_8': '61_8', '61_0': '62_0', '61_1': '62_1', '61_2': '62_2', '61_3': '62_3', '61_4': '62_5', '61_5': '62_6', '61_6': '62_7', '61_7': '62_8', '61_8': '62_11', '62_0': '63_0', '62_1': '63_1', '62_2': '63_2', '62_3': '63_4', '62_4': '63_5', '62_5': '63_6', '62_6': '63_7', '63_0': '64_0', '63_1': '64_1', '63_2': '64_2', '63_3': '64_3', '63_4': '64_4', '63_5': '64_5', '63_6': '64_6', '63_7': '64_7', '63_8': '64_8', '63_9': '64_9', '63_10': '64_10', '63_11': '64_11', '63_12': '64_12', '64_0': '65_0', '64_1': '65_2', '64_2': '65_3', '65_0': '66_1', '65_1': '66_2', '65_2': '66_3', '65_3': '66_4', '65_4': '66_6', '65_5': '66_7', '65_6': '66_8', '65_7': '66_9', '65_8': '66_10', '65_9': '66_11', '65_10': '66_12', '66_0': '67_1', '66_1': '67_2', '66_2': '67_3', '66_3': '67_5', '66_4': '67_6', '66_5': '67_7', '66_6': '67_8', '66_7': '67_9', '66_8': '67_10', '66_9': '67_11', '66_10': '67_12', '66_11': '67_13', '66_12': '67_14', '66_13': '67_15', '67_0': '68_0', '67_1': '68_1', '67_2': '68_2', '68_0': '69_0', '68_1': '69_1', '68_2': '69_2', '68_3': '69_4', '68_4': '69_5', '68_5': '69_6', '69_0': '70_0', '69_1': '70_1', '69_2': '70_2', '69_3': '70_3', '69_4': '70_4', '69_5': '70_5', '69_6': '70_6', '69_7': '70_7', '69_8': '70_8', '70_0': '71_0', '70_1': '71_1', '71_0': '72_0', '71_1': '72_2', '71_2': '72_4', '71_3': '72_5', '71_4': '72_6', '71_5': '72_7', '72_0': '73_0', '72_1': '73_1', '72_2': '73_2', '72_3': '73_3', '72_4': '73_4', '72_5': '73_5', '72_6': '73_7', '72_7': '73_8', '72_8': '73_9', '72_9': '73_11', '73_0': '74_0', '73_1': '74_1', '73_2': '74_2', '73_3': '74_3', '73_4': '74_5', '73_5': '74_6', '74_0': '75_0', '75_0': '76_0', '75_1': '76_1', '75_2': '76_2', '75_3': '76_3', '75_4': '76_4', '75_5': '76_5', '75_6': '76_6', '75_7': '76_7', '76_0': '77_0', '76_1': '77_1', '76_2': '77_2', '76_3': '77_3', '76_4': '77_4', '77_0': '78_0', '77_1': '78_1', '77_2': '78_2', '77_3': '78_3', '78_0': '79_0', '78_1': '79_2', '78_2': '79_4', '78_3': '79_6', '78_4': '79_7', '79_0': '80_0', '79_1': '80_1', '79_2': '80_3', '79_3': '80_4', '79_4': '80_5', '80_0': '81_0', '80_1': '81_1', '80_2': '81_2', '80_3': '81_3', '80_4': '81_5', '80_5': '81_6', '80_6': '81_8', '80_7': '81_9', '80_8': '81_10', '80_9': '81_11', '80_10': '81_12', '80_11': '81_13', '80_12': '81_14', '80_13': '81_15', '81_0': '82_0', '81_1': '82_1', '81_2': '82_2', '81_3': '82_4', '81_4': '82_5', '81_5': '82_7', '81_6': '82_10', '81_7': '82_11', '82_0': '83_0', '82_1': '83_1', '82_2': '83_2', '82_3': '83_3', '83_0': '84_1', '83_1': '84_2', '83_2': '84_3', '83_3': '84_4', '83_4': '84_5', '83_5': '84_6', '84_0': '85_0', '84_1': '85_1', '84_2': '85_2', '84_3': '85_3', '84_4': '85_4', '85_0': '86_0', '85_1': '86_1', '85_2': '86_3', '85_3': '86_4', '85_4': '86_6', '85_5': '86_7', '85_6': '86_8', '85_7': '86_9', '85_8': '86_10', '86_0': '87_0', '86_1': '87_1', '86_2': '87_2', '86_3': '87_3', '86_4': '87_4', '86_5': '87_5', '86_6': '87_6', '86_7': '87_7', '86_8': '87_8', '86_9': '87_9', '87_0': '88_0', '87_1': '88_1', '87_2': '88_2', '87_3': '88_3', '88_0': '89_0', '88_1': '89_1', '88_2': '89_2', '88_3': '89_4', '88_4': '89_5', '88_5': '89_6', '88_6': '89_7', '88_7': '89_8', '88_8': '89_9', '88_9': '89_11', '88_10': '89_15', '88_11': '89_16', '89_0': '91_0', '90_0': '92_0', '90_1': '92_1', '91_0': '93_0', '91_1': '93_1', '91_2': '93_2', '91_3': '93_4', '92_0': '94_0', '92_1': '94_1', '92_2': '94_3', '92_3': '94_6', '92_4': '94_7', '92_5': '94_8', '92_6': '94_9', '92_7': '94_10', '92_8': '94_11', '92_9': '94_12', '92_10': '94_13', '92_11': '94_14', '92_12': '94_15', '93_0': '95_0', '93_1': '95_2', '93_2': '95_3', '93_3': '95_4', '93_4': '95_5', '93_5': '95_8', '93_6': '95_9', '93_7': '95_10', '93_8': '95_11', '93_9': '95_13', '93_10': '95_14', '93_11': '95_15', '93_12': '95_16', '93_13': '95_17', '94_0': '96_0', '94_1': '96_1', '94_2': '96_2', '94_3': '96_3', '94_4': '96_4', '95_0': '97_0', '95_1': '97_1', '95_2': '97_2', '95_3': '97_3', '96_0': '98_1', '97_0': '99_0', '97_1': '99_1', '97_2': '99_2', '97_3': '99_3', '97_4': '99_4', '97_5': '99_6', '97_6': '99_7', '97_7': '99_8', '97_8': '99_9', '97_9': '99_10', '97_10': '99_11', '97_11': '99_12', '97_12': '99_13', '97_13': '99_14', '97_14': '99_15', '97_15': '99_16', '97_16': '99_17', '97_17': '99_18', '98_0': '100_0', '98_1': '100_1', '98_2': '100_2', '98_3': '100_4', '98_4': '100_5', '98_5': '100_6', '99_0': '101_0', '100_0': '102_0', '100_1': '102_1', '100_2': '102_2', '101_0': '103_0', '101_1': '103_1', '101_2': '103_3', '101_3': '103_4', '101_4': '103_5', '101_5': '103_6', '101_6': '103_7', '102_0': '104_1', '103_0': '106_0', '103_1': '106_2', '103_2': '106_3', '103_3': '106_4', '104_0': '107_0', '104_1': '107_1', '104_2': '107_2', '104_3': '107_4', '104_4': '107_5', '104_5': '107_6', '104_6': '107_7', '104_7': '107_8', '104_8': '107_9', '104_9': '107_10', '105_0': '108_0', '105_1': '108_1', '105_2': '108_2', '105_3': '108_3', '105_4': '108_4', '105_5': '108_5', '105_6': '108_6', '106_0': '109_0', '106_1': '109_3', '106_2': '109_4', '106_3': '109_5', '106_4': '109_6', '106_5': '109_7', '106_6': '109_8', '106_7': '109_9', '106_8': '109_10', '106_9': '109_11', '106_10': '109_12', '106_11': '109_13', '106_12': '109_14', '106_13': '109_15', '106_14': '109_20', '106_15': '109_21', '107_0': '110_0', '107_1': '110_3', '107_2': '110_4', '107_3': '110_5', '108_0': '111_0', '108_1': '111_2', '108_2': '111_3', '108_3': '111_7', '108_4': '111_9', '108_5': '111_11', '108_6': '111_12', '108_7': '111_14', '109_0': '112_2', '109_1': '112_3', '110_0': '113_0', '110_1': '113_1', '110_2': '113_2', '110_3': '113_3', '110_4': '113_4', '110_5': '113_5', '110_6': '113_6', '110_7': '113_7', '110_8': '113_8', '110_9': '113_9', '110_10': '113_10', '111_0': '114_0', '111_1': '114_2', '111_2': '114_3', '111_3': '114_4', '112_0': '115_0', '112_1': '115_1', '112_2': '115_2', '112_3': '115_3', '112_4': '115_4', '112_5': '115_5', '113_0': '116_0', '113_1': '116_1', '113_2': '116_2', '113_3': '116_4', '113_4': '116_5', '113_5': '116_6', '113_6': '116_7', '114_0': '117_0', '114_1': '117_1', '114_2': '117_2', '114_3': '117_3', '115_0': '118_0', '115_1': '118_2', '115_2': '118_3', '115_3': '118_4', '115_4': '118_5', '116_0': '119_0', '116_1': '119_1', '116_2': '119_2', '116_3': '119_3', '116_4': '119_4', '116_5': '119_5', '116_6': '119_6', '116_7': '119_8', '116_8': '119_9', '116_9': '119_10', '116_10': '119_11', '116_11': '119_12', '117_0': '120_1', '117_1': '120_3', '117_2': '120_7', '117_3': '120_8', '117_4': '120_9', '117_5': '120_10', '117_6': '120_11', '117_7': '120_12', '117_8': '120_13', '117_9': '120_14', '117_10': '120_15', '118_0': '121_0', '118_1': '121_1', '118_2': '121_2', '118_3': '121_5', '118_4': '121_6', '118_5': '121_7', '119_0': '122_2', '119_1': '122_3', '119_2': '122_4', '119_3': '122_5', '119_4': '122_6', '119_5': '122_7', '119_6': '122_8', '119_7': '122_10', '119_8': '122_11', '119_9': '122_14', '119_10': '122_15', '120_0': '123_1', '120_1': '123_2', '120_2': '123_3', '120_3': '123_4', '120_4': '123_5', '120_5': '123_6', '120_6': '123_7', '120_7': '123_8', '120_8': '123_9', '120_9': '123_10', '121_0': '124_0', '121_1': '124_2', '121_2': '124_4', '121_3': '124_5', '121_4': '124_6', '121_5': '124_8', '121_6': '124_9', '121_7': '124_10', '121_8': '124_11', '122_0': '125_0', '122_1': '125_1', '122_2': '125_2', '122_3': '125_3', '122_4': '125_4', '122_5': '125_5', '122_6': '125_6', '122_7': '125_7', '123_0': '126_0', '123_1': '126_1', '123_2': '126_2', '123_3': '126_3', '123_4': '126_4', '123_5': '126_5', '123_6': '126_6', '123_7': '126_7', '123_8': '126_8', '123_9': '126_9', '123_10': '126_10', '123_11': '126_11', '123_12': '126_12', '124_0': '127_0', '125_0': '128_0', '125_1': '128_1', '125_2': '128_2', '125_3': '128_3', '125_4': '128_4', '125_5': '128_5', '125_6': '128_6', '125_7': '128_7', '125_8': '128_8', '125_9': '128_9', '126_0': '129_0', '126_1': '129_1', '126_2': '129_2', '126_3': '129_4', '126_4': '129_5', '126_5': '129_7', '126_6': '129_8', '126_7': '129_9', '126_8': '129_10', '126_9': '129_13', '126_10': '129_14', '127_0': '130_0', '127_1': '130_1', '127_2': '130_2', '127_3': '130_4', '127_4': '130_5', '127_5': '130_6', '127_6': '130_7', '127_7': '130_8', '127_8': '130_10', '127_9': '130_12', '127_10': '130_13', '127_11': '130_14', '127_12': '130_15', '127_13': '130_17', '128_0': '131_0', '128_1': '131_1', '128_2': '131_2', '129_0': '132_1', '129_1': '132_2', '129_2': '132_4', '129_3': '132_5', '129_4': '132_6', '129_5': '132_7', '129_6': '132_8', '129_7': '132_9', '129_8': '132_10', '130_0': '134_0', '131_0': '135_0', '131_1': '135_1', '131_2': '135_2', '131_3': '135_4', '131_4': '135_5', '131_5': '135_6', '131_6': '135_7', '131_7': '135_8', '132_0': '136_0', '132_1': '136_1', '132_2': '136_4', '132_3': '136_5', '133_0': '137_0', '133_1': '137_1', '133_2': '137_2', '133_3': '137_3', '133_4': '137_4', '133_5': '137_5', '133_6': '137_6', '133_7': '137_7', '134_0': '138_0', '134_1': '138_1', '134_2': '138_2', '134_3': '138_3', '134_4': '138_7', '134_5': '138_8', '134_6': '138_9', '134_7': '138_10', '134_8': '138_11', '134_9': '138_12', '134_10': '138_14', '134_11': '138_15', '135_0': '139_0', '135_1': '139_1', '135_2': '139_2', '135_3': '139_3', '135_4': '139_5', '136_0': '140_0', '136_1': '140_1', '137_0': '141_0', '137_1': '141_1', '137_2': '141_2', '137_3': '141_3', '137_4': '141_4', '137_5': '141_5', '137_6': '141_6', '137_7': '141_7', '138_0': '142_0', '138_1': '142_1', '139_0': '143_0', '139_1': '143_1', '139_2': '143_2', '139_3': '143_3', '139_4': '143_4', '140_0': '144_0', '140_1': '144_1', '140_2': '144_2', '140_3': '144_3', '140_4': '144_5', '141_0': '145_1', '141_1': '145_2', '141_2': '145_3', '141_3': '145_6', '141_4': '145_8', '141_5': '145_12', '141_6': '145_13', '141_7': '145_14', '141_8': '145_17', '142_0': '146_0', '142_1': '146_1', '142_2': '146_2', '142_3': '146_3', '142_4': '146_4', '142_5': '146_5', '142_6': '146_6', '142_7': '146_7', '142_8': '146_8', '142_9': '146_9', '142_10': '146_10', '142_11': '146_11', '142_12': '146_12', '143_0': '147_0', '143_1': '147_1', '143_2': '147_2', '143_3': '147_3', '143_4': '147_5', '143_5': '147_9', '143_6': '147_10', '143_7': '147_11', '144_0': '148_0', '144_1': '148_1', '144_2': '148_2', '144_3': '148_3', '144_4': '148_4', '144_5': '148_7', '144_6': '148_8', '144_7': '148_9', '144_8': '148_10', '144_9': '148_11', '144_10': '148_12', '144_11': '148_14', '144_12': '148_16', '144_13': '148_17', '144_14': '148_18', '144_15': '148_19', '144_16': '148_20', '145_0': '149_0', '145_1': '149_1', '145_2': '149_2', '145_3': '149_3', '146_0': '150_1', '146_1': '150_2', '146_2': '150_3', '147_0': '151_0', '147_1': '151_1', '147_2': '151_2', '147_3': '151_3', '147_4': '151_7', '147_5': '151_8', '147_6': '151_9', '147_7': '151_10', '147_8': '151_11', '147_9': '151_12', '147_10': '151_13', '147_11': '151_15', '147_12': '151_16', '147_13': '151_17', '148_0': '152_1', '148_1': '152_4', '148_2': '152_5', '148_3': '152_6', '148_4': '152_7', '148_5': '152_9', '148_6': '152_10', '148_7': '152_11', '149_0': '153_0', '149_1': '153_1', '149_2': '153_2', '149_3': '153_3', '149_4': '153_4', '149_5': '153_5', '149_6': '153_7', '149_7': '153_9', '149_8': '153_10', '149_9': '153_11', '149_10': '153_12', '150_0': '154_0', '150_1': '154_1', '150_2': '154_2', '150_3': '154_4', '150_4': '154_6', '150_5': '154_7', '150_6': '154_8', '151_0': '155_0', '151_1': '155_1', '151_2': '155_2', '151_3': '155_3', '151_4': '155_4', '151_5': '155_6', '151_6': '155_7', '151_7': '155_9', '152_0': '156_0', '152_1': '156_1', '152_2': '156_3', '152_3': '156_9', '153_0': '157_0', '153_1': '157_1', '153_2': '157_2', '154_0': '158_6', '155_0': '159_0', '155_1': '159_2', '155_2': '159_3', '155_3': '159_5', '155_4': '159_6', '155_5': '159_7', '155_6': '159_8', '156_0': '160_1', '156_1': '160_5', '156_2': '160_8', '156_3': '160_10', '156_4': '160_12', '156_5': '160_15', '156_6': '160_16', '157_0': '161_1', '158_0': '163_1', '158_1': '163_3', '158_2': '163_8', '158_3': '163_9', '159_0': '164_4', '159_1': '164_8', '159_2': '164_9', '160_0': '165_1', '160_1': '165_5', '160_2': '165_7', '160_3': '165_8', '160_4': '165_16', '160_5': '165_17', '160_6': '165_18', '160_7': '165_19', '161_0': '167_0', '161_1': '167_2', '161_2': '167_6', '161_3': '167_8', '161_4': '167_9', '162_0': '168_9', '162_1': '168_11', '163_0': '169_0', '163_1': '169_1', '164_0': '170_10', '165_0': '171_0', '166_0': '172_3', '166_1': '172_4', '166_2': '172_8', '167_0': '173_0', '167_1': '173_2', '167_2': '173_3', '167_3': '173_4', '167_4': '173_6', '167_5': '173_8', '167_6': '173_10', '168_0': '174_0', '168_1': '174_1', '168_2': '174_2', '168_3': '174_3', '168_4': '174_6', '169_0': '175_1', '169_1': '175_2', '169_2': '175_5', '169_3': '175_7', '169_4': '175_9', '169_5': '175_11', '170_0': '176_10', '171_0': '177_0', '171_1': '177_4', '171_2': '177_6', '171_3': '177_7', '171_4': '177_8', '171_5': '177_9', '172_0': '178_0', '173_0': '179_10', '173_1': '179_19', '174_0': '180_0', '174_1': '180_4', '174_2': '180_5', '174_3': '180_9', '174_4': '180_14', '175_0': '181_0', '176_0': '182_1', '176_1': '182_2', '176_2': '182_3', '176_3': '182_4', '176_4': '182_5', '176_5': '182_6', '176_6': '182_8', '176_7': '182_9', '177_0': '183_2', '177_1': '183_5', '177_2': '183_9', '177_3': '183_12', '177_4': '183_19', '178_0': '184_0', '178_1': '184_3', '178_2': '184_4', '178_3': '184_5', '178_4': '184_6', '178_5': '184_9', '178_6': '184_10', '178_7': '184_13', '178_8': '184_16', '178_9': '184_17', '178_10': '184_19', '178_11': '184_21', '179_0': '185_5', '180_0': '186_0', '180_1': '186_1', '180_2': '186_2', '180_3': '186_3', '181_0': '187_0', '181_1': '187_2', '181_2': '187_9', '182_0': '188_3', '182_1': '188_4', '182_2': '188_5', '183_0': '189_5', '183_1': '189_6', '183_2': '189_8', '183_3': '189_13', '183_4': '189_14', '184_0': '190_2', '184_1': '190_4', '184_2': '190_6', '185_0': '191_0', '185_1': '191_3', '186_0': '192_0', '186_1': '192_3', '186_2': '192_4', '186_3': '192_6', '187_0': '193_1', '187_1': '193_2', '188_0': '194_1', '189_0': '195_2', '189_1': '195_4', '189_2': '195_8', '189_3': '195_14', '189_4': '195_19', '190_0': '197_0', '190_1': '197_2', '190_2': '197_4', '191_0': '198_2', '191_1': '198_6', '192_0': '199_0', '192_1': '199_1', '192_2': '199_2', '192_3': '199_3', '193_0': '200_1', '193_1': '200_4', '193_2': '200_5', '193_3': '200_6', '193_4': '200_7', '193_5': '200_8', '193_6': '200_9', '194_0': '201_2', '194_1': '201_4', '194_2': '201_8', '194_3': '201_9', '195_0': '202_0', '195_1': '202_8', '195_2': '202_16', '196_0': '203_0', '196_1': '203_2', '196_2': '203_3', '196_3': '203_4', '197_0': '204_0', '197_1': '204_13', '198_0': '205_4', '198_1': '205_6', '199_0': '206_1', '199_1': '206_4', '199_2': '206_7', '199_3': '206_9', '199_4': '206_11', '199_5': '206_12', '199_6': '206_13', '199_7': '206_14', '199_8': '206_15', '199_9': '206_16', '199_10': '206_18', '200_0': '207_0', '200_1': '207_10', '200_2': '207_11', '200_3': '207_13', '200_4': '207_17', '200_5': '207_18', '201_0': '208_5', '201_1': '208_6', '201_2': '208_9', '201_3': '208_11', '201_4': '208_14', '202_0': '210_0', '202_1': '210_2', '202_2': '210_3', '202_3': '210_4', '202_4': '210_5', '202_5': '210_6', '202_6': '210_7', '202_7': '210_8', '203_0': '211_0', '203_1': '211_2', '203_2': '211_5', '203_3': '211_10', '204_0': '212_1', '204_1': '212_7', '205_0': '213_2', '205_1': '213_4', '206_0': '214_2', '206_1': '214_8', '207_0': '215_0', '208_0': '216_0', '209_0': '218_4', '209_1': '218_6', '210_0': '219_0', '210_1': '219_3', '210_2': '219_4', '210_3': '219_6', '210_4': '219_8', '210_5': '219_10', '210_6': '219_11', '210_7': '219_12', '211_0': '220_1', '211_1': '220_3', '212_0': '222_0', '212_1': '222_3', '212_2': '222_4', '212_3': '222_5', '212_4': '222_8', '212_5': '222_11', '212_6': '222_12', '212_7': '222_13', '213_0': '223_5', '214_0': '224_4', '214_1': '224_5', '215_0': '225_2', '215_1': '225_3', '215_2': '225_6', '215_3': '225_9', '216_0': '226_1', '216_1': '226_9', '216_2': '226_17', '216_3': '226_19', '216_4': '226_21', '217_0': '228_0', '217_1': '228_1', '217_2': '228_4', '217_3': '228_5', '217_4': '228_7', '218_0': '229_1', '219_0': '230_1', '220_0': '231_7', '220_1': '231_9', '220_2': '231_10', '221_0': '232_5', '221_1': '232_7', '222_0': '233_0', '223_0': '234_1', '223_1': '234_6', '223_2': '234_11', '224_0': '235_0', '224_1': '235_2', '224_2': '235_3', '224_3': '235_5', '224_4': '235_8', '225_0': '236_0', '225_1': '236_5', '225_2': '236_13', '225_3': '236_14', '225_4': '236_15', '226_0': '237_2', '226_1': '237_5', '226_2': '237_8', '226_3': '237_9', '227_0': '239_2', '227_1': '239_3', '227_2': '239_6', '228_0': '241_0', '228_1': '241_1', '228_2': '241_3', '228_3': '241_4', '228_4': '241_5', '228_5': '241_8', '228_6': '241_10', '229_0': '242_0', '229_1': '242_6', '230_0': '243_0', '231_0': '245_0', '231_1': '245_1', '231_2': '245_2', '231_3': '245_3', '231_4': '245_5', '231_5': '245_7', '231_6': '245_8', '231_7': '245_9', '231_8': '245_13', '231_9': '245_15', '232_0': '246_0', '232_1': '246_1', '233_0': '247_2', '234_0': '248_1', '234_1': '248_3', '234_2': '248_4', '234_3': '248_5', '234_4': '248_6', '234_5': '248_7', '235_0': '249_3', '235_1': '249_4', '235_2': '249_5', '235_3': '249_6', '235_4': '249_7', '235_5': '249_9', '235_6': '249_10', '235_7': '249_13', '236_0': '250_1', '236_1': '250_3', '237_0': '251_0', '237_1': '251_6', '238_0': '252_14', '238_1': '252_17', '239_0': '253_1', '239_1': '253_5', '239_2': '253_8', '239_3': '253_10', '240_0': '254_1', '240_1': '254_2', '240_2': '254_3', '240_3': '254_4', '240_4': '254_5', '240_5': '254_6', '240_6': '254_8', '240_7': '254_9', '241_0': '255_5', '241_1': '255_7', '241_2': '255_9', '241_3': '255_10', '241_4': '255_11', '241_5': '255_12', '242_0': '256_0', '242_1': '256_2', '242_2': '256_6', '242_3': '256_7', '242_4': '256_10', '242_5': '256_11', '242_6': '256_12', '242_7': '256_13', '242_8': '256_14', '243_0': '257_0', '243_1': '257_4', '244_0': '258_0', '244_1': '258_6', '245_0': '259_2', '245_1': '259_3', '246_0': '260_1', '246_1': '260_3', '246_2': '260_4', '246_3': '260_5', '246_4': '260_6', '246_5': '260_7', '246_6': '260_11', '247_0': '261_3', '247_1': '261_4', '247_2': '261_5', '247_3': '261_8', '248_0': '262_3', '248_1': '262_4', '249_0': '263_6', '249_1': '263_8', '250_0': '264_4', '250_1': '264_7', '250_2': '264_15', '251_0': '265_0', '251_1': '265_2', '251_2': '265_4', '252_0': '270_1', '252_1': '270_2', '253_0': '271_1', '253_1': '271_2', '253_2': '271_3', '253_3': '271_4', '253_4': '271_9', '254_0': '272_2', '254_1': '272_3', '254_2': '272_7', '254_3': '272_8', '255_0': '273_1', '255_1': '273_2', '255_2': '273_3', '255_3': '273_4', '255_4': '273_8', '255_5': '273_13', '256_0': '274_5', '256_1': '274_6', '256_2': '274_7', '256_3': '274_9', '257_0': '275_0', '257_1': '275_1', '257_2': '275_4', '257_3': '275_11', '257_4': '275_16', '258_0': '276_0', '258_1': '276_2', '259_0': '277_3', '260_0': '278_1', '260_1': '278_2', '260_2': '278_4', '260_3': '278_5', '260_4': '278_7', '260_5': '278_8', '261_0': '279_1', '261_1': '279_2', '261_2': '279_4', '261_3': '279_9', '261_4': '279_11', '261_5': '279_13', '262_0': '280_1', '263_0': '281_0', '263_1': '281_1', '264_0': '282_0', '264_1': '282_2', '265_0': '285_1', '265_1': '285_3', '265_2': '285_7', '266_0': '287_1', '266_1': '287_2', '266_2': '287_4', '266_3': '287_7', '266_4': '287_12', '267_0': '288_0', '267_1': '288_6', '267_2': '288_7', '267_3': '288_8', '267_4': '288_15', '267_5': '288_20', '268_0': '289_3', '268_1': '289_4', '269_0': '290_0', '270_0': '291_0', '270_1': '291_1', '271_0': '292_1', '271_1': '292_4', '272_0': '293_8', '272_1': '293_10', '272_2': '293_12', '273_0': '294_0', '274_0': '295_4', '274_1': '295_5', '274_2': '295_6', '274_3': '295_7', '274_4': '295_8', '274_5': '295_9', '274_6': '295_10', '274_7': '295_11', '274_8': '295_13', '274_9': '295_14', '274_10': '295_16', '274_11': '295_17', '274_12': '295_18', '274_13': '295_19', '274_14': '295_20', '274_15': '295_21', '275_0': '296_5', '275_1': '296_11', '276_0': '297_1', '276_1': '297_2', '276_2': '297_4', '276_3': '297_6', '276_4': '297_8', '277_0': '298_0', '277_1': '298_1', '277_2': '298_2', '277_3': '298_4', '277_4': '298_5', '277_5': '298_6', '278_0': '300_0', '278_1': '300_3', '278_2': '300_9', '278_3': '300_10', '279_0': '301_8', '279_1': '301_11', '279_2': '301_16', '280_0': '302_1', '280_1': '302_3', '281_0': '303_0', '282_0': '304_8', '282_1': '304_9', '282_2': '304_10', '283_0': '305_0', '283_1': '305_5', '283_2': '305_12', '283_3': '305_13', '283_4': '305_14', '284_0': '306_0', '284_1': '306_1', '284_2': '306_2', '284_3': '306_3', '284_4': '306_5', '284_5': '306_6', '285_0': '307_1', '285_1': '307_2', '286_0': '308_1', '287_0': '309_0', '287_1': '309_2', '288_0': '310_2', '288_1': '310_3', '288_2': '310_6', '289_0': '311_1', '289_1': '311_2', '289_2': '311_3', '289_3': '311_5', '289_4': '311_7', '290_0': '312_1', '290_1': '312_2', '290_2': '312_4', '290_3': '312_5', '290_4': '312_9', '290_5': '312_12', '290_6': '312_13', '290_7': '312_17', '290_8': '312_18', '290_9': '312_21', '291_0': '313_3', '292_0': '314_0', '292_1': '314_1', '292_2': '314_2', '292_3': '314_4', '292_4': '314_5', '292_5': '314_9', '293_0': '315_2', '293_1': '315_3', '293_2': '315_4', '293_3': '315_6', '293_4': '315_7', '293_5': '315_8', '293_6': '315_9', '294_0': '316_2', '294_1': '316_5', '294_2': '316_6', '295_0': '317_1', '295_1': '317_3', '295_2': '317_5', '295_3': '317_6', '295_4': '317_7', '295_5': '317_9', '295_6': '317_10', '295_7': '317_12', '295_8': '317_13', '295_9': '317_14', '295_10': '317_15', '296_0': '319_1', '296_1': '319_6', '296_2': '319_8', '296_3': '319_14', '296_4': '319_17', '296_5': '319_19', '296_6': '319_20', '297_0': '320_4', '297_1': '320_5', '297_2': '320_6', '297_3': '320_8', '297_4': '320_13', '297_5': '320_17', '298_0': '321_0', '298_1': '321_1', '298_2': '321_2', '298_3': '321_7', '299_0': '323_0', '300_0': '326_0', '300_1': '326_6', '301_0': '327_0', '301_1': '327_4', '301_2': '327_6', '302_0': '328_0', '302_1': '328_1', '302_2': '328_2', '302_3': '328_4', '302_4': '328_6', '302_5': '328_7', '302_6': '328_8', '303_0': '329_0', '303_1': '329_1', '303_2': '329_3', '303_3': '329_4', '303_4': '329_7', '303_5': '329_9', '304_0': '330_2', '305_0': '331_0', '305_1': '331_4', '305_2': '331_8', '305_3': '331_9', '306_0': '332_5', '306_1': '332_7', '306_2': '332_12', '307_0': '334_0', '307_1': '334_2', '308_0': '335_1', '309_0': '336_1', '309_1': '336_6', '309_2': '336_7', '309_3': '336_9', '309_4': '336_11', '309_5': '336_15', '309_6': '336_16', '309_7': '336_17', '309_8': '336_18', '309_9': '336_21', '310_0': '337_4', '310_1': '337_5', '310_2': '337_6', '310_3': '337_15', '310_4': '337_18', '311_0': '338_1', '311_1': '338_4', '311_2': '338_8', '312_0': '339_3', '312_1': '339_4', '313_0': '342_5', '313_1': '342_6', '313_2': '342_7', '313_3': '342_10', '313_4': '342_13', '313_5': '342_15', '313_6': '342_17', '314_0': '344_0', '314_1': '344_6', '314_2': '344_7', '314_3': '344_11', '314_4': '344_12', '315_0': '345_2', '315_1': '345_4', '316_0': '346_0', '317_0': '347_0', '317_1': '347_4', '317_2': '347_7', '317_3': '347_9', '317_4': '347_10', '318_0': '348_0', '318_1': '348_1', '318_2': '348_2', '319_0': '349_0', '319_1': '349_1', '319_2': '349_2', '319_3': '349_4', '319_4': '349_5', '319_5': '349_7', '319_6': '349_10', '320_0': '350_1', '320_1': '350_2', '320_2': '350_3', '321_0': '351_0', '321_1': '351_3', '321_2': '351_5', '321_3': '351_8', '321_4': '351_9', '321_5': '351_10', '321_6': '351_11', '321_7': '351_12', '321_8': '351_13', '321_9': '351_15', '322_0': '352_1', '322_1': '352_2', '322_2': '352_3', '322_3': '352_5', '323_0': '354_8', '323_1': '354_10', '323_2': '354_14', '324_0': '355_0', '324_1': '355_3', '325_0': '356_0', '325_1': '356_6', '325_2': '356_7', '325_3': '356_9', '325_4': '356_10', '325_5': '356_13', '325_6': '356_17', '325_7': '356_21', '326_0': '358_3', '326_1': '358_9', '326_2': '358_10', '326_3': '358_14', '326_4': '358_18', '327_0': '359_3', '327_1': '359_4', '327_2': '359_5', '327_3': '359_6', '328_0': '360_0', '328_1': '360_1', '329_0': '361_6', '329_1': '361_10', '329_2': '361_11', '330_0': '362_0', '330_1': '362_3', '330_2': '362_5', '331_0': '365_5', '331_1': '365_6', '331_2': '365_14', '332_0': '366_2', '332_1': '366_5', '332_2': '366_6', '332_3': '366_7', '332_4': '366_9', '333_0': '367_0', '333_1': '367_2', '333_2': '367_8', '334_0': '368_0', '334_1': '368_1', '335_0': '370_1', '336_0': '371_11', '337_0': '372_1', '337_1': '372_2', '337_2': '372_3', '337_3': '372_4', '337_4': '372_5', '337_5': '372_6', '337_6': '372_8', '337_7': '372_9', '337_8': '372_10', '338_0': '373_4', '339_0': '374_0', '339_1': '374_4', '339_2': '374_7', '339_3': '374_8', '339_4': '374_9', '339_5': '374_12', '339_6': '374_13', '339_7': '374_14', '340_0': '375_0', '340_1': '375_2', '340_2': '375_3', '341_0': '376_0', '341_1': '376_1', '341_2': '376_2', '342_0': '377_6', '343_0': '379_1', '343_1': '379_2', '343_2': '379_3', '343_3': '379_4', '343_4': '379_5', '343_5': '379_7', '344_0': '380_1', '344_1': '380_2', '344_2': '380_4', '345_0': '382_0', '345_1': '382_2', '346_0': '383_2', '346_1': '383_3', '347_0': '384_4', '348_0': '385_0', '348_1': '385_1', '348_2': '385_2', '348_3': '385_3', '349_0': '386_0', '350_0': '387_0', '350_1': '387_1', '350_2': '387_2', '350_3': '387_3', '350_4': '387_4', '351_0': '388_0', '351_1': '388_5', '351_2': '388_12', '351_3': '388_14', '351_4': '388_16', '352_0': '389_1', '352_1': '389_2', '352_2': '389_6', '352_3': '389_11', '352_4': '389_12', '353_0': '390_1', '354_0': '392_0', '354_1': '392_1', '354_2': '392_2', '354_3': '392_3', '354_4': '392_4', '354_5': '392_6', '354_6': '392_8', '354_7': '392_9', '354_8': '392_12', '355_0': '393_4', '355_1': '393_5', '355_2': '393_6', '355_3': '393_7', '356_0': '394_0', '357_0': '395_0', '357_1': '395_2', '357_2': '395_3', '357_3': '395_4', '357_4': '395_5', '357_5': '395_6', '358_0': '397_0', '359_0': '398_2', '359_1': '398_3', '359_2': '398_6', '359_3': '398_8', '360_0': '400_0', '360_1': '400_1', '360_2': '400_2', '360_3': '400_5', '360_4': '400_8', '361_0': '401_7', '361_1': '401_11', '362_0': '404_0', '362_1': '404_1', '362_2': '404_3', '362_3': '404_4', '362_4': '404_7', '362_5': '404_8', '362_6': '404_9', '362_7': '404_10', '363_0': '405_1', '363_1': '405_2', '363_2': '405_3', '363_3': '405_4', '364_0': '406_0', '364_1': '406_1', '364_2': '406_2', '364_3': '406_3', '365_0': '407_0', '365_1': '407_1', '365_2': '407_6', '365_3': '407_7', '366_0': '409_3', '367_0': '410_2', '367_1': '410_3', '367_2': '410_10', '367_3': '410_15', '367_4': '410_18', '368_0': '411_3', '369_0': '412_0', '369_1': '412_4', '369_2': '412_8', '369_3': '412_14', '370_0': '414_7', '370_1': '414_8', '371_0': '418_10', '371_1': '418_18', '371_2': '418_19', '372_0': '420_0', '372_1': '420_1', '372_2': '420_2', '373_0': '423_6', '373_1': '423_7', '373_2': '423_9', '373_3': '423_10', '374_0': '424_0', '374_1': '424_12', '374_2': '424_14', '374_3': '424_15', '374_4': '424_16', '375_0': '425_2', '376_0': '426_1', '376_1': '426_2', '376_2': '426_7', '377_0': '427_0', '378_0': '428_1', '378_1': '428_2', '378_2': '428_4', '378_3': '428_6', '378_4': '428_8', '379_0': '430_0', '379_1': '430_2', '379_2': '430_3', '379_3': '430_8', '380_0': '431_6', '380_1': '431_11', '380_2': '431_12', '380_3': '431_13', '380_4': '431_14', '380_5': '431_16', '380_6': '431_17', '381_0': '434_0', '381_1': '434_1', '381_2': '434_2', '381_3': '434_10', '382_0': '436_2', '383_0': '437_0', '384_0': '438_0', '384_1': '438_10', '385_0': '441_1', '385_1': '441_7', '386_0': '445_2', '386_1': '445_9', '386_2': '445_10', '386_3': '445_13', '386_4': '445_14', '386_5': '445_15', '386_6': '445_16', '386_7': '445_17', '386_8': '445_18', '386_9': '445_19', '386_10': '445_20', '387_0': '446_14', '387_1': '446_15', '387_2': '446_16', '387_3': '446_17', '387_4': '446_18', '388_0': '448_1', '388_1': '448_2', '388_2': '448_3', '388_3': '448_4', '388_4': '448_5', '388_5': '448_6', '388_6': '448_8', '388_7': '448_9', '388_8': '448_10', '389_0': '449_2', '389_1': '449_3', '389_2': '449_7', '389_3': '449_9', '389_4': '449_10', '389_5': '449_12', '390_0': '450_7', '391_0': '451_1', '391_1': '451_4', '391_2': '451_7', '392_0': '454_2', '393_0': '455_8', '393_1': '455_11', '394_0': '456_2', '394_1': '456_4', '394_2': '456_6', '394_3': '456_7', '394_4': '456_8', '394_5': '456_9', '394_6': '456_11', '395_0': '457_2', '395_1': '457_7', '395_2': '457_8', '396_0': '459_1', '396_1': '459_3', '396_2': '459_11', '397_0': '463_1', '397_1': '463_3', '397_2': '463_5', '397_3': '463_10', '398_0': '464_17', '398_1': '464_18', '398_2': '464_20', '399_0': '465_3', '400_0': '466_9', '400_1': '466_11', '400_2': '466_12', '400_3': '466_13', '400_4': '466_14', '400_5': '466_15', '401_0': '468_1', '402_0': '473_2', '403_0': '474_1', '403_1': '474_3', '403_2': '474_7', '403_3': '474_8', '403_4': '474_9', '403_5': '474_10', '403_6': '474_16', '404_0': '475_0', '405_0': '476_0', '405_1': '476_4', '405_2': '476_11', '406_0': '477_6', '406_1': '477_15', '407_0': '478_10', '407_1': '478_12', '408_0': '479_4', '409_0': '480_9', '410_0': '481_12', '410_1': '481_13', '411_0': '482_2', '411_1': '482_7', '411_2': '482_9', '412_0': '483_1', '412_1': '483_6', '412_2': '483_8', '412_3': '483_9', '413_0': '486_10', '413_1': '486_12', '413_2': '486_13', '413_3': '486_14', '413_4': '486_15', '414_0': '491_1', '414_1': '491_2', '415_0': '493_0', '415_1': '493_12', '416_0': '495_2', '417_0': '496_2', '417_1': '496_7', '417_2': '496_10', '417_3': '496_12', '418_0': '497_1', '418_1': '497_2', '419_0': '498_5', '419_1': '498_6', '419_2': '498_7', '420_0': '503_9', '420_1': '503_11', '421_0': '504_12', '421_1': '504_13', '422_0': '505_12', '422_1': '505_14', '423_0': '506_0', '423_1': '506_2', '423_2': '506_3', '423_3': '506_5', '423_4': '506_6', '423_5': '506_7', '423_6': '506_8', '423_7': '506_9', '423_8': '506_10', '423_9': '506_11', '424_0': '507_4', '425_0': '508_0', '425_1': '508_19', '425_2': '508_20', '425_3': '508_21', '425_4': '508_22', '426_0': '509_4', '426_1': '509_10', '426_2': '509_11', '427_0': '511_0', '427_1': '511_6', '428_0': '512_8', '428_1': '512_10', '428_2': '512_11', '428_3': '512_12', '428_4': '512_14', '429_0': '513_3', '429_1': '513_4', '430_0': '516_0', '430_1': '516_5', '431_0': '517_0', '431_1': '517_5', '432_0': '518_0', '432_1': '518_1', '432_2': '518_3', '433_0': '519_1', '434_0': '521_2', '434_1': '521_3', '434_2': '521_4', '434_3': '521_5', '435_0': '523_6', '435_1': '523_12', '435_2': '523_14', '435_3': '523_21', '436_0': '524_2', '436_1': '524_11', '436_2': '524_13', '437_0': '525_3', '437_1': '525_4', '438_0': '526_16', '439_0': '527_0', '440_0': '528_5', '441_0': '529_0', '441_1': '529_3', '442_0': '530_0', '442_1': '530_5', '442_2': '530_6', '442_3': '530_7', '442_4': '530_8', '442_5': '530_9', '442_6': '530_10', '442_7': '530_11', '442_8': '530_12', '442_9': '530_16', '442_10': '530_17', '442_11': '530_23', '443_0': '531_3', '443_1': '531_5', '443_2': '531_6', '443_3': '531_10', '443_4': '531_11', '443_5': '531_14', '444_0': '532_7', '444_1': '532_8', '444_2': '532_9', '444_3': '532_10', '444_4': '532_13', '445_0': '533_2', '446_0': '535_2', '447_0': '536_4', '448_0': '537_3', '449_0': '538_0', '449_1': '538_1', '449_2': '538_3', '450_0': '539_6', '451_0': '540_0', '451_1': '540_5', '452_0': '541_4', '453_0': '542_1', '454_0': '543_0', '455_0': '545_0', '455_1': '545_1', '455_2': '545_3', '455_3': '545_11', '456_0': '546_8', '456_1': '546_9', '456_2': '546_12', '456_3': '546_14', '457_0': '548_5', '457_1': '548_10', '458_0': '549_2', '458_1': '549_7', '459_0': '553_19', '459_1': '553_20', '460_0': '554_0', '460_1': '554_1', '460_2': '554_10', '461_0': '555_0', '461_1': '555_2', '461_2': '555_3', '461_3': '555_4', '462_0': '556_2', '462_1': '556_4', '463_0': '557_7', '464_0': '558_6', '464_1': '558_8', '465_0': '561_5', '466_0': '564_9', '467_0': '567_0', '468_0': '568_4', '468_1': '568_10', '468_2': '568_11', '469_0': '570_0', '470_0': '571_2', '470_1': '571_13', '470_2': '571_15', '471_0': '572_0', '471_1': '572_6', '471_2': '572_7', '471_3': '572_8', '471_4': '572_10', '471_5': '572_12', '471_6': '572_19', '471_7': '572_20', '472_0': '573_8', '472_1': '573_10', '473_0': '574_19', '474_0': '576_10', '475_0': '583_1', '476_0': '585_0', '476_1': '585_1', '476_2': '585_2', '476_3': '585_3', '476_4': '585_4', '476_5': '585_6', '476_6': '585_10', '476_7': '585_12', '477_0': '588_0', '477_1': '588_1', '477_2': '588_5', '478_0': '589_5', '478_1': '589_7', '478_2': '589_10', '479_0': '590_5', '479_1': '590_8', '479_2': '590_11', '480_0': '592_3', '480_1': '592_9', '480_2': '592_10', '480_3': '592_12', '480_4': '592_13', '481_0': '593_3', '481_1': '593_4', '481_2': '593_5', '481_3': '593_6', '481_4': '593_7', '481_5': '593_8', '482_0': '594_0', '482_1': '594_1', '482_2': '594_2', '482_3': '594_3', '482_4': '594_5', '482_5': '594_6', '482_6': '594_7', '483_0': '595_0', '483_1': '595_2', '483_2': '595_3', '483_3': '595_4', '483_4': '595_5', '484_0': '596_0', '484_1': '596_5', '484_2': '596_6', '484_3': '596_8', '484_4': '596_10', '484_5': '596_15', '484_6': '596_16', '484_7': '596_19', '484_8': '596_20', '484_9': '596_21', '485_0': '598_2', '486_0': '599_2', '486_1': '599_4', '486_2': '599_5', '486_3': '599_10', '487_0': '605_1', '488_0': '607_2', '489_0': '612_6', '490_0': '613_9', '490_1': '613_10', '491_0': '614_19', '492_0': '615_3', '492_1': '615_5', '492_2': '615_7', '492_3': '615_8', '492_4': '615_9', '492_5': '615_10', '492_6': '615_11', '492_7': '615_13', '492_8': '615_14', '492_9': '615_18', '492_10': '615_22', '493_0': '616_0', '493_1': '616_2', '494_0': '617_6', '494_1': '617_7', '495_0': '619_7', '496_0': '621_3', '496_1': '621_4', '497_0': '622_4', '498_0': '623_2', '498_1': '623_3', '499_0': '625_11', '499_1': '625_13', '500_0': '626_4', '501_0': '632_3', '501_1': '632_4', '501_2': '632_5', '501_3': '632_10', '501_4': '632_11', '501_5': '632_16', '501_6': '632_17', '502_0': '633_9', '503_0': '634_0', '503_1': '634_2', '503_2': '634_3', '503_3': '634_5', '503_4': '634_7', '504_0': '637_1', '504_1': '637_2', '504_2': '637_3', '504_3': '637_6', '504_4': '637_8', '505_0': '638_4', '505_1': '638_7', '506_0': '644_2', '506_1': '644_7', '506_2': '644_9', '507_0': '645_5', '507_1': '645_6', '508_0': '646_9', '508_1': '646_20', '509_0': '648_1', '509_1': '648_2', '509_2': '648_3', '510_0': '649_23', '511_0': '650_0', '512_0': '651_10', '513_0': '652_3', '514_0': '654_0', '514_1': '654_3', '514_2': '654_4', '515_0': '655_3', '516_0': '657_14', '517_0': '658_5', '517_1': '658_11', '518_0': '659_3', '518_1': '659_6', '519_0': '660_4', '519_1': '660_5', '519_2': '660_6', '519_3': '660_14', '520_0': '661_0', '520_1': '661_7', '520_2': '661_8', '520_3': '661_9', '521_0': '662_6', '522_0': '664_1', '522_1': '664_2', '522_2': '664_3', '522_3': '664_4', '523_0': '665_18', '524_0': '666_4', '524_1': '666_9', '524_2': '666_11', '525_0': '667_2', '525_1': '667_8', '526_0': '670_1', '526_1': '670_2', '526_2': '670_3', '526_3': '670_15', '527_0': '672_8', '528_0': '676_0', '528_1': '676_3', '528_2': '676_5', '528_3': '676_6', '528_4': '676_7', '528_5': '676_9', '529_0': '680_5', '529_1': '680_13', '529_2': '680_14', '530_0': '681_2', '530_1': '681_4', '531_0': '682_0', '532_0': '686_6', '533_0': '692_0', '534_0': '694_3', '535_0': '698_5', '535_1': '698_7', '536_0': '700_6', '536_1': '700_11', '537_0': '702_1', '538_0': '716_1', '539_0': '717_6', '539_1': '717_9', '540_0': '722_5', '540_1': '722_6', '541_0': '726_0', '542_0': '733_10', '543_0': '736_4', '544_0': '738_0', '545_0': '740_0', '545_1': '740_5', '545_2': '740_7', '546_0': '745_0', '547_0': '747_2', '547_1': '747_14', '547_2': '747_15', '548_0': '750_3', '548_1': '750_4', '549_0': '754_0', '550_0': '756_5', '551_0': '759_14', '552_0': '763_2', '552_1': '763_5', '553_0': '767_3', '554_0': '768_1', '555_0': '769_0', '556_0': '770_15', '557_0': '772_2', '558_0': '774_0', '558_1': '774_1', '559_0': '778_1', '560_0': '780_4', '560_1': '780_6', '561_0': '783_8', '562_0': '784_1', '563_0': '791_9', '564_0': '792_13', '564_1': '792_16', '564_2': '792_17', '564_3': '792_18', '565_0': '794_5', '565_1': '794_7', '566_0': '796_0', '566_1': '796_19', '567_0': '797_11', '567_1': '797_12', '567_2': '797_13', '568_0': '798_4', '568_1': '798_5', '569_0': '803_1', '570_0': '804_4', '570_1': '804_9', '571_0': '806_5', '571_1': '806_6', '571_2': '806_8', '572_0': '810_1', '572_1': '810_13', '573_0': '813_5', '574_0': '816_0', '574_1': '816_1', '574_2': '816_2', '574_3': '816_3', '575_0': '817_10', '576_0': '818_6', '577_0': '820_4', '578_0': '822_7', '579_0': '826_4', '580_0': '830_6', '580_1': '830_7', '581_0': '831_6', '582_0': '832_11', '583_0': '835_0', '584_0': '840_15', '585_0': '841_6', '585_1': '841_9', '585_2': '841_10', '586_0': '842_4', '586_1': '842_5', '586_2': '842_6', '587_0': '843_1', '587_1': '843_7', '587_2': '843_9', '588_0': '844_5', '589_0': '848_4', '589_1': '848_8', '590_0': '852_2', '590_1': '852_4', '590_2': '852_8', '591_0': '853_2', '591_1': '853_4', '592_0': '855_5', '593_0': '856_2', '594_0': '861_0', '594_1': '861_1', '594_2': '861_2', '595_0': '865_4', '596_0': '866_10', '597_0': '871_1', '597_1': '871_10', '598_0': '872_0', '599_0': '874_1', '599_1': '874_3', '599_2': '874_4', '599_3': '874_5', '599_4': '874_7', '599_5': '874_10', '599_6': '874_11', '599_7': '874_12', '600_0': '884_12', '601_0': '888_0', '602_0': '892_16', '603_0': '893_2', '604_0': '895_9', '605_0': '896_1', '605_1': '896_2', '606_0': '897_2', '607_0': '901_13', '607_1': '901_14', '607_2': '901_17', '608_0': '904_2', '608_1': '904_3', '609_0': '905_0', '610_0': '916_13', '611_0': '918_8', '611_1': '918_11', '612_0': '920_0', '613_0': '921_3', '613_1': '921_5', '613_2': '921_7', '613_3': '921_8', '613_4': '921_9', '614_0': '922_3', '615_0': '923_1', '615_1': '923_4', '615_2': '923_12', '616_0': '924_5', '616_1': '924_6', '617_0': '926_1', '617_1': '926_2', '617_2': '926_5', '618_0': '933_15', '619_0': '935_4', '619_1': '935_7', '620_0': '936_8', '620_1': '936_10', '620_2': '936_11', '620_3': '936_12', '620_4': '936_14', '620_5': '936_15', '620_6': '936_18', '620_7': '936_19', '621_0': '938_1', '621_1': '938_2', '622_0': '939_8', '622_1': '939_9', '622_2': '939_11', '623_0': '940_0', '624_0': '945_1', '624_1': '945_2', '624_2': '945_4', '625_0': '949_0', '625_1': '949_6', '626_0': '950_1', '626_1': '950_4', '626_2': '950_7', '627_0': '953_7', '627_1': '953_11', '627_2': '953_20', '628_0': '954_4', '629_0': '957_0', '629_1': '957_1', '630_0': '958_0', '631_0': '959_1', '632_0': '961_6', '632_1': '961_8', '632_2': '961_10', '633_0': '964_7', '633_1': '964_8', '633_2': '964_10', '633_3': '964_12', '633_4': '964_15', '633_5': '964_16', '634_0': '968_2', '635_0': '971_1', '636_0': '972_7', '637_0': '973_6', '637_1': '973_11', '637_2': '973_13', '638_0': '976_10', '639_0': '977_0', '639_1': '977_10', '640_0': '980_3', '640_1': '980_11', '641_0': '981_1', '641_1': '981_3', '642_0': '983_3', '642_1': '983_4', '642_2': '983_6', '643_0': '985_0', '643_1': '985_3', '644_0': '988_3', '644_1': '988_4', '644_2': '988_6', '645_0': '989_2', '646_0': '990_6', '647_0': '992_10', '647_1': '992_15', '648_0': '994_2', '649_0': '996_6', '649_1': '996_20', '650_0': '1001_4', '651_0': '1006_0', '651_1': '1006_2', '652_0': '1007_1', '652_1': '1007_2', '652_2': '1007_5', '652_3': '1007_6', '652_4': '1007_7', '652_5': '1007_8', '652_6': '1007_12', '652_7': '1007_13', '652_8': '1007_14', '653_0': '1009_0', '653_1': '1009_7', '653_2': '1009_18', '654_0': '1010_5', '655_0': '1017_0', '655_1': '1017_1', '655_2': '1017_2', '656_0': '1018_3', '657_0': '1019_7', '658_0': '1021_4', '659_0': '1023_4', '660_0': '1024_15', '661_0': '1025_3', '662_0': '1026_12', '663_0': '1027_1', '663_1': '1027_2', '663_2': '1027_3', '664_0': '1030_1', '664_1': '1030_3', '664_2': '1030_4', '665_0': '1036_1', '665_1': '1036_3', '665_2': '1036_8', '665_3': '1036_20'}\n",
            "TEXT LSTM  Tensor(\"lstm_t_1/concat:0\", shape=(?, 27, 400), dtype=float32)\n",
            "TEXT OUTPUT  Tensor(\"time_distributed_5/Reshape_1:0\", shape=(?, 27, 4), dtype=float32)\n",
            "Train on 666 samples, validate on 114 samples\n",
            "Epoch 1/20\n",
            "666/666 [==============================] - 15s 22ms/sample - loss: 0.2100 - val_loss: 0.3832\n",
            "Epoch 2/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.2001 - val_loss: 0.3742\n",
            "Epoch 3/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.1812 - val_loss: 0.3875\n",
            "Epoch 4/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.1621 - val_loss: 0.4606\n",
            "Epoch 5/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.1504 - val_loss: 0.3923\n",
            "Epoch 6/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.1197 - val_loss: 0.4376\n",
            "Epoch 7/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.1016 - val_loss: 0.4487\n",
            "Epoch 8/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.0882 - val_loss: 0.4617\n",
            "Epoch 9/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.0746 - val_loss: 0.4476\n",
            "Epoch 10/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.0487 - val_loss: 0.5395\n",
            "Epoch 11/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.0350 - val_loss: 0.6324\n",
            "Epoch 12/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.0258 - val_loss: 0.6048\n",
            "Epoch 13/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.0143 - val_loss: 0.7758\n",
            "Epoch 14/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.0119 - val_loss: 0.7172\n",
            "Epoch 15/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.0094 - val_loss: 0.7847\n",
            "Epoch 16/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.0067 - val_loss: 0.7559\n",
            "Epoch 17/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.0041 - val_loss: 0.7994\n",
            "Epoch 18/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.0038 - val_loss: 0.8237\n",
            "Epoch 19/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.0044 - val_loss: 0.8471\n",
            "Epoch 20/20\n",
            "666/666 [==============================] - 7s 11ms/sample - loss: 0.0046 - val_loss: 0.7646\n",
            "Confusion Matrix :\n",
            "[[555 286 238 177]\n",
            " [ 60  66  36  46]\n",
            " [ 65  54 211  72]\n",
            " [ 59  52  76 158]]\n",
            "Classification Report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7510    0.4419    0.5564      1256\n",
            "           1     0.1441    0.3173    0.1982       208\n",
            "           2     0.3761    0.5249    0.4382       402\n",
            "           3     0.3488    0.4580    0.3960       345\n",
            "\n",
            "    accuracy                         0.4478      2211\n",
            "   macro avg     0.4050    0.4355    0.3972      2211\n",
            "weighted avg     0.5630    0.4478    0.4762      2211\n",
            "\n",
            "Weighted FScore: \n",
            "  (0.562992981208288, 0.44776119402985076, 0.4761785744085266, None)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"filename2 = '/content/drive/My Drive/mca/MCA_Project/Final_files/Bimodal_test_x_text.pkl'\\noutfile2 = open(filename2, 'wb')\\nnp.save(outfile2, N.test_x_text)\\n\\nfilename3 = '/content/drive/My Drive/mca/MCA_Project/Final_files/Bimodal_test_x_audio.pkl'\\noutfile3 = open(filename3, 'wb')\\nnp.save(outfile3, N.test_x_audio)\\n\\nfilename5 = '/content/drive/My Drive/mca/MCA_Project/Final_files/Bimodal_test_y.pkl'\\noutfile5 = open(filename5, 'wb')\\nnp.save(outfile5, N.test_y)\\n\\nfilename6 = '/content/drive/My Drive/mca/MCA_Project/Final_files/Bimodal_test_mask.pkl'\\noutfile6 = open(filename6, 'wb')\\nnp.save(outfile6, N.test_mask)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqjUR0GyMlwj",
        "colab_type": "code",
        "outputId": "26e2e0d7-91b6-49a5-c06e-a5b6dc2b7c2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "M.save(\"/content/drive/My Drive/mca/MCA_Project/Testing5.tf\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-72336f9d85a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/mca/MCA_Project/Testing5.tf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \"\"\"\n\u001b[1;32m   1170\u001b[0m     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1171\u001b[0;31m                       signatures)\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures)\u001b[0m\n\u001b[1;32m    107\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    108\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 109\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    110\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mmodel_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# TODO(b/128683857): Add integration tests between tf.keras and external\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_legacy_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0mweight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_group\u001b[0;34m(self, name, track_order)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mgcpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcpl_crt_order\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrack_order\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mgid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mGroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to create group (name already exists)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HikVk5X8Pbqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cstpne9Hzwbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/drive/My Drive/mca/MCA_Project/Final_files/Bimodal_AttentionLayer_Model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9Q7HO2p0Wb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "calc_test_result(model.predict(N.test_x), N.test_y, N.test_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC27_FGpwesl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fn_dict = '/content/drive/My Drive/mca/MCA_Project/MELD_Dataset/Audio_AttentionLayer_Model.sav'\n",
        "outfile = open(fn_dict, 'wb')\n",
        "np.save(outfile,N.audio_lstm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x0lwXovYvIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHHJDsqQLPPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install keras-multi-head"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FvDr0AIe3Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwnogt9VlAaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/scalars"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}